% !TEX root = ../notes.tex

\documentclass[../notes.tex]{subfiles}

\begin{document}

\section{February 6}

Today we continue talking about linear algebra.

\subsection{Determinants}
Today, $R$ will be a commutative ring. Given a free $R$-module $E$ of rank $n$, we showed last time that there is an isomorphism
\[\op{End}_R(E)\cong M_n(R)\]
of rings. Roughly speaking, this isomorphism is fixed as soon as we choose a basis for $E$. Restricting to invertible endomorphisms, we produce an isomorphism
\[\op{Aut}_R(E)\cong\op{GL}_n(R)\]
of groups.
\begin{notation}
	Fix a commutative ring $R$. Given a free $R$-module $E$, we set $\op{GL}_R(E)\coloneqq\op{Aut}_R(E)$. We may also write $\op{GL}(E)$.
\end{notation}
We would like a way to check if an element of $M_n(R)$ is invertible and thus lives in $\op{GL}_n(R)$. This is the role of the determinant.
\begin{definition}[determinant]
	Fix a commutative ring $R$. Then the determinant is the unique function $\det\colon M_n(R)\to R$ which satisfies the following.
	\begin{listalph}
		\item $\det$ is $R$-multilinear in the row and columns: scaling and adding two columns together does similar to the determinant.
		\item $\det$ is alternating in the row and columns: switching two rows or columns switches the sign of $\det$.
		\item $\det I_n=1$, where $I_n$ is the identity matrix.
	\end{listalph}
\end{definition}
\begin{remark}
	For characteristic $2$ reasons, perhaps one should change (ii) the requirement that the determinant vanishes whenever two columns are equal.
\end{remark}
\begin{remark}
	One can show using Gaussian elimination that these properties produce at most one function $\det\colon M_n(R)\to R$. Of course, it is not yet obvious that such a function exists at all, but we will explain shortly that it does.
\end{remark}
\begin{remark}
	These properties can be seen as geometrically intuitive, viewing $\det$ as a signed volume of the parallelepiped generated by the columns of the matrix.
\end{remark}
Let's say something about why the determinant exists. This boils down to the following claim.
\begin{proposition}
	Fix a commutative ring $R$ and a free module $E$ of rank $n$. Then there is a unique function $\det\colon E^n\to R$ satisfying the following properties.
	\begin{listalph}
		\item $\det$ is $R$-multilinear: one has $\det(v_1,\ldots,v_{i-1},ax+by,v_{i+1},\ldots,v_n)$ equals
		\[a\det(v_1,\ldots,v_{i-1},x,v_{i+1},\ldots,v_n)+b\det(v_1,\ldots,v_{i-1},y,v_{i+1},\ldots,v_n).\]
		\item $\det$ is alternating: one has
		\[\det(v_1,\ldots,v_n)=0\]
		if $v_i=v_j$ for any distinct pairs $(i,j)$.
		\item $\det(e_1,\ldots,e_n)=1$ for any choice of basis $\{e_1,\ldots,e_n\}$.
	\end{listalph}
\end{proposition}
\begin{remark}
	Roughly speaking, this proposition amounts to saying that the $n$th alternating power $\land^nE$ is free over $R$ of rank $1$. Then $\det$ makes a basis of this space.
\end{remark}
\begin{remark}
	One can show that $\det AB=\det A\cdot\det B$ for matrices $A,B\in M_n(R)$. In particular, we see that $\det\colon\op{GL}_n(R)\to R^\times$ is a group homomorphism.
\end{remark}
\begin{remark}
	Using the previous remark, we see that $A\in\op{GL}_n(R)$ implies that $\det A\in R^\times$. In fact, the converse is also true, but it is not so obvious. Roughly speaking, one should explicitly construct an inverse using something like Kramer's rule or the adjugate matrix, and somewhere in the construction of the inverse one needs to invert a determinant.
\end{remark}

\subsection{Bilinear Forms}
Here is our definition.
\begin{definition}[bilinear]
	Fix a commutative ring $R$ and two $R$-modules $E$ and $F$. Then a \textit{bilinear form} is a function $f\colon E\times F\to R$ satisfying the following.
	\begin{listalph}
		\item For any $x\in E$, the function $F\to R$ defined by $y\mapsto f(x,y)$ is $R$-linear.
		\item For any $y\in F$, the function $E\to R$ defined by $x\mapsto f(x,y)$ is $R$-linear.
	\end{listalph}
	We let $L^2(E,F;R)$ denote the set of bilinear forms $E\times F\to R$.
\end{definition}
\begin{remark}
	We note that $L^2(E,F;R)$ is an $R$-module.
\end{remark}
One can expand out these conditions in terms of elements. For example, the linearity of $y\mapsto f(x,y)$ amounts to requiring
\[f(x,b_1y_1+b_2y_2)=b_1f(x,y_1)+b_2f(x,y_2).\]
As in geometry, it will be helpful to have a notion of orthogonality.
\begin{definition}[orthogonal]
	Fix a commutative ring $R$, and let $\langle\cdot,\cdot\rangle\colon E\times F\to R$ be a bilinear form. Then we say that $x\in E$ and $y\in F$ are \textit{orthogonal} if and only if $\langle x,y\rangle=0$. We write $F^\perp$ for the collection of $x\in E$ such that $\langle x,y\rangle=0$ for all $y\in R$; we define $E^\perp\subseteq F$ similarly.
\end{definition}
\begin{remark}
	One can directly check that $F^\perp$ and $E^\perp$ are $R$-submodules of $E$ and $F$, respectively.
\end{remark}
\begin{definition}
	Fix a commutative ring $R$, and let $\langle\cdot,\cdot\rangle\colon E\times F\to R$ be a bilinear form.
	\begin{itemize}
		\item $\langle\cdot,\cdot\rangle$ is \textit{non-degenerate on the left} if and only if $F^\perp=0$.
		\item $\langle\cdot,\cdot\rangle$ is \textit{non-degenerate on the right} if and only if $E^\perp=0$.
		\item $\langle\cdot,\cdot\rangle$ is \textit{non-degenerate} if and only if we have both $E^\perp=0$ and $F^\perp=0$.
	\end{itemize}
\end{definition}
We take a moment to explain some currying.
\begin{proposition}
	Fix a commutative ring $R$ and $R$-modules $E$ and $F$. Then there is a canonical isomorphism
	\[L^2(E,F;R)\to\op{Hom}_R(E,\op{Hom}_R(F,R)).\]
\end{proposition}
\begin{proof}
	The forward map sends some bilinear form $f\colon E\times F\to R$ to the morphism $\varphi_f\colon E\to\op{Hom}_R(F,R)$ defined by
	\[\varphi_f(x)(y)\coloneqq f(x,y).\]
	Here, $\varphi_f(x)$ is a function $F\to R$, which explains the notation $\varphi_f(x)(y)$. The bilinearity conditions on $f$ exactly condition to the needed linearity checks on our forward map $\varphi_\bullet$.

	The backward map sends some $\varphi\colon E\to\op{Hom}_R(F,R)$ to the bilinear form $f\colon E\times F\to R$ by
	\[f(x,y)\coloneqq\varphi(x)(y).\]
	Once again, one checks that $f$ is bilinear using the linearity of $\varphi$ and $\varphi(x)$. One can check that these constructions are mutually inverse, so we have indeed defined some isomorphisms.
\end{proof}
This discussion motivates the following definition.
\begin{definition}
	Fix a commutative ring $R$, and let $f\colon E\times F\to R$ be a bilinear form.
	\begin{itemize}
		\item $f$ is \textit{non-singular on the left} if and only if the corresponding map $F\to\op{Hom}_R(E,R)$ is an isomorphism.
		\item $f$ is \textit{non-singular on the right} if and only if the corresponding map $E\to\op{Hom}_R(F,R)$ is an isomorphism.
	\end{itemize}
\end{definition}
\begin{remark}
	It more or less follows from the definitions that being non-singular (on the left/right) implies being non-degenerate (on the left/right, respectively).
\end{remark}
\begin{remark}
	For finite-dimensional vector spaces over a field, one can see that being non-degenerate is equivalent to being non-singular. However, this is not true in general: it already fails if we pass to infinite-dimensional vector spaces.
\end{remark}
\begin{remark}
	If $f$ is non-singular, then we note that we have a composite isomorphism
	\[L^2(E,F;R)\cong\op{Hom}_R(E,\op{Hom}_R(F,R))\from\op{Hom}_R(E,E)=\op{End}_R(E).\]
	In general, the backwards map is well-defined: it simply sends $A\in\op{End}_R(E)$ the bilinear map $(x,y)\mapsto f(Ax,y)$.
\end{remark}
\begin{definition}[transpose]
	Fix a commutative ring $R$ and a non-singular bilinear form $f\colon E\times F\to R$. We describe a construction of the transpose $\op{End}_R(E)\to\op{End}_R(F)$ in the presence of a non-singular bilinear form $f$. Given $A\in\op{End}_R(E)$, one can define $A^\intercal\in\op{End}_R(F)$ by satisfying the equation
	\[\langle Ax,y\rangle=\langle x,A^\intercal y\rangle.\]
\end{definition}
In particular, we see that the vector $A^\intercal y$ is uniquely defined by the non-singularity of $f$. One can check some basic properties such as the fact that $A^\intercal$ is actually linear and satisfies $(A+B)^\intercal=A^\intercal+B^\intercal$ and $(cA)^\intercal=cA^\intercal$.

\end{document}