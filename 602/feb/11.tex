% !TEX root = ../notes.tex

\documentclass[../notes.tex]{subfiles}

\begin{document}

\section{February 11}

The next homework can be found on pages 567--570; please do exercises 1--6 and 19--22.

\subsection{More on Bilinear Forms}
Euclidean geometry benefits from having access to an orthogonal group. In general, we may be interested in the automorphisms of a particular bilinear form.
\begin{definition}
	Fix a bilinear form $f\colon E\times E\to R$ of an $R$-module $E$. Then an \textit{automorphism of $f$}, denoted $A\in\op{Aut}f$, is an automorphism $A\in\op{Aut}_RE$ such that
	\[f(Ax,Ay)=f(x,y)\]
	for any $x,y\in E$.
\end{definition}
\begin{remark}
	If $E$ is free of finite rank and $f$ is nonsingular, then we can define the transpose $A^\intercal$ of some $A$. Then we see that
	\[f(Ax,Ay)=f(A^\intercal Ax,y),\]
	so we see that $A\in\op{Aut}f$ if and only if $A^\intercal A=\id_E$.
\end{remark}
Let's list a few more adjectives for our bilinear forms.
\begin{definition}
	Fix a commutative ring $R$ and a free module $E$ of finite rank. Further, fix a matrix $M$ and a bilinear form $f$ on $E$.
	\begin{itemize}
		\item We say that $M$ is \textit{symmetric} if and only if $M=M^\intercal$. Similarly, $M$ is \textit{alternating} if and only if $M=-M^\intercal$, and the diagonal entries of $M$ vanish.
		\item We say that $f$ is \textit{symmetric} if and only if $f(x,y)=f(y,x)$ always. Similarly, $f$ is \textit{alternating} if and only if $f(x,x)=0$ always.
	\end{itemize}
\end{definition}
Let's explain the correspondence here. This depends on a map
\[M_n(R)\cong L^2(E),\]
where $n$ is the rank of $E$. In brief, this depends on a choice of basis $B=\{x_1,\ldots,x_n\}$ of $E$, whereupon we see that we can send a matrix $M$ to the bilinear form
\[\langle x,y\rangle\coloneqq x^\intercal My,\]
where $x$ and $y$ are realized as elements of $R^n$ via the basis $B$. Conversely, a bilinear form $f\colon E\times E\to R$ produces a matrix $M$ by letting the coefficient $M_{ij}$ be given by $f(x_i,x_j)$. It is not hard to check that these define inverse maps $M_n(R)\to L^2(E)$ of $R$-modules.
\begin{proposition}
	Fix a free module $E$ of finite rank $n$ over a commutative ring $R$, and let $B$ be a basis. Under the isomorphism $M_n(R)\cong L^2(E)$, a matrix is symmetric or alternating if and only the corresponding bilinear form is symmetric or alternating, respectively.
\end{proposition}
\begin{proof}
	This is direct. For example, one finds that $f$ is symmetric if and only if $f(x_i,x_j)=f(x_j,x_i)$ on a basis, which is equivalent to the corresponding matrix being symmetric. A similar argument works in the alternating case.
\end{proof}

\subsection{Representations of Algebras}
Let $k$ be a commutative ring (sadly), and let $R$ be a $k$-algebra.
\begin{definition}[representation]
	Fix a $k$-algebra $R$. Then a \textit{representation} of $R$ is a homomorphism
	\[\rho\colon R\to\op{End}_k(E)\]
	of $k$-algebras, where $E$ is some $k$-module. Given two representations $\rho_1\colon R\to\op{End}_k(E_1)$ and $\rho_2\colon R\to\op{End}_k(E_2)$, a homomorphism $f\colon\rho_1\to\rho_2$ of representations is a homomorphism $f\colon E_1\to E_2$ such that
	\[f(\rho_1(r)x)=\rho_2(r)f(x)\]
	for all $r\in R$ and $x\in E_1$.
\end{definition}
\begin{remark}
	Given two representations $\rho_1\colon R\to\op{End}_k(E_1)$ and $\rho_2\colon R\to\op{End}_k(E_2)$, we remark that there is a direct sum representation $(\rho_1\oplus\rho_2)\colon R\to\op{End}_k(E_1\oplus E_2)$.
\end{remark}
\begin{example}[principal representation]
	Suppose that $I\subseteq R$ is an ideal. Then multiplication in $R$ defines a representation $\rho\colon R\to\op{End}_k(I)$.
\end{example}
Representations are helpful because they allow us to classify (and compute with) $k$-algebras by placing them inside matrix algebras, and matrix algebras can be studied via the sort of linear algebra we've already discussed.
\begin{remark}
	The presence of the homomorphism $\rho$ allows us to view $E$ as a module over $R$, where the action map $R\to\op{End}(E)$ is given by $\rho$.
\end{remark}
Representation theory may be interested in decomposing $E$ into ``subrepresentations.'' This frequently amounts to finding invariant subspaces, as in the following definition.
\begin{definition}[invariant submodule]
	Fix a commutative ring $k$ and a representation $\rho\colon R\to\op{End}_k(E)$. An \textit{invariant submodule} is a submodule $F\subseteq E$ such that $\im\rho(r)|_F\subseteq F$ for all $r\in R$.
\end{definition}
\begin{remark}
	An invariant submodule $F\subseteq E$ induces an embedding of representations $F\into E$. There is also a way to realize a representation structure on $E/F$ by $\rho(r)\colon(x+F)\mapsto(\rho(r)x+F)$.
\end{remark}
Thus, we hope that the building blocks of our representation theory are those with no interesting invariant submodules.
\begin{definition}[irreducible]
	Fix a commutative ring $k$ and a representation $\rho\colon R\to\op{End}_k(E)$. Then $\rho$ is \textit{simple} or \textit{irreducible} if and only if $E$ is nonzero, and there are no nonzero proper invariant submodules of $E$.
\end{definition}
Sadly, it is not always case that one can write a representation as a sum of irreducible representations, so we introduce a new word.
\begin{definition}[semisimple]
	Fix an algebra $R$ over a commutative ring $k$. Then a representation $\rho\colon R\to\op{End}_k(E)$ is \textit{semisimple} if and only if $E$ is isomorphic to a direct sum of irreducible representations.
\end{definition}
\begin{example}[Maschke]
	Fix a finite group $G$, and consider the $\CC$-algebra $\CC[G]$. Then it turns out that all representations of $\CC[G]$ are semisimple.
\end{example}

\subsection{Representations of the Polynomial Ring}
Fix now a field $k$, and we will work with the commutative $k$-algebra $R\coloneqq k[t]$. Then the data of a representation $\rho\colon k[t]\to\op{End}_k(E)$ amounts to dictating $\rho(t)$, from which the rest of the representation is determined uniquely: given $A\coloneqq\rho(t)$, then the rest of the representation is given by
\[\rho\Bigg(\sum_{i=0}^d c_it^i\Bigg)=\sum_{i=0}^\infty c_iA^i,\]
and it is not hard to check that this definition produces a well-defined representation $\rho$ no matter how $A$ is chosen. Thus, representations of $k[t]$ have equivalent data to a pair $(E,A)$ of a vector space $E$ over $k$ together with an endomorphism $A$ of $E$.

This perspective grants a clean definition of the minimal polynomial.
\begin{definition}[minimal polynomial]
	Fix a finite-dimensional vector space $E$ over $k$. For some $A\in\op{End}_kE$, we let $\rho_A\colon k[t]\to\op{End}_kE$ be the representation with $\rho_A(t)=A$. Then the \textit{minimal polynomial} for $A$ is the monic polynomial $q_A\in k[t]$ generating the ideal $\ker\rho_A\subseteq k[t]$.
\end{definition}
\begin{example} \label{ex:cyclic-kt-module}
	Suppose that there is some $v\in E$ such that the vectors $\{A^iv\}_{i\ge0}$ span $E$. With $d=\dim E$, then one finds that the vectors $\{v,Av,\ldots,A^{d-1}v\}$ should be a basis of $E$: certainly, there is $e$ large enough so that the vectors $\{v,Av,\ldots,A^ev\}$ span $E$, so one can extract a basis as a subset. Now, expanding $A^dv=\sum_{i=0}^{d-1}-a_iA^iv$, we see that the minimal polynomial is $q_A(t)=t^d+a_{d-1}t^{d-1}+\cdots+a_1t+a_0$, and $A$ has the matrix form
	\[A=\begin{bmatrix}
		0 & 0 & 0 & \cdots & 0 & -a_0 \\
		1 & 0 & 0 & \cdots & 0 & -a_1 \\
		0 & 1 & 0 & \cdots & 0 & -a_2 \\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & 0 & -a_{d-2} \\
		0 & 0 & 0 & \cdots & 1 & -a_{d-1}
	\end{bmatrix}.\]
	Additionally, we remark that the map $k[t]\to E$ given by $p(t)\mapsto p(A)v$ is surjective by hypothesis on $E$, so we see that we have an isomorphism $E\cong k[t]/(q_A(t))$ of $k[t]$-modules.
\end{example}
It turns out that all representations of $k[t]$ look like the above, maybe up to decomposition.
\begin{theorem} \label{thm:jordan-normal}
	Fix a finite-dimensional vector space $E$ over a field $k$. Choose $A\in\op{End}_k(E)$. Then is a unique sequence of nonzero, nonconstant, monic polynomials $(q_1,\ldots,q_r)$ such that $q_i\mid q_{i+1}$ for each $i$, and there is an isomorphism of $k[t]$-modules
	\[E\cong\bigoplus_{i=1}^r\frac{k[t]}{(q_i(t))}.\]
	Further, the sequence $(q_1,\ldots,q_r)$ is uniquely determined by the pair $(E,A)$.
\end{theorem}
\begin{proof}
	This is essentially the theory of the Jordan normal form. Alternatively, one can use the theory of finitely generated modules over a principal ideal domain because $k[t]$ is a principal ideal domain, for which we refer to \cite[Theorem~III.7.7]{lang-algebra}. In fact, $k[t]$ is a Euclidean domain, so one is able to give a fairly explicit argument.

	Let's take a moment to explain how the classification of finitely generated modules over a principal ideal domain is used: one then knows that we can write $E$ as
	\[E\cong\bigoplus_{i=1}^r\frac{k[t]}{(q_i(t))}\]
	for some monic or zero polynomials $(q_1,\ldots,q_r)$ satisfying $q_i\mid q_{i+1}$ for each $i$. However, $E$ is finite-dimen\-sional, so we are not allowed to have $q_i=0$ for any $i$ (for then $k[t]$ embeds into $E$, making $\dim_kE=\infty$). The result follows.
\end{proof}
\begin{remark}
	It follows from the statement that $q_r$ is the minimal polynomial of $A$. Indeed, the point is to recognize that $q_1\mid q_2\mid\cdots q_{r-1}\mid q_r$.
\end{remark}
\begin{definition}[invariants]
	Fix a finite-dimensional vector space $E$ over a field $k$. Choose $A\in\op{End}_k(E)$. The sequence of \textit{invariants} of $A$ is the sequence $(q_1,\ldots,q_r)$ constructed in \Cref{thm:jordan-normal}.
\end{definition}
We will show one the homework that the pair $(E,A)$ is essentially determined by the sequence $(q_1,\ldots,q_r)$.

\end{document}