% !TEX root = ../notes.tex

\documentclass[../notes.tex]{subfiles}

\begin{document}

\section{February 13}
Here we go.

\subsection{More on Invariants}
Let's give a few examples of invariants.
\begin{example}
	Consider the case where $A\in\op{End}_k(E)$ is the zero operator. Then $E=k\oplus\cdots\oplus k$ as a $k[t]$-representation, where $k[t]$ acts on $k$ by $t\mapsto0$. We conclude that our sequence of invariants are given by $(q_1,\ldots,q_n)=(0,\ldots,0)$.
\end{example}
Let's give a couple applications of these invariants.
\begin{corollary}
	Fix a field extension $k'/k$, and choose some $A\in M_n(k)$. Suppose the invariants of $A$ split into linear polynomials in $k$. Then the invariants of $A$ as an element of $\op{End}_k(k^n)$ are the same as the invariants of $A$ as an element of $\op{End}_{k'}((k')^n)$.
\end{corollary}
\begin{proof}
	The idea is that the isomorphism
	\[E\cong\bigoplus_{i=1}^r\frac{k[t]}{(q_i(t))}\]
	extends to an isomorphism
	\[E_{k'}\cong\bigoplus_{i=1}^r\frac{k'[t]}{(q_i(t))},\]
	from which the result follows.
\end{proof}
We now turn to the Jordan decomposition.
\begin{theorem} \label{thm:one-jordan-block}
	Fix a finite-dimensional vector space $E$ over a field $k$. Suppose that $A\in\op{End}_k(E)$ has sequence of invariants given by the single polynomial $q(t)=(t-\alpha)^e$. Then $E$ admits a basis over $k$ such that $A$ is the matrix
	\[J_e(\alpha)=\begin{bmatrix}
		\alpha & 0 & 0 & \cdots & 0 & 0 \\
		1 & \alpha & 0 & \cdots & 0 & 0 \\
		0 & 1 & \alpha & \cdots & 0 & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & \alpha & 0 \\
		0 & 0 & 0 & \cdots & 1 & \alpha
	\end{bmatrix}.\]
\end{theorem}
\begin{proof}
	By hypothesis, we have an isomorphism $E\cong k[t]/\left((t-\alpha)^e\right)$ of $k[t]$-modules. Then we fix such an isomorphism, and we let $v\in E$ be the image of $1$ under such an isomorphism. Then we see that
	\[\left\{1,(t-\alpha),(t-\alpha)^2,\ldots,(t-\alpha)^{e-1}\right\}\]
	is a basis of $k[t]/\left((t-\alpha)^e\right)$ (over $k$), so
	\[\left\{v,(A-\alpha)v,(A-\alpha)^2,\ldots,(A-\alpha)^{e-1}v\right\}\]
	is a basis for $E$. One can now check that this is the required basis; for example, $Av=\alpha v+(A-\alpha)v$ explains the first column of the given matrix.
\end{proof}
\begin{notation}
	Fix a field $k$. For $\alpha\in k$ and nonnegative integer $n\ge0$, we define the \textit{Jordan block} $J_n(\alpha)\in M_n(k)$ as the matrix
	\[J_n(\alpha)=\begin{bmatrix}
		\alpha & 0 & 0 & \cdots & 0 & 0 \\
		1 & \alpha & 0 & \cdots & 0 & 0 \\
		0 & 1 & \alpha & \cdots & 0 & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & \alpha & 0 \\
		0 & 0 & 0 & \cdots & 1 & \alpha
	\end{bmatrix}.\]
\end{notation}
\begin{corollary}[Jordan normal form]
	Fix a finite-dimensional vector space $E$ over a field $k$ which is algebraically closed. Fix $A\in\op{End}_k(E)$. Then $E$ admits a basis so that $A$ is the block matrix
	\[\begin{bmatrix}
		J_{n_1}(\alpha_1) \\ & \ddots \\ && J_{n_k}(\alpha_k)
	\end{bmatrix},\]
	where $n_1,\ldots,n_k\ge0$ are nonnegative, and $\alpha_1,\ldots,\alpha_k\in k$ are some elements.
\end{corollary}
\begin{proof}
	Combine \Cref{thm:jordan-normal} with the basis constructed by \Cref{thm:one-jordan-block}.
\end{proof}

\subsection{The Characteristic Polynomial}
We now discuss the characteristic polynomial.
\begin{definition}
	Fix a commutative ring $k$ and matrix $M\in M_n(k)$. Then we define the \textit{characteristic polynomial} $P_M(t)$ as the determinant of the matrix
	\[tI_n-M\in M_n(k[t]),\]
	where $I_n$ is the identity matrix.
\end{definition}
\begin{example}
	If $M=0$, then $P_M(t)=\det tI_n=(t-1)^n$.
\end{example}
\begin{remark}
	If $N\in M_n(k)$ is invertible, then we claim that $P_{N^{-1}MN}(t)=P_M(t)$. Indeed, the inverse of $N$ in $M_n(k)$ explains that $N$ is still invertible in $M_n(k[t])$, so we see that
	\[\det\left(tI_n-N^{-1}MN\right)=\det\left(N^{-1}\right)\det\left(tI_n-M\right)\det(N),\]
	from which the result follows.
\end{remark}
\begin{remark}
	If $\varphi\colon k\to k'$ is a homomorphism of commutative rings, then
	\[P_{\varphi(M)}(t)=\varphi(P_M(t)).\]
\end{remark}
The previous remark allows us to prove the following theorem cleanly.
\begin{theorem}[Cayley--Hamilton]
	Fix a commutative ring $k$. For matrix $M\in M_n(k)$, one has $P_M(M)=0$.
\end{theorem}
\begin{proof}
	Omitted. This is fairly technical, requiring a discussion of the adjugate matrix. We refer to \cite[Theorem~XIV.3.1]{lang-algebra}.
\end{proof}
Here is the chief application of the characteristic polynomial.
\begin{definition}[eigenvalue]
	Fix a vector space $E$ over a field $k$ and an operator $A\in\op{End}_k(E)$. An \textit{eigenvector} of $A$ is a nonzero element $v\in E$ for which there is an \textit{eigenvalue} $\lambda\in k$ such that $Av=\lambda v$.
\end{definition}
\begin{theorem}
	Fix a vector space $E$ over a field $k$ and an operator $A\in\op{End}_k(E)$. Then the eigenvectors of $A$ are exactly the roots of $P_A(t)$ in $k$.
\end{theorem}
\begin{proof}
	Note that $\lambda$ is an eigenvalue if and only if $A-\lambda\id_E$ fails to be invertible, which is equivalent to $\det(A-\lambda{\id_E})=0$, which is equivalent to $P_A(\lambda)=0$.
\end{proof}
While we're here, let's talk a bit more about eigenvectors.
\begin{theorem}
	Fix a vector space $E$ over a field $k$ and an operator $A\in\op{End}_k(E)$. For each $\lambda\in k$, let $E_\lambda\subseteq E$ be the (possibly trivial) subspace of eigenvectors of $E$ with eigenvalue $\lambda$ (together with $0$). Then the spaces $\{E_\lambda:\lambda\in k\}$ are linearly independent.
\end{theorem}
\begin{proof}
	Suppose these are not linearly independent. Then there is a minimal relation among these eigen\-spaces, allowing us to produce a minimal collection of distinct eigenvalues $\{\lambda_1,\ldots,\lambda_n\}$ together with eigenvectors $\{v_1,\ldots,v_n\}\subseteq E$ (with $v_i\in E_{\lambda_i}$ for each $i$) such that
	\[v_1+\cdots+v_n=0.\]
	Note Applying $A$ to this list, we see that
	\[\lambda_1v_1+\cdots+\lambda_nv_n=0,\]
	so we see that
	\[(\lambda_n-\lambda_1)v_1+\cdots+(\lambda_n-\lambda_{n-1})v_{n-1}+\underbrace{(\lambda_n-\lambda_n)}_0v=0.\]
	This is a strictly smaller relation, so we are done.
\end{proof}
\begin{corollary}
	Fix a vector space $E$ over a field $k$ and an operator $A\in\op{End}_k(E)$. Suppose that $A$ has $\dim_kE$ distinct eigenvalues in $k$. Then $E$ admits a basis making $A$ a diagonal matrix.
\end{corollary}
\begin{proof}
	By hypothesis, $P_A(t)$ factors into linear factors, so we produce many distinct eigenvectors, which we now know are linearly independent. This is the needed basis.
\end{proof}
As an aside, we connect the characteristic polynomial with the invariants.
\begin{proposition}
	Fix a vector space $E$ over a field $k$ and an operator $A\in\op{End}_k(E)$. Let $(q_1,\ldots,q_r)$ be the sequence of invariants of $A$. Then
	\[P_A(t)=q_1(t)\cdots q_r(t).\]
\end{proposition}
\begin{proof}
	By decomposing $E$ as a $k[t]$-module, we may assume that $E=k[t]/(q(t))$ for some polynomial $q(t)$. Then the result follows from \Cref{ex:cyclic-kt-module}.
\end{proof}
\begin{remark}
	We can now see that the minimal polynomial $q_r(t)$ and the characteristic polynomial $P_A(t)$ have the same irreducible factors.
\end{remark}
Let's check that the characteristic polynomial is functorial.
\begin{proposition}
	Fix a commutative ring $k$, and choose some $f\in k[t]$. For an $n(\times n)$-matrix $M$, suppose that $P_M(t)=\prod_{i=1}^n(t-\alpha_i)$. Then
	\[P_{f(M)}(t)=\prod_{i=1}^n(t-f(\alpha_i)).\]
\end{proposition}
\begin{proof}
	Over a field, one can base-change to the algebraic closure and then upper-triangularize $M$, from which the result follows by a computation. The general case can be reduced to the field case. Roughly speaking, the above statement can be seen as a statement about an equality of two polynomials in $n^2$ variables in $\ZZ[a_{00},\ldots,a_{nn}]$ (where we view the entries in $M$ as indeterminate elements), and then the equality can be checked after base-changing to the fraction field, where we know the result.
\end{proof}
In the sequel, it will be helpful to know something about additivity of the characteristic polynomial. Let's discuss this.
\begin{theorem} \label{thm:char-poly-ses}
	Fix a commutative ring $k$, and let $E',E,E''$ be free $k$-modules. Suppose that we have endomorphisms $A'\in\op{End}_kE'$ and $A\in\op{End}_kE$ and $A''\in\op{End}_kE''$ fitting into a commutative diagram as follows.
	% https://q.uiver.app/#q=WzAsMTAsWzAsMCwiMCJdLFsxLDAsIkUnIl0sWzIsMCwiRSJdLFszLDAsIkUnJyJdLFs0LDAsIjAiXSxbMCwxLCIwIl0sWzEsMSwiRSciXSxbMiwxLCJFIl0sWzMsMSwiRScnIl0sWzQsMSwiMCJdLFsxLDYsIkEnIl0sWzIsNywiQSJdLFszLDgsIkEnJyJdLFswLDFdLFsxLDJdLFsyLDNdLFszLDRdLFs1LDZdLFs2LDddLFs3LDhdLFs4LDldXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		0 & {E'} & E & {E''} & 0 \\
		0 & {E'} & E & {E''} & 0
		\arrow[from=1-1, to=1-2]
		\arrow[from=1-2, to=1-3]
		\arrow["{A'}", from=1-2, to=2-2]
		\arrow[from=1-3, to=1-4]
		\arrow["A", from=1-3, to=2-3]
		\arrow[from=1-4, to=1-5]
		\arrow["{A''}", from=1-4, to=2-4]
		\arrow[from=2-1, to=2-2]
		\arrow[from=2-2, to=2-3]
		\arrow[from=2-3, to=2-4]
		\arrow[from=2-4, to=2-5]
	\end{tikzcd}\]
	Then $P_A(t)=P_{A'}(t)P_{A''}(t)$.
\end{theorem}
\begin{proof}
	Embed $E'$ into $E$, and we consider $E''$ as the quotient $E/E'$. Then one can find a basis
	\[\{v_1,\ldots,v_{e'},w_1,\ldots,w_{e''}\}\]
	of $E$, where $\{v_1,\ldots,v_{e'}\}\subseteq E'$ is a basis, and the projections $\{w_1+E',\ldots,w_{e''}+E'\}\subseteq E/E'$ is a basis. Then one can write down a ``block upper-triangular'' matrix representation for $A$ by gluing together matrix representations for $A'$ and $A''$: namely, if $M'$ and $M''$ are the representations of $A'$ and $A''$ with respect to the given bases of $E'$ and $E''$ (respectively), from which we see that $A$ admits a matrix representation
	\[\begin{bmatrix}
		M & * \\ & M'
	\end{bmatrix},\]
	where $*$ denotes some coefficients which do not matter. Subtracting $t\id_E$ and taking the determinant completes the proof.
\end{proof}
\begin{remark}
	The professor made a big deal about the fact that having a short exact sequence
	\[0\to E'\to E\to E''\to0\]
	does not necessarily imply that $E\cong E'\oplus E''$. However, this is true for fields $k$. In general, one needs some hypothesis on our modules; for example, this is also true if $E''$ is free.
\end{remark}
As a last application of the characteristic polynomial, we compute the trace and determinant.
\begin{proposition}
	Fix a free module $E$ of finite rank over a ring $k$. Suppose $A\in\op{End}_k(E)$ has characteristic polynomial
	\[P_A(t)=t^n+c_{n-1}t^{n-1}+\cdots+c_1t+c_0.\]
	Then $\tr A=-c_{n-1}$ and $\det A=(-1)^nc_0$.
\end{proposition}
\begin{proof}
	For the determinant, one simply has
	\[c_0=P_A(0)=\det(-A)=(-1)^nc_0.\]
	For the trace, one needs to expand out the summation form of the determinant to compute $\det(tI_n-A)$. The idea is that not many terms in the big sum will actually contain a factor of $t^{n-1}$.
\end{proof}
\begin{remark}
	In the situation of \Cref{thm:char-poly-ses}, we have $\tr(A')+\tr(A'')=\tr (A)$ and $\det(A')\det(A'')=\det (A)$. This follows by using the above computation of $\tr$ and $\det$.
\end{remark}
As a last aside, we remark that \Cref{thm:char-poly-ses} tells us that the characteristic polynomial $A\mapsto P_A$ is a certain map of categories. Fix a ground ring $A$. Let's define the categories.
\begin{itemize}
	\item On one hand, we let $\mc A$ denote the ``Euler--Grothendieck'' category of pairs $(E,A)$ where $E$ is a free $k$-module of finite rank, and $A\in\op{End}_k(E)$. One can think of $\mc A$ as being made of $k[t]$-modules where the underlying $k$-module is free of finite rank.
	\item On the other hand, we consider the multiplicative monoid $k[t]$.
\end{itemize}
Thus, we see that $(E,A)\mapsto P_A$ provides a map from $\mc A$ to the multiplicative monoid. We remark that $P_A$ also factors through short exact sequences as described in \Cref{thm:char-poly-ses}, which one can view in the following sense: let $K(\mc A)$ be the category $\mc A$ where we take the quotient by isomorphisms $(E',A')\oplus(E'',A'')\cong(E,A)$ whenever there is a diagram
% https://q.uiver.app/#q=WzAsMTAsWzAsMCwiMCJdLFsxLDAsIkUnIl0sWzIsMCwiRSJdLFszLDAsIkUnJyJdLFs0LDAsIjAiXSxbMCwxLCIwIl0sWzEsMSwiRSciXSxbMiwxLCJFIl0sWzMsMSwiRScnIl0sWzQsMSwiMCJdLFsxLDYsIkEnIl0sWzIsNywiQSJdLFszLDgsIkEnJyJdLFswLDFdLFsxLDJdLFsyLDNdLFszLDRdLFs1LDZdLFs2LDddLFs3LDhdLFs4LDldXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
\[\begin{tikzcd}
	0 & {E'} & E & {E''} & 0 \\
	0 & {E'} & E & {E''} & 0
	\arrow[from=1-1, to=1-2]
	\arrow[from=1-2, to=1-3]
	\arrow["{A'}", from=1-2, to=2-2]
	\arrow[from=1-3, to=1-4]
	\arrow["A", from=1-3, to=2-3]
	\arrow[from=1-4, to=1-5]
	\arrow["{A''}", from=1-4, to=2-4]
	\arrow[from=2-1, to=2-2]
	\arrow[from=2-2, to=2-3]
	\arrow[from=2-3, to=2-4]
	\arrow[from=2-4, to=2-5]
\end{tikzcd}\]
which commutes. Of course, we also know that we can do something similar for the trace and determinant.

\end{document}