% !TEX root = ../notes.tex

\documentclass[../notes.tex]{subfiles}

\begin{document}

\section{March 4}

I missed one class because I was travelling, and then we missed a week of class because the professor was out. The homework can be found on pages 637--639. Please do the exercises 1, 3, 5(a, b), and 12.

\subsection{Some Spectral Theory}
Let's finish up talking about bilinear forms.
\begin{definition}[unitary]
	Fix a finite-dimensional vector space $E$ over $\CC$, and equip $E$ with a positive-definite Hermitian form $\langle\cdot,\cdot\rangle$. A linear map $U\colon E\to E$ is \textit{unitary} if and only if $U^*=U^{-1}$, or equivalently,
	\[\langle Ux,Uy\rangle=\langle x,y\rangle\]
	for all $x,y\in E$.
\end{definition}
\begin{remark}
	Intuitively, we are asking for $U$ to preserve distances (and angles).
\end{remark}
There is a spectral theorem in this case.
\begin{theorem}[Spectral, unitary]
	Fix a unitary operator $U\colon E\to E$ on a Hermitian vector space $E$ over $\CC$. Then $U$ admits an eigenbasis, and all eigenvalues of $U$ have absolute value $1$.
\end{theorem}
The proof of this result is rather technical, so we will not give it.
\begin{remark}
	The fact that the eigenvalues have absolute value $1$ is purely formal: if $\lambda$ is an eigenvalue with $Uv=\lambda v$ for nonzero $v$, then we see that
	\[\left|\lambda\right|^2\langle v,v\rangle=\langle Uv,Uv\rangle=\langle v,v\rangle,\]
	so $\left|\lambda\right|^2=1$.
\end{remark}
It is sometimes desirable to drop invertibility requirements.
\begin{definition}[semipositive]
	Fix an operator $P\colon E\to E$ on a Hermitian vector space $E$. Then $P$ is \textit{semipositive} if and only if it is Hermitian (i.e., self-adjoint: $P^*=P$) and $\langle Px,x\rangle\ge0$ for all $x\in E$. If equality in $\langle Px,x\rangle\ge0$ holds if and only if $x=0$, then $P$ is \textit{positive-definite}.
\end{definition}
\begin{example}
	For any operator $A\colon E\to E$, we see that $P\coloneqq A^*A$ is semipositive: certainly $P^*=P$, and one has $\langle A^*Ax,x\rangle=\langle Ax,Ax\rangle\ge0$ for all $x\in E$. In fact, if $A$ is invertible, then we see that $P=A^*A$ is actually positive-definite.
\end{example}
This hypothesis is strong enough to admit controlled square roots.
\begin{proposition}
	Fix a semipositive operator $P\colon E\to E$ on a Hermitian vector space $E$. Then $P$ admits a semipositive square root $\sqrt P$. Further, if $P$ is positive-definite, then $\sqrt P$ is also positive-definite.
\end{proposition}
\begin{proof}
	In the semipositive case, one can diagonalize (using the spectral theorem for self-adjoint operators) and then take a square root of all the eigenvalues. The last sentence follows from this construction.
\end{proof}
One also has the following decomposition.
\begin{theorem}
	Fix an invertible linear map $A\colon E\to E$ on a Hermitian vector space $E$. Then $A$ admits a decomposition $A=UP$ where $U$ is unitary and $P$ is positive-definite.
\end{theorem}
\begin{proof}
	Take $P=\sqrt{A^*A}$ and $U=AP^{-1}$. Certainly this gives $A=UP$, and we can check that $P$ is positive-definite by its construction as this square root. Thus, it only remains to check that $U$ is actually unitary, which is a direct computation: we find.
	\begin{align*}
		UU^* &= AP^{-1}P^{-*}A^* \\
		&= AP^{-2}A^* \\
		&= A(A^*A)^{-1}A^* \\
		&= 1,
	\end{align*}
	as required.
\end{proof}
\begin{remark}
	A suitable version of this still holds in the case of Hilbert spaces.
\end{remark}
% \subsection{Symmetric Operators}
As with Hermitian operators, one can tell much the same story over $\RR$.
\begin{definition}[symmetric]
	Fix a vector space $E$ over $\RR$ equipped with a positive-definite symmetric form. Choose an operator $A\colon E\to E$.
	\begin{itemize}
		\item Then $A$ is \textit{symmetric} or \textit{self-adjoint} if and only if $A=A^\intercal$. Because the transpose $A^\intercal$ is defined so that $\langle Ax,y\rangle=\langle x,A^\intercal y\rangle$, this is equivalent to asking for $\langle Ax,y\rangle=\langle x,Ay\rangle$ for all $x,y\in E$.
		\item  We say that $A$ is \textit{orthogonal} if and only if $AA^\intercal=1$. Again, this is equivalent to having $\langle Ax,Ay\rangle=\langle x,y\rangle$ for all $x,y\in E$.
	\end{itemize}
\end{definition}
\begin{example}
	The typical $E$ is $\RR^n$ equipped with the positive-definite symmetric form
	\[\langle x,y\rangle\coloneqq\sum_{i=1}^nx_iy_i.\]
\end{example}
As in the Hermitian case, we have a spectral theorem.
\begin{theorem}[Spectral, symmetric]
	Fix a vector space $E$ over $\RR$ equipped with a positive-definite symmetric form. Then any symmetric operator $A\colon E\to E$ admits an orthonormal basis of eigenvectors.
\end{theorem}
One can prove this as a consequence of the Hermitian spectral theorem, or it can be proved directly by basically looking for maximal axes for the ellipsoid produced by the image of the unit ball under $A$.
\begin{remark} \label{rem:simul-sym-basis}
	One can use this spectral theorem to show that any pair of symmetric positive-definite forms $f$ and $g$ on $E$ admit a common orthonormal basis. Indeed, one can find a symmetric linear map $A$ such that $g(x,y)=f(Ax,y)$, so the required basis arises by diagonalizing $A$.
\end{remark}

\subsection{Alternating Forms}
Here is our definition.
\begin{definition}[alternating]
	Fix a vector space $E$ over a field $k$. Then a bilinear form $f\colon E\times E\to k$ is \textit{alternating} if and only if $f(x,x)=0$ for all $x\in E$.
\end{definition}
\begin{remark}
	Note that having $f(x,x)=0$ for all $x$ implies that $f(x,y)=-f(y,x)$ for all $x,y\in k$, which we see by expanding out $f(x+y,x+y)=0$. In characteristic $2$, the converse is also true: having $f(x,y)=-f(y,x)$ for all $x,y\in k$ implies that $f(x,x)=-f(x,x)$ and hence $f(x,x)=0$ provided that $2\in k^\times$.
\end{remark}
Here are some basic examples of spaces equipped with alternating forms.
\begin{definition}[null]
	Fix a vector space $E$ over a field $k$ equipped with an alternating form. Then $E$ is \textit{null} if and only if the alternating form is $0$.
\end{definition}
\begin{example}[hyperbolic plane]
	If $f$ is a non-degenerate bilinear form on a two-dimensional vector space $E$, then it turns out that one can give $E$ a basis $E\cong k^2$ so that $f$ corresponds to the matrix
	\[\begin{bmatrix}
		0 & 1 \\ -1 & 0
	\end{bmatrix}.\]
	Namely, $f((x_1,x_2),(y_1,y_2))=x_1y_2-x_2y_1$. One can prove this by a direct computation: simply choose two vectors $x,y\in E$ with $f(x,y)=1$ (which can be found because $f$ is non-degenerate and then scaling!). Then we see that $x$ and $y$ are not linearly dependent, so they produce a basis of $E$, and this basis yields the above matrix form for $f$ by a quick computation.
\end{example}
\begin{definition}[hyperbolic]
	Fix a vector space $E$ over a field $k$ equipped with an alternating form. Then $E$ is \textit{hyperbolic} if and only if it is isomorphic to a direct sum of \textit{hyperbolic planes}, where a hyperbolic plane is the unique $2$-dimensional such space with non-degenerate alternating form.
\end{definition}
\begin{remark}
	Note that being hyperbolic implies that the alternating form is non-degenerate.
\end{remark}
It turns out that these examples exhaust all of our spaces.
\begin{proposition} \label{prop:classify-non-degen-alt-forms}
	Fix a vector space $E$ over a field $k$ equipped with a non-degenerate alternating form. Then $E$ is hyperbolic.
\end{proposition}
\begin{proof}
	This can be shown rather explicitly on matrices. Roughly speaking, one gradually applies row operations to a given invertible alternating matrix until it becomes block-diagonal with diagonal blocks given by $\begin{bsmallmatrix}
		0 & 1 \\ -1 & 0
	\end{bsmallmatrix}$.
\end{proof}
\begin{remark}
	Note that \Cref{prop:classify-non-degen-alt-forms} implies that $\dim_k E$ is even, which is not a priori obvious!
\end{remark}
\begin{corollary}
	Fix a vector space $E$ over a field $k$ equipped with an alternating form. Then $E$ is an orthogonal direct sum of a null space and a hyperbolic space.
\end{corollary}
\begin{proof}
	Let $f$ be the alternating form. The exact sequence
	\[0\to\ker f\to E\to E/\ker f\to 0\]
	splits, so one can write $E$ as an orthogonal(!) direct sum of the null space $\ker f$ and the space $E/\ker f$. By construction, $E/\ker f$ is non-degenerate, so it is hyperbolic by \Cref{prop:classify-non-degen-alt-forms}, and we are done.
\end{proof}
This classification theorem is as close to a spectral theorem as we will get. For example, it has some similar applications.
\begin{remark}
	Akin to \Cref{rem:simul-sym-basis}, suppose that we have a finite-dimensional vector space $E$ equip\-ped with a non-degenerate symmetric form $\langle\cdot,\cdot\rangle$ and non-degenerate alternating form $\Omega$. Then one can use a standard form of $\Omega$ to show that $E$ admits a decomposition $E=E_1\oplus E_2$ together with a symmetric automorphism $A$ of $E$ such that
	\[\Omega((x_1,x_2),(y_1,y_2))=\langle Ax_1,y_2\rangle-\langle Ax_2,y_1\rangle\]
	for any $(x_1,x_2),(y_1,y_2)\in E_1\oplus E_2$.
\end{remark}

\subsection{Tensor Products}
We begin by giving the tensor product by universal property.
\begin{definition}[multilinear]
	Fix a commutative ring $R$, and choose $R$-modules $E_1,\ldots,E_n,F$. We recall that an $n$-\textit{multilinear} map $f\colon E_1\times\cdots\times E_n\to F$ is one which is $R$-linear individually in each coordinate. We let $L^n(E_1,\ldots,E_n;F)$ denote the $R$-module of such multilinear maps.
\end{definition}
\begin{remark}
	Fixing $E_1,\ldots,E_n$, we note that $L^n(E_1,\ldots,E_n;-)$ defines a functor $\mathrm{Mod}_R\to\mathrm{Mod}_R$: given a map $\varphi\colon F\to G$, we see that there is an induced map
	\[\begin{array}{cccc}
		L^n\varphi\colon& L^n(E_1,\ldots,E_n;F) &\to& L^n(E_1,\ldots,E_n;G) \\
		& f &\mapsto& \varphi\circ f
	\end{array}\]
	which can be checked to satisfy the required functoriality checks.
\end{remark}
\begin{proposition}
	Fix a commutative ring $R$, and choose $R$-modules $E_1,\ldots,E_n$. Then there is an object $E_1\otimes\cdots\otimes E_n$ together with an $R$-multilinear map $E_1\times\cdots E_n\to E_1\otimes\cdots\otimes E_n$ (unique up to isomorphism) which is universal in the following sense: for any multilinear map $f\colon E_1\times\cdots\times E_n\to F$, there exists a unique linear map $\ov f\colon E_1\otimes\cdots\otimes E_n\to F$ making the following diagram commute.
	% https://q.uiver.app/#q=WzAsMyxbMCwwLCJFXzFcXHRpbWVzXFxjZG90c1xcdGltZXMgRV9uIl0sWzEsMCwiRV8xXFxvdGltZXNcXGNkb3RzXFxvdGltZXMgRV9uIl0sWzEsMSwiRiJdLFswLDIsImYiLDJdLFsxLDIsIlxcb3YgZiJdLFswLDFdXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}[cramped]
		{E_1\times\cdots\times E_n} & {E_1\otimes\cdots\otimes E_n} \\
		& F
		\arrow[from=1-1, to=1-2]
		\arrow["f"', from=1-1, to=2-2]
		\arrow["{\ov f}", from=1-2, to=2-2]
	\end{tikzcd}\]
\end{proposition}
\begin{remark}
	An equivalent way to state this universality is to ask for a natural isomorphism
	\[\op{Hom}_R(E_1\otimes\cdots\otimes E_n,-)\simeq L^n(E_1,\ldots,E_n;-).\]
	For example, the universal multilinear map $E_1\times\cdots\times E_n\to E_1\otimes\cdots\otimes E_n$ arises from plugging in the identity $\id_{E_1\otimes\cdots\otimes E_n}$ into this natural isomorphism.
\end{remark}
We will explain how to prove the proposition next time. We remark that the real difficulty comes from the construction of $E_1\otimes\cdots\otimes E_n$, which is a little tricky.

\end{document}