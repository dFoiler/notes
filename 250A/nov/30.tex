% !TEX root = ../notes.tex













A dying man can do nothing easy.

\subsection{Inverse Galois Problem: Abelian Groups}
Let's continue or discussion of cyclotomic fields. For example, last time we showed that there are infinitely many primes $1\pmod n$ for any positive integer $n,$ and we will use this fact to solve the inverse Galois problem in the abelian case.
\begin{proposition}
	Fix $G$ a finite abelian group. Then we can find a Galois extension $K/\QQ$ such that $\op{Gal}(K/\QQ)\cong G.$
\end{proposition}
\begin{proof}
	As usual, our approach is to find some extension $L/\QQ$ with a homomorphism $\op{Gal}(L/\QQ)\onto G$ and take quotients to finish. The main idea is to see that $G,$ being finite, can be written as
	\[G\cong\prod_{k=1}^n\ZZ/m_k\ZZ\]
	for some positive integers $\{m_k\}_{k=1}^n,$ not necessarily coprime. Setting $N=m_1\cdot m_2\cdot\ldots\cdot m_n$ so that we see
	\[G\cong\prod_{k=1}^n\frac{\ZZ/N\ZZ}{m_k\ZZ/N\ZZ},\]
	so there is a surjection $(\ZZ/N\ZZ)^n\onto G$ given above, so we focus on creating an extension $L/\QQ$ with Galois group such that $\op{Gal}(L/\QQ)\onto(\ZZ/N\ZZ)^n.$ For this, we select $n$ primes $\{q_k\}_{k=1}^n$ such that $q_k\equiv1\pmod N$ and set
	\[M:=q_1\cdot q_2\cdot\ldots\cdot q_n\]
	so that
	\[\op{Gal}(\QQ(\zeta_M)/\QQ)\cong(\ZZ/M\ZZ)^\times\cong\prod_{k=1}^n(\ZZ/q_k\ZZ)^\times\cong\prod_{k=1}^n\ZZ/(q_k-1)\ZZ\]
	which has a surjection
	\[\prod_{k=1}^n\ZZ/(q_k-1)\ZZ\onto\prod_{k=1}^n\frac{\ZZ/(q_k-1)\ZZ}{N\ZZ/(q_k-1)\ZZ}\cong(\ZZ/N\ZZ)^n\onto G.\]
	So, to finish, set $H$ to be the kernel of this surjection $\op{Gal}(L/\QQ)\onto G$ and define $K:=L^H.$ Then we see that $H$ is the kernel, so it is a normal subgroup of $\op{Gal}(L/\QQ),$ and we compute
	\[\op{Gal}(K/\QQ)\cong\frac{\op{Gal}(L/\QQ)}{\op{Gal}(L/K)}=\frac{\op{Gal}(L/\QQ)}H\cong G,\]
	where we are using our discussion of normal subgroups to work this out. This finishes.
\end{proof}
The above statement even has a partial converse.
\begin{theorem}[Kronecker--Weber]
	Any abelian extension $K/\QQ$ is contained in a cyclotomic extension.
\end{theorem}
\begin{proof}
	This is a pretty difficult theorem in number theory, more or less a primer on class field theory. For example, it would make a good capstone for a first course on algebraic number theory. Anyways, we will not prove this here.
\end{proof}
Verifying Kronecker--Weber is not even easy for quadratic extensions, but it is not out of reach. We will do this, for fun.
\begin{exe}[Nir] \label{exe:quadratickw}
	Fix an integer $m.$ Then there exists an integer $n$ such that $\QQ(\sqrt m)\subseteq\QQ(\zeta_n).$
\end{exe}
\begin{proof}
	The approach will be to focus on the case where $m$ is prime and build up from there, so fix $p:=m$ a prime. As a first guess, we check the quadratic subextension of $\QQ(\zeta_p),$ which will almost work. The main idea, now, is to use ``Gauss sums'' as we did \autoref{exe:17gon}. As motivation, we fix
	\[\alpha=\sum_{k=1}^{p-1}\zeta_p^{k^2},\]
	which by \autoref{lem:normalcyclotomic} will generate the subfield of $\QQ(\zeta_p)$ fixed by the squares of $(\ZZ/p\ZZ)^\times,$ which is the index-$2$ subgroup of $(\ZZ/p\ZZ)^\times,$ so $\alpha$ will generate the quadratic subfield of $\QQ(\zeta_p).$
	
	To make $\alpha$ better-behaved, we fix
	\[G:=\frac12\alpha-(-1)=\frac12\alpha-\sum_{k=1}^{p-1}\zeta_p^k=\sum_{k\in\FF_p^\times}\left(\frac kp\right)\zeta_p^k,\]
	where $\left(\frac kp\right)$ is $1$ when $k\pmod p$ is a nonzero square, $-1$ when $k\pmod p$ is not a square, and $0$ when $k\equiv0.$ (This last equality above is checked by casework on the various $k.$) Now, $G$ will still generate the quadratic subfield of $\QQ(\zeta_p),$ and $G$ is better-behaved because
	\[G^2=\left(\sum_{k\in\FF_p^\times}\left(\frac kp\right)\zeta_p^k\right)\left(\sum_{\ell\in\FF_p^\times}\left(\frac\ell p\right)\zeta_p^\ell\right)=\sum_{k,\ell\in\FF_p^\times}\left(\frac{k\ell}p\right)\zeta_p^{k+\ell}.\]
	Setting $x$ so that $\ell=kx,$ we see
	\[G^2=\sum_{k,x\in\FF_p^\times}\left(\frac{k^2x}p\right)\left(\zeta_p^k\right)^{1+x}=\sum_{x=1}^{p-1}\left(\frac xp\right)\left[\sum_{k=1}^{p-1}\left(\zeta_p^k\right)^{1+x}\right]\]
	When $x\ne p-1,$ the inner sum will cycle as the sum of all the primitive $p$th roots of unity and produce $-1.$ When $x=p-1,$ we simply accumulate $p-1,$ which in total gives
	\[G^2=\sum_{x=1}^{p-2}\left(\frac xp\right)(-1)+\left(\frac{-1}p\right)(p-1).\]
	Adding back in the $x=p-1$ term to the sum, we see $\sum_{x=1}^{p-1}\left(\frac xp\right)=0$ because $\left(\frac{\cdot}p\right):\FF_p^\times\to\CC$ is a nontrivial character. So this term will cancel, leaving us with $G^2=\left(\frac{-1}p\right)p.$

	It's not too hard to show that $G^2=(-1)^{(p-1)/2}p,$ in fact, but we will not do this. The point is that $G=\pm\sqrt{\pm p},$ for some particular choice of signs. So we see that, the quadratic subfield of $\QQ(\zeta_p)$ is either $\QQ(\sqrt p)$ or $\QQ(\sqrt{-p})=\QQ(i\sqrt p),$ so in either case, $\QQ(i,\zeta_p)$ will surely contain $\sqrt p.$

	So to finish, fix $m$ a general integer. Then, $m$ has a prime factorization, and let $p_1,p_2,\ldots,p_n$ be the prime factors of $m.$ Then, adding in a sign $\varepsilon\in\{\pm1\}$ for $m,$ we see
	\[\sqrt m=\sqrt\varepsilon\prod_{k=1}^n(\sqrt{p_k})^{\nu_{p_k}(m)}\in\QQ(i,\zeta_{p_1},\zeta_{p_2},\ldots,\zeta{p_n})\subseteq\QQ(\zeta_{4p_1p_2\cdots p_n}),\]
	finishing.
\end{proof}
% In office hours, Professor Borcherds mentioned that, even though there might be lots of ways to embed a given extension into a cyclic one, making these extensions fully compatible is somewhat difficult. To be explicit, we have the following statement.
% \begin{proposition}[Nir]
% 	Fix $p$ an odd prime. There exists a unique sequence of (Galois) extensions
% 	\[\QQ\subseteq L_1\subseteq L_2\subseteq L_3\subseteq\cdots\]
% 	such that $\op{Gal}(L_\bullet/\QQ)\cong\ZZ/p^\bullet\ZZ.$
% \end{proposition}
% This statement is also true for $p=2$ but a bit more annoying to prove.
% \begin{proof}
% 	We outline. Note that taking the union of all the $L_\bullet$ means we are essentially asking for a profinite extension $L_\infty/\QQ$ such that
% 	\[\op{Gal}(L_\infty/\QQ)\cong\limit\ZZ/p^\bullet\ZZ\cong\ZZ_p.\]
% 	But by the Kronecker--Weber theorem, the absolute abelian extension of $\QQ$ has Galois group
% 	\[\op{Gal}(\QQ^{\op{ab}}/\QQ)\cong\limit\op{Gal}(\QQ(\zeta_n)/\QQ)\cong\limit(\ZZ/n\ZZ)^\times\cong\prod_\ell\ZZ_\ell^\times.\]
% 	Now, we notice that, for $\ell$ odd, $\ZZ_\ell^\times\cong\FF_\ell^\times\times(1+\ell\ZZ_\ell)$ and the group $1+\ell\ZZ_\ell$ under multiplication is simply isomorphic to $\ZZ_p.$ With this in mind, there is essentially one way to continuously surject
% 	\[\op{Gal}(\QQ^{\op{ab}}/\QQ)\onto\ZZ_p,\]
% 	and this is by taking $\op{Gal}(\QQ^{\op{ab}}/\QQ)\onto\ZZ_p^\times\onto\ZZ_p,$ and this unique projection gives us our unique $L_\infty.$ Technically we must show some annoying result like the lack of a continuous surjection $\ZZ_\ell\onto\ZZ_p$ for $\ell\ne p,$ but I would rather not care for this outline.
% \end{proof}
% \begin{remark}
% 	 For example, it is somewhat difficult to find the smallest $n$ for which
% 	\[\QQ(\sqrt7)\subseteq\QQ(\zeta_n).\]
% 	For example, $n=7$ does not work (but $2\cdot7$ does).
% \end{remark}

\subsection{Inverse Galois Problem: Solvable Groups}
Here is an extension.
\begin{theorem}[Shafarevich]
	Any finite, solvable group is the Galois group of some Galois extension $K/\QQ.$
\end{theorem}
This theorem is hard to prove, and let's give some reasoning why.
\begin{proof}[Not a proof]
	Here is one attempt: suppose that we are given a short exact sequence
	\[1\to A\to B\to C\to 1\]
	such that $A$ and $C$ are Galois groups, and we want to show it for $B$ as well. For this, we might want to take an extension $K/\QQ$ with Galois group $C$ and then try to extend this up to an extension $L\supseteq K$ with Galois group $\op{Gal}(L/\QQ)=B.$
\end{proof}
However, this approach might not even be possible. Namely, if we have a short exact sequence
\[1\to A\to B\to C\to 1\]
such that $C$ is the Galois group of $L/\QQ,$ there need not exist an extension $M\supseteq L$ with $M/\QQ$ having group $B.$ For example, consider the short exact sequence
\[0\to\ZZ/2\ZZ\to\ZZ/4\ZZ\to\ZZ/2\ZZ\to0.\]
Here, $\ZZ/2\ZZ$ is a quadratic extension, say $\QQ(\sqrt n),$ and we would like to find $L$ such that $\op{Gal}(L/\QQ)\cong\ZZ/4\ZZ.$ The following lemma provides a physical obstruction for this.
\begin{lemma}
	Fix $n$ a squarefree integer. Then, if there exists a Galois extension $L/\QQ$ containing $\QQ(\sqrt n)$ such that $\op{Gal}(L/\QQ)\cong\ZZ/4\ZZ,$ then there exist rationals $b,c\in\QQ$ such that $b^2-nc^2=-1.$
\end{lemma}
\begin{proof}
	Well, suppose that we can do this so that $\op{Gal}(L/\QQ)=\langle\sigma\rangle\cong\ZZ/4\ZZ.$ Then $\QQ(\sqrt n)$ is going to be the field fixed by $\left\langle\sigma^2\right\rangle$ because $\left\langle\sigma^2\right\rangle$ is the only index-$2$ subgroup of $\langle\sigma\rangle.$

	The main idea, now, is that $L/\QQ$ is a cyclic extension, and even though $\QQ$ does not contain all fourth roots of unity, we can try to imitate our Kummer theory.

	Much of Kummer theory was concerned with verifying the existence of the correct generating element, but ours will be somewhat easy to find: the extension $L/\QQ(\sqrt n)$ must have some $a\in\QQ(\sqrt n)$ such that $\alpha:=\sqrt a$ has $L=\QQ(\sqrt n)(\alpha).$
	
	Now, our magic word continues to be ``eigenvalue.'' We do still have $\sigma^4\alpha=\alpha,$ but this no longer helps us because, when we write $\lambda:=\sigma\alpha/\alpha$ so that
	\[\sigma\alpha=\lambda\alpha,\]
	we might not have $\lambda\in\QQ.$ However, looking at $L/\QQ(\sqrt n),$ we see that
	\[\sigma^2\alpha=-\alpha\]
	because $\alpha$ was chosen an element with eigenvalue $-1,$ and $-1$ will certainly be fixed by $\sigma.$ So to salvage our approach, we notice
	\[\sigma^2\left(\frac{\sigma\alpha}\alpha\right)=\frac{-\sigma\alpha}{-\alpha}=\frac{\sigma\alpha}\alpha,\]
	so $\lambda\in\QQ(\sqrt n),$ which is the next best thing.
	
	Continuing to salvage our Kummer theory, instead of using $\sigma\alpha=\lambda\alpha$ to pin down $\lambda,$ we notice that $\sigma^2\alpha=-\alpha$ will give
	\[-\alpha=\sigma^2\alpha=\sigma(\lambda\alpha)=\sigma\lambda\cdot\sigma\alpha=(\sigma\lambda\cdot\lambda)\alpha.\]
	Thus, $\lambda\cdot\sigma\lambda=-1.$

	To finish, we set $\lambda:=b+c\sqrt n$ with $b,c\in\QQ.$ Then $\sigma\sqrt n\ne\sqrt n$ (because $\sigma\notin\op{Gal}(L/\QQ(\sqrt n))=\left\langle\sigma^2\right\rangle$), so $\sigma\sqrt n=-\sqrt n$ instead, implying
	\[\lambda\cdot\sigma\lambda=\left(b+c\sqrt n\right)\left(b-c\sqrt n\right)=b^2-nc^2.\]
	Thus, $b^2-nc^2=-1.$ This finishes.
\end{proof}
\begin{remark}[Nir]
	The condition $b^2-nc^2=-1$ is actually effective: take $\alpha:=\sqrt{b-c\sqrt n},$ which has minimal polynomial $f(X)=\left(X^2-b\right)^2-nc^2.$ We can see that $\left(b+c\sqrt n\right)\sqrt{b-c\sqrt n}=\pm\sqrt{-b+c\sqrt n},$ which is another root of $f(X),$ so there is indeed an isomorphism $\sigma$ defined as the composite
	\[L=\QQ({\textstyle\sqrt{b-c\sqrt n}})\cong\frac{\QQ[X]}{(f(X))}\cong\QQ(\pm\textstyle\sqrt{-b+c\sqrt n})=\QQ\left((b+c\sqrt n)\textstyle\sqrt{b-c\sqrt n}\right)=L,\]
	sending $\sigma\alpha=(b+c\sqrt n)\alpha.$ We can check by hand $\sigma$ has order four, which finishes verifying that $L/\QQ$ has $\#\op{Aut}(L/\QQ)\ge4,$ so $L/\QQ$ is Galois with $\op{Gal}(L/\QQ)=\langle\sigma\rangle\cong\ZZ/4\ZZ.$
\end{remark}
And let's see this in action.
\begin{example}
	If $n<0,$ then $b^2-cn^2\ge b^2\ge0>-1,$ so there exists no Galois extension $L/\QQ$ containing $\QQ(\sqrt n)$ such that $\op{Gal}(L/\QQ)\cong\ZZ/4\ZZ.$ To be explicit, $\QQ(i)$ cannot be embedded into a $\ZZ/4\ZZ$-extension.
\end{example}
\begin{example}
	Our work with Gauss sums in \autoref{exe:quadratickw} shows that $\sqrt 5=\pm G\in\QQ(\zeta_5),$ so we can embed $\QQ(\sqrt 5)\subseteq\QQ(\zeta_5).$ And indeed, we see that $2^2-5\cdot1^2=-1$ is a solution, as needed.
\end{example}
\begin{example}
	Taking $n=3,$ we are trying to solve $b^2-3c^2=-1$ for $b,c\in\QQ,$ which becomes
	\[x^2+z^2=3y^2\tag{$*$}\]
	for some $x,y,z\in\ZZ$ and $z\ne0$ after setting $z$ to be the least common multiple of the denominators of $b$ and $c.$ However, by checking$\pmod3,$ we can see $z^2\equiv-x^2$ forces $z$ and $x$ to be divisible by $3,$ which forces $y$ to be divisible by $3,$ meaning that solutions to $(*)$ can be transformed under
	\[(x,y,z)\mapsto(x/3,y/3,z/3).\]
	However, this implies that $z$ is divisible by infinitely many powers of $3,$ which does not make sense.
\end{example}
\begin{remark}
	The actual proof of {Shafarevich}'s theorem involves a lot of back-tracking to attempt to find bigger extensions, finding that sometimes we cannot do this, and then going backwards.
\end{remark}

\subsection{Division Rings}
Let's continue with our applications of cyclotomic polynomials. Here is the object we will focus on.
\begin{definition}[Division ring]
	A \textit{division ring} $K$ is a ring $K$ with identity (but not necessarily commutative) such that every element has a left and right multiplicative inverse.
\end{definition}
\begin{example}
	Any field is a division ring.
\end{example}
\begin{example}
	The quaternions $\mathbb H$ make a noncommutative division ring.
\end{example}
Here is our theorem.
\begin{theorem}[Wedderburn]
	Any finite division ring is a field.
\end{theorem}
\begin{proof}
	The proof is in two steps: write down the conjugacy class equation and then apply some theory of cyclotomic polynomials.
	% Fix $K$ our finite division ring, and checking the kernel of $\ZZ\to K$ (by $1\mapsto 1_K$), there exists a prime $p$ for which $\FF_p\subseteq K.$ Additionally, 
	We set
	\[\FF_q:=\{a\in K:ab=ba\text{ for each }b\in K\}.\]
	It is not too hard to see that $\FF_q$ contains $1$ and $0,$ is closed under addition ($(a_1+a_2)b=a_1b+a_2b=ba_1+ba_2=b(a_1+a_2)$), and closed is closed under multiplication ($a_1a_2b=ba_1a_2$), so in fact $\FF_q$ is a subring of $K$ where multiplication commutes and hence is a field. To finish our set-up, we note that $K$ is an abelian group containing $\FF_q$ and hence will behave like an $\FF_q$-vector space. In particular, $\#K=q^{[K:\FF_q]}$ is a power of $q$; set $n:=[K:\FF_q].$

	Our end goal is to show that $K=\FF_q.$ We now apply our first trick, writing down the conjugacy class equation of $K^\times$ as
	\[\#K^\times=\sum_{\mathcal C\subseteq K^\times}\#\mathcal C=\#\FF_q^\times+\sum_{\substack{[x]\subseteq K^\times\\\#[x]>1}}\#[x],\]
	where our sums are over distinct conjugacy classes. Now, by the Orbit-stabilizer theorem, the size of $[x]$ for some $a\in K$ is equal to $\#K^\times/\op{Stab}(x),$ where
	\[\op{Stab}(x)=\left\{a\in K^\times:ax=xa\right\}.\]
	Essentially the same checks as before verify that $C(x):=\{a\in K:ax=xa\}$ contains $1$ and $0$ and is closed under addition and multiplication, so we won't write them out. So $C(x)$ is a subring of $K$ and in particular also a $\FF_q$-vector space, so it will have size $q^{[C(x):\FF_1]}.$ So, throwing out $0$ as appropriate,
	\[\#K^\times=(q-1)+\sum_{\substack{[x]\subseteq K^\times\\\#[x]>1}}\frac{q^n-1}{q^{[C(x):\FF_q]}-1}.\tag{$*$}\]
	Now, $\#[x]>1$ becomes $[C(a):\FF_q]<[K:\FF_q]=n.$

	We now apply our second trick, bringing in cyclotomic polynomials. Namely, look at $\Phi_n(q).$ This will certainly divide $q^n-1,$ and it will certainly divide $\frac{q^n-1}{q^k-1}$ for each $k=[C(x):\FF_q]<n$ from the conjugacy classes, so $(*)$ implies that
	\[\Phi_n(q)\mid q-1.\]
	But this will force $n=1$ because $|\Phi_n(q)|>|q-1|$ for $n\ne1.$ Namely,
	\[\left|\Phi_n(q)\right|=\left|\prod_{k\in(\ZZ/n\ZZ)^\times}(q-\zeta_n^k)\right|=\prod_{k\in(\ZZ/n\ZZ)^\times}\left|q-\zeta_n^k\right|\ge|q-1|^{\varphi(n)}\ge|q-1|.\]
	Here, the bound $\left|q-\zeta_n^k\right|\ge|q-1|$ comes essentially because $q\ge1,$ coming from the following picture.
	\begin{center}
		\begin{asy}
			unitsize(1.5cm);
			draw((-1.25,0)--(3,0));
			draw((0,-1.25)--(0,1.25));
			draw(dir(360/7) -- (2.7,0));
			draw(circle((0,0),1));
			dot("$\zeta_n^k$", dir(360/7), dir(360/7));
			dot("$1$", (1,0), NW);
			dot("$q$", (2.7,0), S);
		\end{asy}
	\end{center}
	Getting this bound rigorously would be annoying, but the main point is that $\left|q-e^{i\theta}\right|^2=(q-\cos\theta)^2+(\sin\theta)^2$ achieves its minimum when we simultaneously minimize $(q-\cos\theta)^2$ to $(q-1)^2$ and $(\sin\theta)^2$ to $0.$

	Anyways, if we are to have $|\Phi_n(q)|\le|q-1|,$ then we must be hitting all of our equality cases, so in particular $\zeta_n^k=1$ for each $k\in(\ZZ/n\ZZ)^\times$ so that $n=1$ is forced. Thus, $K$ is one-dimensional over $\FF_q,$ so $K=\FF_q.$ This finishes.
\end{proof}
As an application of {Wedderburn}'s theorem to projective geometry, we state Pappus's theorem.
\begin{theorem}[Pappus]
	Fix $X:=\mathbb P^n(F)$ some $n$-dimensional projective space over a field $F.$ Then given three collinear points $P_1,P_2,P_3\in X$ and three more collinear points $Q_1,Q_2,Q_3\in X,$ define the point $R_k$ as being the intersection of $\overline{P_{k+1}Q_{k+2}}$ and $\overline{P_{k+2}Q_{k+1}},$ where the indices are taken$\pmod3.$ Then $R_1,R_2,R_3$ are collinear.
\end{theorem}
\begin{proof}
	As a bad proof, one can give coordinates to everything and solve for $R_1,R_2,R_3$ explicitly in terms of the coordinates of everything else and then find the line explicitly containing all three points. Roughly speaking, because the statement is true, this approach must work.
\end{proof}
Here is the mandatory image for this theorem.
\begin{center}
	\begin{asy}
		unitsize(1cm);
		pair p1=(1,0), p2=(3,0), p3=(4.1415926,0);
		pair q1=(0,2), q2=(1.5,2.5), q3=(4.5,3.5);
		pair r1=(1.235,1.176), r2=(2.023,1.023), r3=(3.329,0.769);
		draw(p1--p3); draw(q1--q3);
		draw(p1--q2, dotted); draw(p1--q3, dotted);
		draw(p2--q1, dotted); draw(p2--q3, dotted);
		draw(p3--q1, dotted); draw(p3--q2, dotted);
		draw(r1--r3, red);
		dot("$P_1$", p1, S);
		dot("$P_2$", p2, S);
		dot("$P_3$", p3, S);
		dot("$Q_1$", q1, dir(180-63));
		dot("$Q_2$", q2, dir(180-63));
		dot("$Q_3$", q3, dir(180-63));
		dot("$R_1$", r1, W);
		dot("$R_2$", r2, N);
		dot("$R_3$", r3, E);
	\end{asy}
\end{center}
To see the importance of Pappus's theorem, we note that the following is true (without proof).
\begin{theorem}
	Fix $X:=\mathbb P^n(R)$ some $n$-dimensional projective space over a division ring $R.$ Then Pappus's theorem holds if and only if $R$ is a field.
\end{theorem}
\begin{proof}
	The backwards direction was given above. As for the forwards direction, one can imagine choosing our six points in a particular convenient way and then writing out $R_1,R_2,R_3$ so that being collinear depends on a particular application of the commutativity of multiplication. I'm not sure how to write this out, but I also don't care very much.
\end{proof}
The point of bringing in Wedderburn's theorem is that it tells us Pappus's theorem will hold whenever $X$ is finite because this forces $R$ to be finite, hence forcing $R$ to be a field.
\begin{remark}
	It feels as if there ought to be a geometric/combinatorial proof that Pappus's theorem holds whenever $X$ is finite, but I think Professor Borcherds said that no such easy proof is known.
\end{remark}
As an aside, we note that the set of finite-dimensional division algebras over a field forms a group.
\begin{definition}[Brauer]
	Roughly speaking, the \textit{Brauer group} of a field $k$ consists of equivalence classes of finite-dimensional division algebras $[D]$ over $k,$ where the group law is given by
	\[[D]*[E]=[F],\]
	where $D\otimes E\cong F^{n\times n}.$
\end{definition}
This group law is strange, but it works.

\subsection{Determinants}
We are going to quickly talk about the determinant and trace of a linear transformation to later talk about the norm and trace of an element.
\begin{definition}[Determinants for \texorpdfstring{$\RR$}{R}]
	Fix $V$ a finite-dimensional $\RR$-vector space with a linear transformation $T:V\to V.$ The amount that $T$ ``multiplies volumes'' is $\det V,$ up to sign.
\end{definition}
Observe that the map taking $T$ to the amount $T$ scales volumes by induces a map
\[d:\op{Hom}(V,V)\to\RR_{\ge0}.\]
As some examples of what this can do, we see that applying one transformation $T_1$ and then another $T_2$ will cause the scaling to compound, so
\[d(T_1T_2)=d(T_1)d(T_2).\]
Additionally, it is not too hard to show that
\[d\begin{bmatrix}
	1 & \bullet & \bullet & \cdots & \bullet & \bullet \\
	& 1 & \bullet & \cdots & \bullet & \bullet\\
	& & 1 & \cdots & \bullet & \bullet\\
	& &   & \ddots & \vdots & \vdots \\
	&   &        &        & 1      & \bullet \\
	&   &        &        &        & 1
\end{bmatrix}=1 \tag{1}\label{eq:det1}\]
by more or less using ``base times height''-type arguments. (Here blank spaces are $0,$ and $\bullet$ are generic elements.) Additionally, we can see that
\[d\begin{bmatrix}
	\lambda_1 \\
	& \lambda_2 \\
	& & \ddots \\
	& & & \lambda_n
\end{bmatrix}=\prod_{k=1}^n|\lambda_k|\]
because the linear transformation sends the unit cube to a $|\lambda_1|\times\cdots\times|\lambda_n|$ box. However, we know that the actual determinant has
\[\det\begin{bmatrix}
	\lambda_1 \\
	& \lambda_2 \\
	& & \ddots \\
	& & & \lambda_n
\end{bmatrix}=\prod_{k=1}^n\lambda_k. \tag{2}\label{eq:det2}\]
So the difference between $d$ and $\det$ is a possible sign in our volume. We might want to consider signed volumes or something to fix this, but oftentimes we do actually want to consider volume changes, in which case we do need to keep track of the absolute value.
\begin{remark}
	The sign of $\det T$ is whether $T$ preserves ``parity.'' Intuitively, $\det T=1$ means that the action of $T$ requires some kind of reflection. Rigorously, it is not a bad idea to define ``rotations'' as linear transformations $T$ with $\det T=1,$ especially in more esoteric spaces.
\end{remark}
We would like to extend this definition to general fields. Here are a few ways we can do this.
\begin{definition}[Determinants, I]
	Fix $k$ a field. Then we simply define the \textit{determinant} of a matrix in $k^{n\times n}$ by
	\[\det\begin{bmatrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{n1} & \cdots & a_{nn}
	\end{bmatrix}:=\sum_{\sigma\in S_n}(\op{sgn}\sigma)a_{1,\sigma1}\cdot a_{2,\sigma2}\cdot\ldots\cdot a_{n,\sigma n}.\]
\end{definition}
Then from this definition we can show that $\det$ is multiplicative and that $\det$ satisfies \hyperref[eq:det1]{(1)} and \hyperref[eq:det2]{(2)}.
\begin{warn}
	The above sum is a really terrible way to evaluate determinants because the number of terms is $n!.$ In practice, one should use Gaussian elimination, which requires merely $n^3$ operations.
\end{warn}
Here is another definition, which is more coordinate-free.
\begin{definition}[Determinants, II]
	Fix $k$ a field and $V$ an $n$-dimensional vector space. Then, given $T:V\to V$ a linear transformation, $\det T$ is the amount $T$ multiplies the one-dimensional vector space $\Lambda^n(V).$
\end{definition}
What is $\Lambda^n(V)$? As a start, we define the symmetric algebra $S(V).$
\begin{definition}[Symmetric algebra]
	Fix $V$ a $k$-vector space. Given a positive integer $n,$ we define the \textit{$n$th symmetric algebra} $S^n(V)$ as $V^{\otimes n}$ modded out by the relation
	\[\cdots\otimes a\otimes b\otimes\cdots=\cdots\otimes b\otimes a\otimes\cdots.\]
	Formally, $V^{\otimes n}$ has an $S_n$-action $S_n\to\op{Aut}V^{\otimes n}$ by permuting the coordinates, so we define $S^n(V)$ as $V^{\otimes n}$ modded out by this action. Then the \textit{symmetric algebra} is defined as
	\[S(V):=\bigoplus_{n=1}^\infty S^n(V).\]
\end{definition}
We will not actually check that $S^n(V)$ is a $k$-algebra, but it is. The main point is that ``modding out by the $S_n$-action'' is really modding out by the subspace generated by the elements
\[\left\{\sigma v-\tau v:\sigma,\tau\in S_n\text{ and }v\in V^{\otimes n}\right\}.\]
Anyways, this definition/intuition can be moved to the slightly more complicated $\Lambda^n(V).$
\begin{definition}[Exterior algebra]
	Fix $V$ a $k$-vector space. Given a positive integer $n,$ we define the \textit{$n$th symmetric algebra} $\Lambda^n(V)$ as $V^{\otimes n}$ modded out by the relation
	\[\cdots\otimes a\otimes b\otimes\cdots=-(\cdots\otimes b\otimes a\otimes\cdots).\]
	Formally, $V^{\otimes n}$ has an $S_n$-action $S_n\to\op{Aut}V^{\otimes n}$ by
	\[\sigma(v_1\otimes\cdots\otimes v_n)=(\op{sgn}\sigma)(v_{\sigma1}\otimes\cdots\otimes v_{\sigma n})m\]
	so we define $S^n(V)$ as $V^{\otimes n}$ modded out by this action. Then the \textit{exterior algebra} is defined as
	\[\Lambda(V):=\bigoplus_{n=1}^\infty\Lambda^n(V).\]
\end{definition}
Elements of $\Lambda^n(V)$ are usually denoted by $v_1\wedge\cdots v_n$ for $\{v_k\}_{k=1}^n\subseteq V.$ Again, we won't actually check that $\Lambda^n(V)$ is a $k$-algebra, mostly because I don't see a way to do this which avoids pain.

To talk about $\Lambda^n(V)$ more concretely, let's give it a basis. Well, give $V$ a basis $\{b_k\}_{k=1}^n,$ and we claim that the set of elements
\[b_{k_1}\wedge\cdots\wedge b_{k_n}\]
such that $k_1<\cdots<k_n.$ To see that these elements span, we see that fully expanding some generic element $v_1\wedge\cdots\wedge v_n$ along the basis and fully distributing along the tensor product, we see that at least the elements
\[b_{k_1}\wedge\cdots\wedge b_{k_n},\]
with no extra constraint on the $k_\bullet,$ will span. However, some permutation $\sigma\in S_n$ will be able to force $\sigma(k_1)\le\cdots\le\sigma(k_n),$ and we see
\[b_{k_1}\wedge\cdots\wedge b_{k_n}=(\op{sgn}\sigma)b_{\sigma k_1}\wedge\cdots\wedge b_{\sigma k_n},\]
so we are allowed to force our basis elements to have $k_1\le\cdots\le k_n.$ Further, we note that, if $k_i=k_j$f for $i\ne j,$ then applying the transposition $(i,j)$ will preserve $b_{k_1}\wedge\cdots\wedge b_{k_n}$ while adding a sign, forcing
\[b_{k_1}\wedge\cdots\wedge b_{k_n}=0.\]
Well, we can throw these elements out too, so we find that we can also force $k_i\ne k_j$ for $i\ne j.$
\begin{remark}[Nir]
	I am not recording here a proof that this basis set is actually linearly independent because I don't think one was given in class, and it seems somewhat removed from the class. The sufficiently inclined can read the check in \href{https://math.stackexchange.com/a/53562/869257}{this MathExchange post}.
\end{remark}
Anyways, we see that if $V$ is also $n$-dimensional, then $\Lambda^n(V)$ has basis consisting of the single element
\[b_1\wedge\cdots\wedge b_n,\]
and surely we can track how much a linear transformation scales a one-dimensional subspace. Explicitly, we are defining $\det T$ by
\[(Tb_1\wedge\cdots\wedge Tb_n)=(\det T)(b_1\wedge\cdots\wedge b_n).\]
It is somewhat believable that this is indeed equal to the symmetric sum definition: if we have
\[T=\begin{bmatrix}
	a_{11} & \cdots & a_{1n} \\
	\vdots & \ddots & \vdots \\
	a_{n1} & \cdots & a_{nn}
\end{bmatrix}\qquad\text{so that}\qquad Tb_j=\sum_{i=1}^na_{ij}b_i.\]
In particular, fully expanding $Tb_1\wedge\cdots\wedge Tb_n$ gives terms of the form
\[a_{i_11}b_{i_1}\wedge\cdots\wedge a_{i_nn}b_{i_n}=(a_{i_11}\cdot\ldots\cdot a_{i_nn})(b_{i_1}\wedge\cdots\wedge b_{i_n}),\]
where the $i_\bullet$ range over any function $\{1,\ldots,n\}\to\{1,\ldots,n\}.$ Note that we can still force the $i_\bullet$ to a permutation of $\{1,\ldots,n\}$ because $i_x=i_y$ would cause the entire term to vanish. But then rearranging the $i_\bullet$ into $b_1\wedge\cdots\wedge b_n$ adds a sign corresponding to the permutation $i_\bullet,$ which gives exactly the sum we want.

\subsection{Trace}
Let's quickly review the trace. This is defined as follows.
\begin{definition}[Trace]
	Fix $k$ a field. Then we define the trace of a matrix in $k^{n\times n}$ by
	\[\tr \begin{bmatrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{n1} & \cdots & a_{nn}
	\end{bmatrix}:=\sum_{i=1}^na_{ii}.\]
\end{definition}
\begin{remark}[Nir]
	As an example of some results we get immediately from the definition are that
	\[\tr(cM)=c\tr M\qquad\text{and}\qquad\tr(M_1+M_2)=\tr M_1+\tr M_2\]
	for $c\in k$ and matrices $M,M_1,M_2\in k^{n\times n}$( or linear transformations in $\op{End}(V)$ for some $k$-vector space $V$). Indeed, the left result comes from writing out $cM,$ and the right result comes from writing out $M_1+M_2.$
\end{remark}
\begin{idea}
	The trace is, more or less, the derivative of the determinant.
\end{idea}
More precisely, we can show the following.
\begin{exe} \label{exe:ddxdet}
	Fix $k$ a field and $A\in k^{n\times n}.$ Then we have that, for $\varepsilon>0$ small,
	\[\det(I+\varepsilon A)=1+\varepsilon\tr A+O\left(\varepsilon^2\right).\]
\end{exe}
\begin{proof}
	In practice, we set $\varepsilon$ to be a transcendental element so that $\det(I+\varepsilon A)$ is a polynomial in $\varepsilon.$ Set
	\[A:=\begin{bmatrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{n1} & \cdots & a_{nn}
	\end{bmatrix}\]
	so that $(I+\varepsilon A)_{ij}=1_{i=j}+\varepsilon a_{ij}.$ Namely, we find that
	\[\det(I+\varepsilon A)=\sum_{\sigma\in S_n}(\op{sgn}\sigma)\prod_{i=1}^n(1_{i=\sigma i}+\varepsilon a_{i,\sigma i})\in k[\varepsilon].\]
	The constant term of this polynomial in $\varepsilon$ will come from setting $\varepsilon=0,$ which we can see gives $\det(I+0A)=1.$

	So it remains to study the linear term. Fixing some $\sigma$ for now, we see that the term
	\[\prod_{i=1}^n(1_{i=\sigma i}+\varepsilon a_{i,\sigma i})\]
	will be able to give us a linear term if we pick $(n-1)$ of the $1_{i=\sigma i}$ terms and one of the $\varepsilon a_{i,\sigma i}.$
	
	But we see now that if $i=\sigma i$ is triggered for $(n-1)$ values of $i,$ then we must have $\sigma=\id,$ so this occurs only once, and our linear term is
	\[\sum_{i=1}^n\varepsilon a_{i,\sigma i}=\varepsilon\tr A,\]
	which gives us what we wanted.
\end{proof}
We also have the following almost ``almost homomorphic'' law.
\begin{prop} \label{prop:traceprod}
	Fix $k$ a field and $A,B\in k^{n\times n}.$ Then $\tr (AB)=\tr (BA).$
\end{prop}
\begin{proof}
	We do this by direct computation. Namely, set
	\[A=\begin{bmatrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{n1} & \cdots & a_{nn}
	\end{bmatrix}\qquad\text{and}\qquad B=\begin{bmatrix}
		b_{11} & \cdots & b_{1n} \\
		\vdots & \ddots & \vdots \\
		b_{n1} & \cdots & b_{nn}
	\end{bmatrix}.\]
	Then we see that, for indices $x,z$ we have
	\[(AB)_{xz}=\sum_{y=1}^na_{xy}b_{yz}\qquad\text{and}\qquad(BA)_{xz}=\sum_{y=1}^nb_{xy}a_{yz}\]
	so that
	\[(AB)_{xx}=\sum_{y=1}^na_{xy}b_{yx}\qquad\text{and}\qquad(BA)_{xx}=\sum_{y=1}^nb_{xy}a_{yx}.\]
	Namely, we find that
	\[\tr (AB)=\sum_{x=1}^n(AB)_{xx}=\sum_{x,y=1}^na_{xy}b_{yx}=\sum_{x,y=1}^nb_{yx}a_{xy}=\sum_{y=1}^n(BA)_{yy}=\tr (BA),\]
	which is what we wanted.
\end{proof}
\begin{remark}[Nir]
	The above two results give a coordinate-free way to define the trace. On one hand, \autoref{prop:traceprod} implies that, under a coordinate change matrix $S,$ we have
	\[\tr \left(SAS^{-1}\right)=\tr \left(AS^{-1}S\right)=\tr (A),\]
	so the trace is invariant under change of basis. To fully remove the coordinates, \autoref{exe:ddxdet} implies that, given any finite-dimensional $k$-vector space $V,$ we can define $\tr A$ for $A\in\op{Hom}(V,V)$ as the linear term of the polynomial $\det(I+\varepsilon A)\in k[\varepsilon].$
\end{remark}
Another more coordinate-free view of the trace and determinant is by eigenvalues.
\begin{prop}
	Fix $V$ a finite-dimensional $k$-vector space and $A\in\op{Hom}(V,V)$ a diagonalizable matrix with eigenvalues $\{\lambda_k\}_{k=1}^n.$ Then
	\[\det (A)=\prod_{k=1}^n\lambda_k\qquad\text{and}\qquad\tr (A)=\sum_{k=1}^n\lambda_k.\]
\end{prop}
\begin{proof}
	Using an eigenbasis for $A,$ we may write
	\[A=\begin{bmatrix}
		\lambda_1 \\
		& \lambda_2 \\
		& & \ddots \\
		& & & \lambda_n
	\end{bmatrix}.\]
	From here we can directly compute $\tr (A)$ and $\det (A)$ to get the result.
\end{proof}
\begin{remark}
	One might imagine that we could look at other elementary symmetric polynomials of the eigenvalues, but they are not homomorphisms in general.
\end{remark}
Let's give a quick application of the trace.
\begin{theorem}[Heisenberg commutation relations]
	Fix $V$ a finite-dimensional $k$-vector space, where $k$ is a field of characteristic $0.$ Then if there are linear transformations $A,B\in\op{Hom}(V,V)$ such that
	\[AB-BA=I,\]
	then $V=\{0\}.$
\end{theorem}
\begin{proof}
	Taking the trace of both sides of our equation, we find that
	\[0=\tr (AB)-\tr (BA)=\tr I=\dim V.\]
	We have to be somewhat careful because $\dim V\in\NN,$ but the above equation takes place in $k,$ so $\dim V=0$ will really only assert that $\op{char}k\mid\dim V.$ But in the case where $\op{char}k=0,$ this does force $\dim V=0,$ so $V$ is the zero space.
\end{proof}
\begin{example}
	The requirement that $V$ be finite-dimensional is necessary: if we take $V=\CC[X]$ as a $\CC$-vector space, then we can consider the linear transformations $D:f\mapsto\frac d{dX}f$ and $\mu_X:f\mapsto Xf.$ There are infinite-dimensional vector spaces; e.g., take $V=\CC[x]$ and $A$ to be the derivative and $B$ to be multiplication by $x.$ Then, for any $f(X)\in V,$ we have
	\[(D\mu_X-\mu_XD)(f(X))=\frac d{dX}(Xf(X))-X\frac d{dX}f(X)=f(X)+Xf'(X)-Xf'(X)=f(X),\]
	so indeed, $D\mu_X-\mu_XD=\id.$
\end{example}
\begin{example}[Nir]
	The requirement that $\op{char}k=0$ is also necessary. Otherwise, the trace condition merely gives $\op{char}k\mid\dim V,$ for which there are examples of $A$ and $B.$ For example, in $k=\FF_2,$
	\[\begin{bmatrix}
		0 & 1 \\
		0 & 0
	\end{bmatrix}\begin{bmatrix}
		0 & 0 \\
		1 & 0
	\end{bmatrix}-\begin{bmatrix}
		0 & 0 \\
		1 & 0
	\end{bmatrix}\begin{bmatrix}
		0 & 1 \\
		0 & 0
	\end{bmatrix}=\begin{bmatrix}
		1 & 0 \\
		0 & 0
	\end{bmatrix}-\begin{bmatrix}
		0 & 0 \\
		0 & 1
	\end{bmatrix}=\begin{bmatrix}
		1 & 0 \\
		0 & 1
	\end{bmatrix},\]
	where we have used the fact that $1=-1$ in $\FF_2.$
\end{example}
\begin{remark}
	Apparently $p$-adic string theory exists.
\end{remark}

\subsection{Norm and Trace}
We have the following definition.
\begin{definition}[Norm and trace]
	Fix $L/K$ a finite extension of fields. Fix $\alpha\in L$ and view $\mu_\alpha:x\mapsto \alpha x$ as a linear transformation $L\to L,$ where we view $L$ as a $K$-vector space.
	\begin{listalph}
		\item The \textit{norm} of $\alpha$ is $\op N^L_K(\alpha):=\det(x\mapsto \alpha x).$ 
		\item The \textit{trace} of $\alpha$ is $\op T^L_K(\alpha):=\tr (x\mapsto \alpha x).$
	\end{listalph}
	When the extension $L/K$ is clear, we will abbreviate $\op N^L_K$ to $\op N$ and $\op T^L_K$ to $\op T.$
\end{definition}
\begin{warn}
	Professor Borcherds would like you to ignore Lang's definition of the norm and trace because it is somewhat complicated, for example doing different cases based on separability.
\end{warn}
And of course, here is a good example to keep track of.
\begin{ex}
	Fix $\CC/\RR$ or extension with $\alpha:=x+yi\in\CC.$ Using $\{1,i\}$ as our basis of $\CC/\RR,$ we see that our multiplication by $\alpha$ sends $(1,0)\mapsto(x,y)$ and $(0,1)\mapsto(-y,x),$ so we get the matrix
	\[\begin{bmatrix}
		x & -y \\
		y & x
	\end{bmatrix}.\]
	In particular, we can compute $\op T(\alpha)=2x$ and $\op N(\alpha)=x^2+y^2.$ To connect this to more familiar functions, we see $\op T(\alpha)=2\op{Re}\alpha$ and $\op N(\alpha)=|\alpha|^2.$
\end{ex}
\begin{remark}[Nir]
	This doesn't show up anywhere below, so we state it now: $\op N$ is multiplicative, and $\op T$ is additive. Indeed, fixing our finite extension $L/K$ and $\alpha_1,\alpha_2\in L,$ we find that
	\[\op T(\alpha_1+\alpha_2)=\tr(x\mapsto(\alpha_1+\alpha_2)x)=\tr\big((x\mapsto\alpha_1x)+(x\mapsto\alpha_2x)\big)=\tr(x\mapsto\alpha_1x)+\tr(x\mapsto\alpha_2x),\]
	which is $\op T(\alpha_1)+\op T(\alpha_2).$ Similarly,
	\[\op N(\alpha_1\alpha_2)=\det(x\mapsto\alpha_1\alpha_2x)=\det\big((x\mapsto\alpha_1x)\circ(x\mapsto\alpha_2x)\big)=\det(x\mapsto\alpha_1x)\det(x\mapsto\alpha_2x),\]
	which is again the needed $\op N(\alpha_1)\op N(\alpha_2).$
\end{remark}
Here is a useful way to compute the trace and norm, and it will be a precursor to the rest of our discussion.
\begin{prop} \label{prop:computetrace}
	Suppose that $L=K(\alpha)$ is a finite extension of degree $n:=[L:K]$ such that $\alpha$ has minimal polynomial
	\[f(X)=\prod_{k=1}^n(X-\alpha_k)\in\overline K[X].\]
	Then
	\[\op T(\alpha)=\sum_{k=1}^n\alpha_k\qquad\text{and}\qquad\op N(\alpha)=\prod_{k=1}^n\alpha_k.\]
	In other words, $\op T(\alpha)$ is the sum of the conjugates of $\alpha,$ and $\op N(\alpha)$ is the product of the conjugates.
\end{prop}
\begin{proof}
	The main idea here is that $L=K(\alpha)$ lets us write our linear transformation $x\mapsto \alpha x$ in our power basis
	\[\left\{1,\alpha,\alpha^2,\ldots,\alpha^{n-1}\right\}.\]
	Letting our minimal polynomial $f(X)\in K[X]$ be
	\[f(X)=\sum_{k=0}^na_kX^k\in K[X],\]
	where we force $a_n=1,$ we see that $x\mapsto \alpha x$ can be defined by sending the basis vectors $\alpha^k\mapsto\alpha^{k+1}$ for $0\le k\le n-2$ and
	\[\alpha^{n-1}\mapsto\alpha^n=\sum_{k=0}^{n-1}(-a_k)\alpha^k.\] 
	Namely, $x\mapsto \alpha x$ looks like the matrix
	\[\begin{bmatrix}
		0 &   &        &        & -a_0   \\
		1 & 0 &        &        & -a_1   \\
		  & 1 & 0      &        & -a_2   \\
		  &   & \ddots & \ddots & \vdots \\
		  &   &        & 1      & -a_{n-1}
	\end{bmatrix}.\]
	We immediately see that the trace is $-a_{n-1},$ which is
	\[\op T(\alpha)=-a_{n-1}=\sum_{k=1}^n\alpha_k\]
	by Vieta's formulae. Similarly, for the norm, we see that we can bubble-sort the top row of this matrix to the bottom with $(n-1)$ swaps and thus introducing $(n-1)$ signs, meaning we want the determinant of the matrix
	\[(-1)^{n-1}\begin{bmatrix}
		1 &  &        &        & -a_1   \\
		  & 1 &       &        & -a_2   \\
		  &   & \ddots &  & \vdots \\
		  &   &        & 1      & -a_{n-1} \\
		  &   &        &        & -a_0.
	\end{bmatrix}.\]
	But now this matrix is upper-triangular and hence the determinant we want is $(-1)^{n-1}(-a_0)=(-1)^na_n,$ which is
	\[\op N(\alpha)=(-1)^na_n=\prod_{k=1}^n\alpha_k,\]
	again using Vieta's formulae.
\end{proof}
\begin{remark}[Nir]
	A more coordinate-free to get this result is to actually go compute the characteristic polynomial of $\mu_\alpha:x\mapsto \alpha x$ and find that it is actually $f(X).$ We outline this. Note $f(X)$ is irreducible over $K[X]$ and has the right degree, so it suffices to check $f(\mu_\alpha)=0$ by the Cayley--Hamilton theorem. But $\mu_\bullet:K\to\op{End}_K(K(\alpha))$ is actually a ring homomorphism, so we see $f(\mu_\alpha)=\mu_{f(\alpha)}=\mu_0=0.$
\end{remark}

\subsection{Norms and Traces in Towers}
More generally, we have the following.
\begin{proposition} \label{prop:computenormtrace}
	Fix $L/K$ a finite extension with $\alpha\in L.$ Then fix $\alpha$ with minimal polynomial
	\[f(X)=\prod_{k=1}^n(X-\alpha_k)\in\overline K[X],\]
	where $n=[K(\alpha):K].$ Then
	\[\op T^L_K(\alpha)=[L:K(\alpha)]\sum_{k=1}^n\alpha_k\qquad\text{and}\qquad\op N^L_K(\alpha)=\left(\prod_{k=1}^n\alpha_k\right)^{[L:K(\alpha)]}.\]
\end{proposition}
\begin{proof}
	Once we note that we have the chain $K\subseteq K(\alpha)\subseteq L,$ it suffices to note the previous proposition gives
	\[\op T^{K(\alpha)}_K(\alpha)=\sum_{k=1}^n\alpha_k\qquad\text{and}\qquad\op N^{K(\alpha)}_K(\alpha)=\prod_{k=1}^n\alpha_k\]
	and then finish by applying the tower law in the next proposition.
\end{proof}
\begin{remark}[Nir]
	We have had to be quite careful above, both in statement and proof because we are not assuming that the $\alpha_\bullet$ are distinct, as might not be the case in inseparable extensions.
\end{remark}
Here is the aforementioned tower law.
\begin{proposition} \label{prop:babytowerlaw}
	Give a chain of fields $K\subseteq L\subseteq M$ and $\alpha\in L,$ we have the tower law
	\[\op N^M_K(\alpha)=\left(\op N^L_K\alpha\right)^{[M:L]}\qquad\text{and}\qquad\op T^M_K(\alpha)=[M:L]\op T^L_K(\alpha).\]
\end{proposition}
\begin{proof}
	We essentially work with the following tower of fields.
	% https://q.uiver.app/?q=WzAsMyxbMCwyLCJLIl0sWzAsMSwiSyhcXGFscGhhKSJdLFswLDAsIkwiXSxbMCwxLCIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzEsMiwiIiwwLHsic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==
	\[\begin{tikzcd}
		M \\
		L \\
		K
		\arrow[no head, from=3-1, to=2-1]
		\arrow[no head, from=2-1, to=1-1]
	\end{tikzcd}\]
	Essentially what is going on, now, is that $\mu_\alpha:x\mapsto \alpha x$ is rather controlled on $L$ because $\mu_\alpha$ is a multiplication by a constant in the ground field. To be explicit, fix $\{v_\ell\}_{\ell=0}^{m-1}$ a basis of $M$ as a $L$-vector space. Then we can write
	\[M=\bigoplus_{\ell=0}^{m-1}Lv_\ell.\]
	Now, the linear transformation $\mu_\alpha:x\mapsto \alpha x$ preserves each of these subspaces $Lv_\ell$ because $\alpha\in L,$ so we can decompose $\mu_\alpha:x\mapsto \alpha x$ as a direct sum of its action on each of these subspaces $Lv_\ell.$

	To finish, we note that $Lv_\ell\cong L$ by division by $v_\ell,$ and the action of $\mu_\alpha$ on $Lv_\ell$ commutes with this isomorphism; in other words, the following diagram commutes because multiplication commutes.\footnote{More rigorously, what is going on is that we are expanding our $\mu_\alpha$ along the basis $\{v_\ell w_\bullet\},$ where $\{w_\bullet\}$ is some fixed basis of $L,$ and we find that the matrix is the same as if we had just acted on $\{w_\bullet\}$ to begin with.}
	% https://q.uiver.app/?q=WzAsNCxbMCwwLCJLKFxcYWxwaGEpdl9cXGVsbCJdLFswLDEsIksoXFxhbHBoYSl2X1xcZWxsIl0sWzEsMCwiSyhcXGFscGhhKSJdLFsxLDEsIksoXFxhbHBoYSkiXSxbMCwxLCJcXG11X1xcYWxwaGEiLDJdLFswLDIsIlxcY29uZyIsMyx7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsxLDMsIlxcY29uZyIsMSx7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6Im5vbmUifSwiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsyLDMsIlxcbXVfXFxhbHBoYSJdXQ==
	\[\begin{tikzcd}
		{Lv_\ell} & {L} \\
		{Lv_\ell} & {L}
		\arrow["{\mu_\alpha|_{Lv_\ell}}"', from=1-1, to=2-1]
		\arrow["\cong"{marking}, draw=none, from=1-1, to=1-2]
		\arrow["\cong"{description}, draw=none, from=2-1, to=2-2]
		\arrow["{\mu_\alpha|_L}", from=1-2, to=2-2]
	\end{tikzcd}\]
	So we can compute the determinant and trace of $\mu_\alpha$ as it behaves on $L$ instead of $Lv_\ell,$ which we have already studied in the previous proposition. Indeed, we find
	\[\op T^M_K(\alpha)=\tr\mu_\alpha=\sum_{\ell=0}^{m-1}\tr(\mu_\alpha|_{Lv_\ell})=\sum_{\ell=0}^{m-1}\tr(\mu_\alpha|_L)=[M:L]\op T^L_K(\alpha),\]
	where we used the previous proposition in the last inequality. Similarly,
	\[\op N^M_K(\alpha)=\det\mu_\alpha=\prod_{\ell=0}^{m-1}\det(\mu_\alpha|_{Lv_\ell})=\prod_{\ell=0}^{m-1}\det(\mu_\alpha|_L)=\left(\op N^L_K\alpha\right)^{[M:L]},\]
	which is what we wanted.
\end{proof}
This more or less lets us define an ``absolute'' trace and norm.
\begin{definition}[Reduced trace and norm]
	Fix $L/K$ a finite extension with $\alpha\in L.$ Then we define the \textit{reduced trace} as $\frac1{[L:K]}\op T^L_K(\alpha)$ and the \textit{reduced norm} is $\op N^L_K(\alpha)^{1/[L:K]}.$
\end{definition}
Both of these definitions have problems: if $K$ has characteristic dividing $[L:K],$ then the reduced trace doesn't exist. If $K$ does not have enough roots of unity, we might not be able to take the $1/[L:K]$ powers.

But given that the reduced trace and norm always actually exist and are well-defined, we can show that there are independent of the field $L.$ Indeed, suppose that $\alpha\in L_1,L_2$ where $L_1,L_2$ are finite extensions of $K.$ Then we know that
\[\frac1{[L_1:K]}\op T^{L_1}_K(\alpha)=\frac1{[L_1:K][L_2L_1:L_1]}\op T^{L_1L_2}_K(\alpha)=\frac1{[L_1L_2:K]}\op T^{L_1L_2}_K(\alpha)\]
where we have applied the tower law. But now $\frac1{[L_1L_2:K]}\op T^{L_1L_2}_K(\alpha)$ is symmetric in $L_1$ and $L_2,$ so the above must also be equal to $\frac1{[L_2:K]}\op T^{L_2}_K(\alpha),$ as needed.

Similarly,\footnote{The roots do not necessarily make sense, but as long as they are defined in some way which is compatible with all of the other roots, we should be safe. I am not going to write this out.}
\[(\op N^{L_1}_K(\alpha))^{1/[L_1:K]}=\op N^{L_1L_2}_K(\alpha)^{1/([L_1:K][L_1L_2:L_1])}=\op N^{L_1L_2}_K(\alpha)^{1/[L_1L_2:K]}\]
where we again have applied the tower law. But now $\op N^{L_1L_2}_K(\alpha)^{1/[L_1L_2:K]}$ is symmetric in $L_1$ and $L_2,$ so the above must also be equal to $\op N^{L_2}_K(\alpha)^{1/[L_2:K]},$ as needed.

Anyways, for Galois extensions, much of our story with the minimal polynomial collapses nicely.
\begin{proposition} \label{lem:galoisnormtrace}
	Fix $L/K$ a separable extension and let $G$ be the set of embeddings $L\into\overline K.$ Then, for $\alpha\in L,$ we have that
	\[\op T(\alpha)=\sum_{\sigma\in G}\sigma(\alpha)\qquad\text{and}\qquad\op N(\alpha)=\prod_{\sigma\in G}\sigma(\alpha).\]
	In particular, when $L/K$ is Galois, $G=\op{Gal}(L/K).$
\end{proposition}
\begin{proof}
	Let $f(X)$ be the monic minimal polynomial $\alpha\in K[X]$ so that
	\[f(X)=\prod_{k=1}^n(X-\alpha_k)\]
	for some elements $\alpha_1,\ldots,\alpha_n\in\overline K$ and $n=[K(\alpha):K].$ But $L/K$ is separable, so these elements are distinct, and $L/K$ is normal, so all of these elements live in $L$ because $\alpha$ is one root of $f(X).$

	The main point is to understand the multiset $\{\sigma\alpha\}_{\sigma\in G}$ in order to compute the sum and product in the statement. To start, we notice that the embedding $K(\alpha)\into\overline K$ by
	\[K(\alpha)\cong\frac{K[X]}{f(X)}\cong K(\alpha_\bullet)\subseteq\overline K\]
	can be extended to a full embedding in $G$ with the property that $\alpha\mapsto\alpha_\bullet.$ In particular, for each $\sigma\in G,$ the fact $\sigma$ is an embedding forces $\sigma\alpha\in\{\alpha_k\}_{k=1}^n,$ and we can indeed hit every $\alpha_\bullet$ as described above, so the orbit of $\alpha$ under $G$ is the entire $\{\alpha_k\}_{k=1}^n.$

	We now directly focus on the multiset
	\[\{\sigma\alpha\}_{\sigma\in G}.\]
	Using the bijection $G/\op{Stab}(\alpha)$ to the set $\{\sigma\alpha\}_{\sigma\in G},$ we see that each $\sigma\alpha$ in the above multiset will be hit $\#\op{Stab}(\alpha)$ times, which by the Orbit-stabilizer theorem is $\#G/n.$ Namely, when we write
	\[\sum_{\sigma\in G}\sigma\alpha,\]
	this sum is hitting each $\alpha_\bullet$ exactly $\#G/n=[L:K]/[K(\alpha):K]=[L:K(\alpha)]$ times (note $\#G=[L:K]$ because $L/K$ is separable!), so it is
	\[\sum_{\sigma\in G}\sigma\alpha=[L:K(\alpha)]\sum_{k=1}^n\alpha_k=\op T^L_K(\alpha).\]
	Similarly, we find that
	\[\prod_{\sigma\in G}\sigma\alpha=\left(\prod_{k=1}^n\alpha_k\right)^{[L:K(\alpha)]}=\op N^L_K(\alpha),\]
	which is what we wanted.
\end{proof}

\subsection{Algebraic Integers}
Here is something algebraic number theorists care about.
\begin{definition}[Algebraic integers]
	Given a finite extension $K/\QQ,$ the \textit{algebraic integers} $\mathcal O_K\subseteq K$ are those which are the roots of some monic polynomial.
\end{definition}
We will take on faith that the sum and product of two algebraic integers is another algebraic integer; showing this is approximately the same as showing that the sum and product of two algebraic numbers is an algebraic number while keeping track of the integral condition. But this is surprisingly technical and somewhat removed from the course, so we will not say more.

Anyways, the point of is that the set of algebraic integers in $K$ forms a ring, once we add in the fact that $0$ and $1$ are algebraic integers. Here are some examples.
\begin{ex}
	The algebraic integers of $\QQ(\sqrt{-3})$ is not $\ZZ[\sqrt{-3}].$ The main point is that
	\[\frac{1+\sqrt{-3}}2\]
	is an algebraic integer because it is the root of the polynomial $x^2+x+1=0.$ 
\end{ex}
\begin{example}
	The algebraic integers of $\QQ(\sqrt{-2})$ are $\ZZ[\sqrt{-2}].$ We will show this shortly.
\end{example}
The following example justifies the name ``integer.''
\begin{exe}
	The set of algebraic integers in $\QQ$ is exactly $\mathcal O_\QQ=\ZZ.$
\end{exe}
\begin{proof}
	Certainly each $n\in\ZZ$ is an algebraic integer because $n$ is the root of the monic polynomial $X-n\in\ZZ[X].$ So in one direction, $\ZZ\subseteq\mathcal O_\QQ.$
	
	Conversely, we show $\frac pq\in\QQ$ with $\gcd(p,q)=1$ is an algebraic integer forces $q=\pm1$ and thus $\frac pq\in\ZZ.$ Indeed, if $p/q$ is an algebraic integer, then find our monic polynomial
	\[f(X):=X^n+\sum_{k=0}^{n-1}a_kX^k\in\ZZ[X]\]
	such that $f(p/q)=0$ so that
	\[0=q^nf(p/q)=p^n+\sum_{k=0}^{n-1}a_kp^kq^{n-k}.\]
	Now, we see $q$ divides the left-hand side as well as big sum, so $q$ divides $p^n$ as well. But then $q$ divides $\gcd\left(p^n,q\right)=1,$ forcing $q=\pm1.$
\end{proof}
As promised, let's compute the algebraic integers of a quadratic extension.
\begin{exe}
	Fix $m$ a squarefree integer not equal to $1,$ and set $K:=\QQ(\sqrt m).$ Then
	\[\mathcal O_K=\begin{cases}
		\ZZ[\sqrt m] & m\equiv2,3\pmod4, \\
		\ZZ\left[\frac{1+\sqrt m}2\right] & m\equiv1\pmod4.
	\end{cases}\]
	Note $m\equiv0\pmod4$ never occurs because $m$ is squarefree.
\end{exe}
\begin{proof}
	Note that all integers will be algebraic integers because $n\in\ZZ$ is the root of the monic polynomial $X-n\in\ZZ[X].$ Additionally, we see that $\sqrt m$ will be a root of $X^2-m\in\ZZ[X],$ so $\sqrt m\in\mathcal O_K.$ When $m\equiv1\pmod4,$ we also have that $\frac{1+\sqrt m}2$ is a root of
	\[X^2-X+\frac{1-m}4\in\ZZ[X],\]
	so $\frac{1+\sqrt m}2$ is also an algebraic integer. All this is to say that $\ZZ[\sqrt m]\subseteq\mathcal O_K$ always, and when $m\equiv1\pmod4,$ we also have $\ZZ\left[\frac{1+\sqrt m}2\right]\subseteq\mathcal O_K.$

	It remains to show the equalities. Suppose $a+b\sqrt m\in\QQ(\sqrt m)$ is an algebraic integer. Well, note that the Galois conjugate $a-b\sqrt m$ will be a root of the same polynomial as $a+b\sqrt m,$ so in particular it will be monic with integer coefficients, so $a-b\sqrt m$ will be an algebraic integer.

	So the key trick is that we know
	\[\op T(a+b\sqrt m)=2a\qquad\text{and}\qquad\op N(a+b\sqrt m)=a^2-bm^2.\]
	will also be algebraic integers. But then $2a\in\QQ$ is an algebraic integer, so $2a,a^2-mb^2\in\ZZ$ is forced. Further, we notice that
	\[4\left(a^2-mb^2\right)-(2a)^2=-m(2b)^2\]
	must also be an integer, so the same logic in the previous case forces $2b\in\ZZ.$ So, setting $a=2c$ and $b=2d,$ we see that $\frac{c+d\sqrt m}2$ is our algebraic integer, and checking its norm now, we see that
	\[\frac{c^2-md^2}4\in\ZZ,\]
	so $c^2\equiv md^2\pmod4.$ We now have two cases.
	\begin{itemize}
		\item If $m\equiv1\pmod4,$ then we notice that $c^2\equiv d^2\pmod4.$ Reducing to$\pmod2$ somewhat brazenly, we see that
		\[c\equiv c^2\equiv d^2\equiv d\pmod2,\]
		so $c\equiv d\pmod2$ is forced. However, this means that we can write
		\[\frac{c+d\sqrt m}2=\frac{c-d}2+d\cdot\frac{1+\sqrt m}2\in\ZZ\left[\frac{1+\sqrt m}2\right],\]
		which is what we wanted.

		\item Otherwise, $m\equiv2,3\pmod4,$ and here $c^2\equiv md^2\pmod4$ forces $c,d\equiv0\pmod2.$ Explicitly, if $d$ is even, then $c^2\equiv0\pmod2$ forces $c$ even. And if $d$ is odd, then we have $c^2\equiv m\pmod4,$ which has no solutions.
		
		Thus,
		\[\frac{c+d\sqrt m}2=\frac c2+\frac d2\sqrt m\in\ZZ[\sqrt m],\]
		which is again what we wanted.
		\qedhere
	\end{itemize}
\end{proof}
% \begin{exe}
% 	We find the algebraic integers of $\QQ(\sqrt{-3}).$
% \end{exe}
% \begin{proof}
% 	We can check that the sum and product of two algebraic integers is an algebraic integer. The proof is somewhat similar to the proof that algebraic numbers form a field. So, given that $a+b\sqrt{-3}$ is an algebraic integer, we see that $a-b\sqrt{-3}$ will also be an algebraic integer because it is a Galois conjugate. Then we can compute the norm and trace as
% 	\[\op T(a+b\sqrt{-3})=2a\qquad\text{and}\qquad\op T(a+b\sqrt{-3})=a^2+3b^2.\]
% 	We see that these must be algebraic integers as well as rational numbers, from which it follows that they are integers.

% 	For example, we see that $a$ need not be integers because $a$ can be a half-integer. But if we then subtract out all halves by $\frac12+\frac12\sqrt{-3}$ from $a+b\sqrt{-3},$ we may assume that $a$ is actually $0.$ But then we see
% 	\[3b^2\in\ZZ,\]
% 	so it follows that $b$ is an integer. Thus, we find that
% 	\[\mathcal O_{\QQ(\sqrt{-3})}=\ZZ\left[\frac{1+\sqrt{-3}}2\right]+\ZZ\left[\sqrt{-3}\right]=\ZZ\left[\frac{1+\sqrt{-3}}2\right],\]
% 	which is what we wanted.
% \end{proof}
% We leave it as an exercise to this for general $\QQ(\sqrt m).$ This is not as obvious as it looks.

\subsection{Trace Form}
To close off, we note that the trace of an extension $L/K$ induces a symmetric bilinear form
\[\langle\alpha,\beta\rangle:=\op T(\alpha\beta).\]
To be explicit, we see that $\langle\alpha,\beta\rangle=\langle\beta,\alpha\rangle$ because multiplication commutes; the additivity of the trace gives $\langle\alpha_1+\alpha_2,\beta\rangle=\langle\alpha_1,\beta\rangle+\langle\alpha_2,\beta\rangle$; and the fact
\[\op T(c\alpha)=\tr\big(c\cdot(x\mapsto\alpha x\big)=c\tr(x\mapsto\alpha x)=c\op T(\alpha),\]
for any $c\in K,$ gives $\langle c\alpha,\beta\rangle=c\langle\alpha,\beta\rangle.$

Here is our favorite example.
\begin{example}
	Fix $\CC/\RR$ to be our finite extension. Then
	\[\langle a+bi,c+di\rangle=\tr ((a+bi)(c+di))=2(ab-cd),\]
	where the minus sign is somewhat important.
\end{example}
We hope that this bilinear form is non-degenerate. We will talk about this next lecture.