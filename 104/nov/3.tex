\documentclass[../notes.tex]{subfiles}

\begin{document}

% !TEX root = ../notes.tex
















So we have a midterm next class.

\subsection{Integrating and Differentiating Power Series}
Quickly, we recall that, if $f_n:[a,b]\to\RR$ is a sequence of continuous functions converging uniformly to some continuous function $f,$ then
\[\lim_{n\to\infty}\in_a^bf_n(x)\,dx=\int_a^bf(x)\,dx.\]
Note that the integral on the right-hand side integrand $f(x)$ is integrable because $f$ is continuous.
\begin{remark}
	We can imagine changing the closed interval $[a,b]$ to something weaker, but bad things happen.
\end{remark}
Indeed, the above is true essentially because, for any $\varepsilon>0,$ we can find $N$ such that $n>N$ has
\[\left|\int_a^b[f_n(x)-f(x)]\,dx\right|\le\int_a^b|f_n(x)-f(x)|\,dx<\varepsilon(b-a),\]
so taking $\varepsilon\to0$ gives what we want.

Now, for our discussion of power series, we start by fixing
\[\sum_{k=0}^\infty a_kx^k\]
some power series with finite radius of convergence $R>0.$ Let's say that this function converges to $f(x)$ pointwise on our interval of convergence. Of course, this need not always uniformly converge.
\begin{example}
	The power series
	\[\sum_{k=0}^\infty x^k\]
	does not uniformly converge to $\frac1{1-x}$ on its interval of convergence. Intuitively, the problem is that the power series explodes next to $x=1.$
\end{example}
However, we can almost get this.
\begin{proposition}
	Fix
	\[f(x)=\sum_{k=0}^\infty a_kx^k\]
	some power series converging pointwise to $f(x)$ with finite radius of convergence $R>0.$ Then for any $r>0$ with $r<R,$ we have that the power series converges uniformly to $f(x)$ on $[-r,r].$
\end{proposition}
\begin{proof}
	We mostly omit this proof. Essentially the point is that the power series geometrically vanishes, we is fast enough to get our uniform convergence. For concreteness, we use the Weierstrass $M$-test. The point is that each summand is bounded above by $a_kr^k,$ and we know that
	\[\sum_{k=0}^\infty a_kr^k\]
	converges because $r<R.$
\end{proof}
For now we are interested in differentiating and integrating a power series. Though we have not formally defined the derivative nor integration, so we will just do this term by term, taking
\[f(x)=\sum_{k=0}^\infty a_kx^k\stackrel{d/dx}\longmapsto f'(x):=\sum_{k=1}^\infty ka_kx^{k-1}=\sum_{k=0}^\infty(k+1)a_{k+1}x^k\]
as an example of our differentiation.

For example, we have the following statement.
\begin{proposition}
	If the power series $f$ has radius of convergence $R,$ then $f'$ also has the same radius of convergence.
\end{proposition}
\begin{proof}
	The point is that, when we write
	\[f(x)=\sum_{k=0}^\infty a_kx^k\stackrel{d/dx}\longmapsto f'(x):=\sum_{k=0}^\infty(k+1)a_{k+1}x^k,\]
	we are interested in evaluating
	\[\beta:=\limsup_{n\to\infty}\sqrt[n]{|(n+1)a_{k+1}|}.\]
	However, we can remove the $\sqrt[n]{n+1},$ which converges to $1.$ So it follows that
	\[\limsup_{n\to\infty}\sqrt[n]{|(n+1)a_{k+1}|}=\limsup_{n\to\infty}\sqrt[n]{|a_{k+1}|}=\limsup_{n\to\infty}\sqrt[n]{|a_{k}|},\]
	so the reciprocals here give the radius of convergence equal.
\end{proof}
\begin{remark}
	Something similar works for integration; we won't show this explicitly here.
\end{remark}
Essentially this means that the ``worst-case'' scenario is that the endpoints change when differentiating. This can indeed happen.
\begin{example}
	The power series
	\[\sum_{k=0}^\infty\frac{x^k}k\]
	has interval of convergence $\left[-1,1\right),$ but the derivative
	\[\sum_{k=0}^\infty x^k\]
	has interval of convergence $(-1,1).$
\end{example}
Of course, we are not totally sure that this kind of term-by-term integration is legal. For example, we might ask if
\[f(x)=\sum_{k=0}^\infty a_kx^k\]
has
\[\sum_{k=0}^\infty a_k\cdot\frac{r^{k+1}}{k+1}=\int_0^rf(x)\,dx.\]
Well, we do: truncate the power series to the partial sums $f_n\to f,$ and then both sides are uniformly converging to the same value, using our discussion from earlier.

\subsection{Abel's Theorem}
There is a last theorem in this section, which is Abel's theorem, but its proof is not very instructive. Regardless, here is the statement.
\begin{theorem}
	Fix
	\[f(x)=\sum_{k=0}^\infty a_kx^k\]
	a power series with finite radius of convergence $R>0.$ If $f$ exists at $x=R,$ then $f$ is continuous at $x=R.$ Similar holds for $x=-R.$
\end{theorem}
\begin{example}
	Because we know that
	\[-\log(1-x)=\sum_{k=1}^\infty\frac{x^k}k\]
	converges at $x=-1$ even though it has radius of convergence $1,$ we are still able to conclude
	\[-\log2=\sum_{k=1}^\infty\frac{(-1)^k}k.\]
\end{example}
Anyways, let's do some examples.
\begin{exercise}
	We compute
	\[\sum_{n=0}^\infty n^2x^n.\]
\end{exercise}
\begin{proof}
	The main idea is to differentiate the series
	\[\frac1{1-x}=\sum_{n=0}^\infty x^n,\]
	which gives
	\[\frac1{(1-x)^2}=\sum_{n=1}^\infty nx^{n-1}.\]
	Now we multiply both sides by $x$ and differentiate again, which gives
	\[\sum_{n=1}^\infty n^2x^{n-1}=\frac{x+1}{(1-x)^3},\]
	where I have omitted the computation with the quotient rule. Multiplying through by $x$ once more tells us that
	\[\sum_{n=1}^\infty n^2x^n=\frac{x(x+1)}{(1-x)^3}.\]
	We can also see that our radius of convergence remains $1,$ and of course we do not converge at the endpoints (say, by the divergence test), so our interval of convergence is $(-1,1).$
\end{proof}
\begin{exercise}
	We cannot create a power series for $|x|.$
\end{exercise}
\begin{proof}
	The point here is that any power series with positive radius of convergence, we were able to take the derivative termwise, so our power series will be infinitely differentiable (strictly) inside of its radius of convergence. However, $|x|$ is not even differentiable at $x=0,$ so we cannot create a power series from this.
\end{proof}

\subsection{Some Closing Remarks}
The last section in this chapter shows that any continuous function on a closed interval has a sequence of polynomials uniformly converge to it. This is fun but not central to the story in this course.

One can ask, if $f_n\to f$ is a sequence of integrable functions converging uniformly, then do we have $f$ integrable? The answer is no.
\begin{exercise}
	We exhibit $f_n\to f$ converging uniformly on $\left[1,\infty\right)$ such that the $f_n$ are integrable while $f$ is not.
\end{exercise}
\begin{proof}
	Take the functions $f_n:\left[1,\infty\right)$ defined by
	\[f_n(x):=\frac1{x^{1+1/n}}.\]
	This sequence of functions is integrable all over $\left[1,\infty\right),$ but the limit function $f(x):=1/x$ is not. It remains to show that $f_n\to f$ uniformly. Recall that it suffices to look at
	\[\lim_{n\to\infty}\sup\{|f_n(x)-f(x)|:x\in\left[1,\infty\right)\},\]
	so we are interested in the difference
	\[f_n(x)-f(x)=\frac1{x^{1+1/n}}-\frac1x.\]
	To bound this, we want the maximums and minimums, so we differentiate it, getting
	\[\frac1{x^2}-\frac{1+\frac1n}{x^{2+1/n}}=\frac1{x^{2+1/n}}\left(x^{1/n}-1-\frac1n\right).\]
	We are interested in where this vanishes, which is at $x=\left(1+\frac1n\right)^n.$ Now note the difference vanishes at $x=1$ and as $x\to\infty,$ so our only candidate to worry about is our critical point, which gives
	\[\frac1{\left(1+\frac1n\right)^{n+1}}-\frac1{\left(1+\frac1n\right)^n}=\frac1{\left(1+\frac1n\right)^n}\left(\frac1{1+\frac1n}-1\right).\]
	So as $n\to\infty,$ this approaches $0$ because of the second term in the product. In particular, it follows
	\[\lim_{n\to\infty}\sup\{|f_n(x)-f(x)|:x\in\left[1,\infty\right)\}=0,\]
	which finishes.
\end{proof}

\end{document}