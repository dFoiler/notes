% !TEX root = ../notes.tex


















So I am present for class this time.

\subsection{L'H\^opital's Rule Warnings}
Let's talk a little more about L'H\^{opital}'s rule. Quickly, we recall our indeterminate forms.
\begin{definition}[Indeterminate forms]
	Any expression/limit of the form
	\[\frac00,\frac\infty\infty,\quad-\cdot\infty,\quad0^0,\infty^0,1^\infty,\quad\infty-\infty\]
	are called \textit{indeterminate forms}.
\end{definition}
Each of the indeterminate forms has a different way of applying L'H\^opital's rule.
\begin{itemize}
	\item For $\frac00$ or $\frac\infty\infty,$ we can apply directly.
	\item For $0\cdot\infty,$ we rewrite this as $\frac0{1/\infty},$ which is now $\frac00.$
	\item For $0^0,\infty^0,1^\infty,$ we call the limit $L,$ and then we are able to default to one of the previous cases with $\log L.$
	\item For $\infty-\infty,$ we call the limit $L,$ and then we are able to go to $\frac00$ by looking at $\exp L.$
\end{itemize}
\begin{example}
	We have a variety of ways to compute
	\[\lim_{x\to1}\frac{\sqrt x-1}{x-1}.\]
	We could multiply the top and bottom by $\sqrt x+1.$ But if we change the numerator to something more complicated, such tricks become difficult; L'H\^opital's rule is the way to finish.
\end{example}
Here are some (non-)examples.
\begin{exercise}
	We compute the limit
	\[\lim_{x\to\infty}\frac{x-\sin x}x.\]
\end{exercise}
\begin{proof}
	This limit directly goes to $\frac\infty\infty,$ so we could try to apply L'H\^opital's rule, but we get
	\[\lim_{x\to\infty}\frac{1-\cos x}1,\]
	which does not exist. However, the limit does actually exist: we can split up the limit as
	\[\lim_{x\to\infty}\left(1-\frac{\sin x}x\right),\]
	and now this goes to $1-0=\boxed1.$
\end{proof}
The point of the above discussion is to remind ourselves that L'H\^opital's rule only applies when the latter limit does not exist.
\begin{exercise}
	We compute the limit
	\[\lim_{x\to\infty}\frac{e^{4x}-e^{-x}}{e^{3x}+e^{2x}}.\]
\end{exercise}
\begin{proof}
	We could apply L'H\^opital's rule, but it will never terminate. To make this actually possible, we write
	\[\frac{e^{4x}-e^{-x}}{e^{3x}+e^{2x}}=\frac{e^{x}-e^{-4x}}{1+e^{-x}},\]
	which we can see goes to $\boxed{+\infty}$ as $x\to\infty.$
\end{proof}
The point of the above exercise is that sometimes L'H\^opital's rule will not always directly apply.
\begin{exercise}
	We compute
	\[\lim_{x\to\infty}x^{\sin(1/x)}.\]
\end{exercise}
\begin{proof}
	Our indeterminate form is $\infty^0.$ So we set the limit equal to $L,$ and we find
	\[\log L=\lim_{x\to\infty}\sin\left(\frac1x\right)\log x.\]
	We would like to differentiate $\log x,$ so we keep it quarantined and move the $\sin\left(\frac1x\right)$ to the denominator, giving
	\[\log L=\lim_{x\to\infty}\frac{\log x}{1/\sin\left(\frac1x\right)}.\]
	Now the form is $\frac\infty\infty,$ so we use L'H\^opital's rule, which gives
	\[\log L=\lim_{x\to\infty}\frac{1/x}{-1/\sin\left(\frac1x\right)^2\cdot\cos\left(\frac1x\right)\cdot-1/x^2}.\]
	This collapses down to
	\[\log L=\lim_{x\to\infty}x\sin\left(\frac1x\right)\tan\left(\frac1x\right).\]
	We could do this by brute force, but it is a bit more efficient to write this as
	\[\log L=\lim_{x\to\infty}\frac{\sin\left(1/x\right)}{1/x}\cdot\tan\left(\frac1x\right),\]
	which will got $1\cdot0=0,$ so $L=\boxed1.$
\end{proof}
\begin{exercise}
	We compute
	\[\lim_{x\to0}(1+2x)^{1/x}.\]
\end{exercise}
\begin{proof}
	The form here is $1^\infty,$ so we set the limit equal to $L$ and compute
	\[\log L=\lim_{x\to0}\frac{\log(1+2x)}x.\]
	Applying L'H\^opital's rule, we get
	\[\log L=\lim_{x\to0}\frac{\frac{2}{1+2x}}1=\lim_{x\to0}\frac2{1+2x}=2.\]
	So it follows $L=\boxed{e^2}.$
\end{proof}
As a further remark, we note that applying L'H\^opital's rule to
\[\lim_{x\to a}\frac{f(x)}{g(x)}\]
needs $f$ and $g$ to be differentiable in a neighborhood around $a$ and $g'$ to be nonzero in a neighborhood around $a.$ This last condition might appear awkward, but it is necessary. Normally this isn't a problem due to, say, smoothness, but it does prevent us from applying the rule for, say,
\[\lim_{x\to\infty}\frac{x-\sin x}x\]
as we saw earlier. Essentially these conditions go into the assertion that
\[\lim_{x\to a}\frac{f'(x)}{g'(x)}\]
actually exists.
\begin{exercise}
	We set $f(x)=x+\cos x\cdot\sin x$ and $g(x)=e^{\sin x}f(x).$ We compute
	\[\lim_{x\to\infty}\frac{f(x)}{g(x)}.\]
\end{exercise}
\begin{proof}
	Each of $f$ and $g$ are nonzero in a neighborhood around $0$ by, say, continuity. We can also compute the derivatives as
	\begin{align*}
		f'(x) &= x+\cos x\cdot\cos x-\sin x\cdot\sin x = 2\cos^2x \\
		g'(x) &= e^{\sin x}\cdot\cos x[2\cos x+f(x)]
	\end{align*}
	after applying some elbow grease. In particular, we see
	\[\frac{f'(x)}{g'(x)}=\frac{2e^{-\sin x}\cdot\cos x}{2\cos x+f(x)}\]
	so that the numerator is bounded and the denominator goes to infinity, giving $0$ as $x\to\infty.$

	However, the original limit is of $\frac fg=e^{-\sin x},$ so this limit does not actually exist. What went wrong in the above example is that the limit of $\frac{f'}{g'}$ does not actually exist because $g'$ vanishes arbitrarily close to $+\infty.$ So it goes.
\end{proof}

\subsection{Taylor's Theorem}
Our last topic on derivatives is Taylor's theorem. Again there will be strange convergence issues to keep track of, so we will want to study this a bit closer. Here is our definition.
\begin{defi}[Taylor series]
	Fix $f:\RR\to\RR$ infinitely differentiable at $x=a.$ Then we define the infinite \textit{Taylor series} (formally, say) by
	\[\sum_{k=0}^\infty\frac{f^{(k)}(a)(a)}{a!}(x-a)^k.\]
	We also define our remainder term by
	\[R_n(x)=f(x)-\sum_{k=0}^{n-1}\frac{f^{(k)}(c)}{k!}(x-c)^k.\]
\end{defi}
\begin{exe}
	We check the remainder for
	\[\frac1{1-x}=\sum_{k=0}^\infty x^k.\]
\end{exe}
\begin{proof}
	We note that for $|x|<1,$ we have
	\[R_n(x)=\sum_{k=n}^\infty x^k=x^n\cdot\frac1{1-x},\]
	which will go to $0$ as $n\to\infty,$ as it should
\end{proof}
Most functions that we like will have the Taylor remainder term approach $0$ as long as the series converges. The typical way to check this is as follows.
\begin{proposition}
	Suppose that $f:\RR\to\RR$ is $n$th differentiable. Then, fixing some $x$ and $c,$ there exists $d$ between $x$ and $c$ such that
	\[R_n(x)=\frac{(x-c)^n}{n!}f^{(n)}(d).\]
\end{proposition}
The point here is that oftentimes we can bound $f^{(n)}(d)$ in such a way that we can promise $n!$ will dominate.
\begin{example}
	For $f(x)=e^{2x}$ take $c=0$ and $x=1.$ Then $f^{(n)}(d)\le2^n$ for all $d$ between $x$ and $c,$ so $R_n(x)\to0$ as $n\to\infty,$ and our Taylor series converges correctly.
\end{example}
And here is the warning to correspond to our hopes.
\begin{exe}
	We consider the Taylor series for
	\[f(x)=\begin{cases}
		e^{-1/x^2} & x\ne0, \\
		0 & x=0.
	\end{cases}\]
	In particular, the Taylor series does not converge to $f$ for any $x\ne0.$
\end{exe}
\begin{proof}
	We can check that $f:\RR\to\RR$ is continuous, and it is in fact infinitely differentiable, even at $0.$ The main point to differentiability at $0$ is that, for each $n\in\NN,$ there exists $p_n\in\ZZ[x]$ such that
	\[f^{(n)}(x)=p_n(1/x)e^{-1/x^2},\]
	which we can show by an induction. But this vanishes as $x\to0,$ so $f^{(n)}(0)=0$ for each $n\in\NN.$ It follows that the Taylor series is the zero series, which is not equal to $f$ for any $x\ne0.$
\end{proof}
\begin{remark}
	More viscerally, we can see that the derivatives of $f$ at particular values $x$ around $0$ are growing at a factorial rate. Essentially, the $p_n(x)$ will accumulate leading coefficients at a combinatorial speed, which is something we can see from direct expansion.
\end{remark}

\subsection{The Binomial Theorem}
Let's spend a moment discussing the Binomial theorem. We have the following.
\begin{proposition}
	Fix $\alpha\in\RR.$ We claim that, for $|x|<1,$
	\[(1+x)^\alpha=\sum_{n=0}^\infty\binom\alpha n x^\alpha,\]
	where
	\[\binom\alpha n:=\prod_{k=0}^{n-1}\frac{\alpha-k}{n-k}.\]
\end{proposition}
\begin{remark}
	It is not hard to verify by hand that $\alpha\in\NN$ causes $\binom\alpha n$ to match what we want it to. I won't write this our because it is just a matter of writing down the formula and staring.
\end{remark}
\begin{proof}
	We can check the convergence of this by the Ratio test. We find that
	\[\frac{\binom\alpha{k+1}x^{k+1}}{\binom\alpha kx^k}=\frac{\alpha-k}k\cdot x,\]
	so we get convergence for $|x|<1.$ In fact, we observe that this sort of argument can be used to verify that the remainder term from $(1+x)^\alpha$ vanishes as the number of terms in the series goes to infinity.
\end{proof}

\subsection{Newton's Method}
The idea of Newton's method is to find roots of a differentiable function $f,$ and we want to find a root. So we start with a guess $x_0,$ and to get closer, we draw the tangent line at $0$ and find the root. Namely, $f$ is hopefully locally linear, so we can hope to get close to the root by a linear approximation. Here is the image.
\begin{center}
	\begin{asy}
		import graph;
		unitsize(4cm);
		real f(real x)
		{
			return x*x*x - x + 0.1;
		}
		draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
		draw(graph(f,-0.6,1.5), blue);

		real a0 = 1.3;
		real a1 = a0 - (a0*a0*a0 - a0) / (3*a0*a0 - 1);

		pair v = (a0, f(a0)) - (a1, 0);
		draw((a1, 0) -- (a1, 0) + 1.8*v, red);

		draw((a0,0) -- (a0,f(a0)), dashed);
		draw((a1,0) -- (a1,f(a1)), dashed);

		dot("$x_0$", (a0,0), S); dot("$x_1$", (a1, 0), S);

		dot("$(x_0,f(x_0))$", (a0, f(a0)), WNW);
		dot("$(x_{1},f(x_{1}))$", (a1, f(a1)), WNW);

		label("\color{red}$y-f(x_0)=f'(x_0)(x-x_0)$", (a1,0) + 1.5*v, W);
	\end{asy}
\end{center}
Expanding this out by hand, our recursion is
\[x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}.\]
This is a recursion we could hopefully do by computer. The rest of \S31 can be omitted. It is quite technical and a lot about approximations, which we tend to not care about in this class.

Anyways, let's do an exercise.
\begin{exe}[Ross 31.4(a)]
	Fix $a,b\in\RR$ such that $a<b.$ Then find $f$ a function infinitely differentiable so that $f(x)=0$ for $x\le0$ and $f(x)>0$ for $x>0.$
\end{exe}
\begin{proof}
	This is somewhat subtle. For example,
	\[f(x)\stackrel?=\begin{cases}
		x^2 & x\ge0, \\
		0 & x\le0,
	\end{cases}\]
	will not work because this function is not twice-differentiable at $0.$ So we need a function with lots of vanishing derivatives at $0$ even though it is not zero, so borrow our sad example from the Taylor series section: we define
	\[f(x)\stackrel?=\begin{cases}
		e^{-1/x} & x>0, \\
		0 & x\le0.
	\end{cases}\]
	The point is that we computed $f^{(n)}(0)=0$ for all $n\in\NN,$ even as we approach from the right, so this function will be differentiable at $0$ and hence infinitely differentiable everywhere.
\end{proof}