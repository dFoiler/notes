\documentclass[../notes.tex]{subfiles}

\begin{document}

% !TEX root = ../notes.tex














I skipped a talk on the Riemann hypothesis to come to this class. This had better be good.

\subsection{Limit Laws}
So we can prove nice things with our rigorous definition of a limit, and by ``nice'' I mean ``obvious.'' Let's have an example.
\begin{proposition}
	A convergent sequence in $\RR$ is always bounded.
\end{proposition}
This is the kind of thing that looks obvious, but proving it formally is annoying
\begin{proof}
	Fix our sequence $\{a_n\}_{n\in\NN}$ of real numbers which converges to $L.$ Now, for any (!) $\varepsilon>0,$ we have an $N$ such that $n>N$ implies
	\[|a_n-L|<\varepsilon.\]
	It follows that $L-\varepsilon<a_N<L+\varepsilon$ by considering cases. Then consider
	\[M:=\max\{a_1,\ldots,a_N,L+\varepsilon\}.\]
	Then we claim each $n\in\NN$ has $a_n<M.$ Indeed, if $n\le N,$ then $n\le M$ because of the maximum. Otherwise $n>N$ so that $a_n<L+\varepsilon.$
\end{proof}
Let's jump into a limit law.
\begin{proposition}
	Suppose that the real sequences $\{s_n\}_{n\in\NN}$ and $\{t_n\}_{n\in\NN}$ converge to the real numbers $s$ and $t$ respectively. Then
	\[\lim_{n\to\infty}s_nt_n=st.\]
\end{proposition}
\begin{proof}
	This requires a careful application of the triangle inequality. We show that $|s_nt_n-st|\to0.$ Then we write
	\[|s_nt_n-st|=|s_nt_n-s_nt+s_nt-st|\le|s_nt_n-s_nt|+|s_nt-st|\]
	by the triangle inequality. Then we can bound
	\[|s_nt_n-st|=|s_n|\cdot|t_n-t|+|t|\cdot|s_n-s|.\]
	Now, taking everything as $n\to\infty,$ we have $|s_n|\cdot|t_n-t|$ has absolute value bounded by $||s|+1|\cdot|t_n-t|\to0$ for sufficiently large $n.$ Similarly, $|t|\cdot|s_n-s|$ will also go to $0.$

	The end of the proof requires some care, so we will provide a few more details for the first case: we will show that $|s_n|\cdot|t_n-t|\to0$ more rigorously. Fix any $\varepsilon>0.$ Then there is some $N_1$ for which $n>N_1$ implies
	\[|t_n-t|<\frac{\varepsilon}{|s|+1},\]
	and there is some $N_2$ for which $n>N_2$ implies $|s_n|<|s|+1.$ Then $N:=\max\{N_1,N_2\}$ has $n>N$ implies
	\[|s_n|\cdot|t_n-t|<(|s|+1)\cdot\frac{\varepsilon}{|s|+1}=\varepsilon,\]
	which is what we wanted.
\end{proof}
\begin{remark}
	This is essentially the same trick that proves the product rule for derivatives: we write
	\begin{align*}
		(f g)'(x) &= \lim_{h\to0}\frac{f(x+h)g(x+h)-f(x)g(x)}h \\
		&= \lim_{h\to0}\frac{f(x+h)g(x+h)-f(x)g(x+h)}h+\lim_{h\to0}\frac{f(x)g(x+h)-f(x)g(x)}h \\
		&= f'(x)g(x)+f(x)g'(x).
	\end{align*}
\end{remark}
Let's do an exercise.
\begin{exercise}
	We show that
	\[\lim_{n\to\infty}n^{1/n}=1.\]
\end{exercise}
In calculus, we might want to take the derivative of the numerator and denominator, but we won't do that here.
\begin{proof}
	We consider the sequence $s_n:=n^{1/n}-1$; we show that $s_n\to0.$ Note that $n\ge1$ implies $n^{1/n}\ge1$ implies $s_n\ge0.$

	Now, the trick is to write, for $n>1,$ that
	\[n=(1+s_n)^n=1+\binom n1s_n+\binom n2s_n^2+\cdots\ge\frac{n(n-1)}2s_n^2.\]
	It follows that $s_n^2\le\frac2{n-1}.$ However, $\sqrt{\frac2{n-1}}$ goes to $0$ with some effort\footnote{We will not do this here, but we just have to show that $\sqrt{\frac2{n-1}}$ gets arbitrarily small.}, so $s_n$ must also go to $0,$ finishing.
\end{proof}

\subsection{Back to \texorpdfstring{$\pm\infty$}{}}
What should it mean for a sequence $\{s_n\}_{n\in\NN}$ to have
\[\lim_{n\to\infty}s_n=+\infty?\]
Note that this definition requires some care. For example, the sequence
\[s_n:=\begin{cases}
	0 & n\text{ is even}, \\
	n & n\text{ is odd},
\end{cases}\]
is mostly increasing and is not bounded above, but it does not go to $\infty.$ The rigorous definition is as follows.
\begin{definition}[\texorpdfstring{$s_n\to\infty$}{}]
	Fix $\{s_n\}_{n\in\NN}$ a sequence of real numbers. Then $\lim s_n=\infty$ if and only if, for every $M,$ there exists an $N$ such that $n>N$ implies
	\[s_n>M.\]
\end{definition}
Intuitively, $s_n$ must keep climbing and eventually never go back. Note this does not mean monotonic.
\begin{remark}
	The above definition uses an $M$ and not a $\varepsilon$ because $M$ is not intended to be a small number; $M$ is supposed to be big. We could use $1/\varepsilon$ if we wanted, but this is unnecessarily awkward.
\end{remark}
\begin{warn}
	To prove
	\[\lim_{n\to\infty}s_n=+\infty,\]
	do not fix $\varepsilon>0$ and try to find $N$ such that $n>N$ implies $|s_n-\infty|<\varepsilon.$ The issue here is that $|s_n-\infty|$ does not
\end{warn}
Similarly, we have the following, almost dual definition.
\begin{definition}[\texorpdfstring{$s_n\to-\infty$}{}]
	Fix $\{s_n\}_{n\in\NN}$ a sequence of real numbers. Then $\lim s_n=\infty$ if and only if, for every $M,$ there exists an $N$ such that $n>N$ implies
	\[s_n<M.\]
\end{definition}
In this definition, $M$ is intended to be a very negative number.

Now that we've added $\pm\infty$ to our vocabulary, we would like to codify this.
\begin{definition}[Has a limit]
	Given a sequence $\{s_n\}_{n\in\NN},$ we say that $s_n$ \textit{has a limit} if and only if
	\[\lim_{n\to\infty}s_n\in\RR\cup\{\pm\infty\}.\]
\end{definition}
Note that having a limit is not the same as converging; converging excludes $\pm\infty.$ Having a limit is more about excluding sequences which oscillate too much. The language is careful here: converging is synonymous with the limit existing, which is distinct from the sequence having a limit.

Anyways, let's do some exercises.
\begin{proposition}
	Fix $\{s_n\}_{n\in\NN}$ a sequence of positive real numbers. Then
	\[\lim_{n\to\infty}s_n=\infty\iff\lim_{n\to\infty}\frac1{s_n}=0.\]
\end{proposition}
\begin{proof}
	Again, this statement is too general for us to be able to do anything other than the formal definition of a limit. We show the directions one at a time.
	\begin{itemize}
		\item Fix $s_n$ with $s_n\to\infty,$ and fix any $\varepsilon>0.$ Then $1/\varepsilon>0,$ so there exists $N$ such that $n>N$ implies
		\[s_n>1/\varepsilon>0.\]
		From this it follows $0<1/s_n<\varepsilon,$ so $|1/s_n|<\varepsilon.$ So our $N$ witnesses that $1/s_n\to0.$
		\item In the other direction, fix $s_n$ with $1/s_n\to0,$ and fix any $M.$ If $M\le0,$ then there is some $N$ such that $1/s_n<1$ so that $s_n>1>M.$
		
		Otherwise, $M>0,$ so we can take $\varepsilon=1/M>0$ and find $N$ so that $n>N$ implies
		\[1/s_n<\varepsilon.\]
		It follows $s_n>1/\varepsilon=M,$ so our $N$ witnesses $s_n\to\infty.$
		\qedhere
	\end{itemize}
\end{proof}
\begin{exercise}
	Fix $t_1=1$ and define $\{t_n\}_{n\in\NN}$ recursively by
	\[t_{n+1}=\frac{t_n^2+2}{2t_n}\]
	for $n\ge1.$ Then, given $\lim t_n$ is a real number, we can compute the limit is $\sqrt2.$
\end{exercise}
\begin{proof}
	The point is to take $n\to\infty$ on both sides of our recursion.
	\[t_{n+1}=\frac{t_n^2+2}{2t_n}.\tag{$*$}\]
	Now, we can show that
	\[\lim_{n\to\infty}t_{n+1}=\lim_{n\to\infty}t_n=:t\]
	by shifting over by one (we won't be rigorous here). We would like to use limit laws to finish, but the denominator of $(*)$ has $t_n$ in it, which we need to verify does not vanish. Well, we claim
	\[t_n\ge1\]
	by induction. Surely this is true for $n=1$; then by induction, we note
	\[\frac{t_n^2+2}{2t_n}=\frac{t_n}2+\frac1{t_n}.\]
	If $t_n\ge2,$ then the first term is at least $1$; if $2\ge t_n\ge1,$ then the sum of the two terms is at least $\frac12+\frac12=1.$

	Now we can use limit laws to note that $(*)$ implies
	\[t=\lim_{n\to\infty}t_{n+1}=\frac{\displaystyle\lim_{n\to\infty}t_n^2+2}{\displaystyle\lim_{n\to\infty}2t_n}=\frac{t^2+2}{2t}\]
	after using limit laws. Solving, we see that $2t^2=t^2+2,$ so $t^2=2,$ so $t\in\{\pm\sqrt2\}.$ Then we see $t\ge1$ implies $\boxed{t=\sqrt2}$ is forced.
\end{proof}
\begin{remark}
	It is a very strong assumption that the limit exists, which is what lets the above argument function. For example, if we change $t_1=1$ to $t_1=-1,$ then the limit ``should'' be $-\sqrt2$ (all terms are negative), but it's not obvious that the limit should converge anymore.
\end{remark}
\begin{exercise}[Ross 9.11(a)]
	Fix $\{s_n\}_{n\in\NN}$ and $\{t_n\}_{n\in\NN}$ sequences such that $s_n\to\infty$ and $\inf\{t_n\}_{n\in\NN}>-\infty.$ Then
	\[\lim_{n\to\infty}(s_n+t_n)=\infty.\]
\end{exercise}
\begin{proof}
	Fix any lower bound $m\in\RR$ for $\{t_n\}_{n\in\NN}.$ Now, fix any $M>0$ so that we want to find $N$ such that $n>N$ implies
	\[s_n+t_n>M.\]
	Well, we have that $t_n\ge m$ so that the above condition is equivalent to
	\[s_n>M-m.\]
	To finish, $s_n\to\infty$ promises us some $N$ such that $n>N$ implies $s_n>M-m,$ which is exactly what we wanted.
\end{proof}
\begin{exercise}[Ross 9.15]
	Fix $a\in\RR$ a positive real number. Then
	\[\lim_{n\to\infty}\frac{a^n}{n!}=0.\]
\end{exercise}
\begin{proof}
	For brevity, fix $d_n:=a^n/n!.$ Consider $N\in\NN$ such that $N>2a$; the main idea is that after $N,$ $d_n$ is being (more than) halved each time and therefore geometrically decreasing. So let's start bounding at $d_N$: we see
	\[d_{N+k}=d_N\prod_{\ell=1}^k\frac a{N+\ell}\le d_N\prod_{\ell=1}^k\frac aN\le d_N\prod_{\ell=1}^k\frac12=d_N\left(\frac12\right)^k.\]
	But now, as $k\to\infty,$ we have $(1/2)^k\to0,$ so $d_N(1/2)^k\to0$ as well.\footnote{It is surprisingly technical to show that $(1/2)^k\to0,$ but it can be done.} It follows that $d_n\to0$ by squeezing.
\end{proof}
\begin{remark}
	One of the motivations for taking $N>2a$ is that, after this point, the sequence is definitely (monotonically!) decreasing, and being bounded below by $0,$ we can be fairly confident that the limit exists. This idea will come up again.
\end{remark}

\end{document}