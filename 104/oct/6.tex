\documentclass[../notes.tex]{subfiles}

\begin{document}

% !TEX root = ../notes.tex















We're finishing up \S15 and \S16 today.

\subsection{Integral Test}
We've probably seen the integral test in other classes. Let's start with the following example.
\begin{prop}[\texorpdfstring{$p$}{}-series test]
	The series
	\[\sum_{n=1}^\infty\frac1{n^p}\]
	if and only if $p>1.$
\end{prop}
\begin{proof}
	This is done by the integral test. Essentially,
	\[\int_2^\infty\frac1{x^p}\,dx\]
	converges if and only if $p>1.$
\end{proof}
\begin{remark}
	This result is quite important because they provide nice series to compare to. For example, we have some amount of control over any polynomials now.
\end{remark}

\subsection{Alternating Series Test}
We start by defining our sequences.
\begin{definition}[Alternating]
	A sequence $\{a_k\}_{k\in\NN}$ is \textit{alternating} if and only if $a_{k+1}$ has a different sign from $a_k$ for each $k\in\NN.$
\end{definition}
\begin{warn}
	Merely having a mix of positive and negative terms is not enough to be alternating. It must alternate at every integer.
\end{warn}
And here is our test.
\begin{prop}[Alternating series test]
	Suppose $\{a_k\}_{k\in\NN}$ is a sequence of nonnegative terms such that
	\begin{itemize}
		\item $a_k\to0$ as $k\to\infty,$ and
		\item $\{a_k\}_{k\in\NN}$ is decreasing.
	\end{itemize}
	Then
	\[\sum_{k=1}^\infty(-1)^{k-1}a_k\]
	converges.
\end{prop}
Note that this is almost as strong as we could want: surely $a_k\to0$ as $k\to\infty$ is necessary for convergence, and all extra we have asked for is decreasing. In exchange for this strength, we are 
\begin{proof}
	We use the Cauchy criterion. Fix any $\varepsilon>0.$ Now, there is some $N$ for which $k>N$ implies $a_k<\varepsilon$ because $a_k\to0.$ Then we claim that $n>m>N$ implies
	\[\left|\sum_{k=m+1}^n(-1)^{k-1}a_k\right|<\varepsilon.\]
	The point is that an induction can show
	\[\sum_{k=m+2}^n(-1)^{k-m-2}a_k>0\]
	for any $n,$ so
	\[\left|\sum_{k=m+1}^n(-1)^{k-1}a_k\right|\le|a_n|<\varepsilon,\]
	which is what we needed.
\end{proof}
\begin{remark}
	The Cauchy criterion is nice here because it provides a somewhat general machinery for how we could prove similar statements. For example, we could imagine using the same machine to show
	\[\sum_{k=1}^\infty\sin\left(\frac{2\pi k}{10}\right)a_k\]
	converges when $\{a_k\}_{k\in\NN}$ is a nonnegative, decreasing sequence of real numbers which goes to $0.$
\end{remark}
As an example, note that the alternating harmonic series
\[\sum_{k=1}^\infty\frac{(-1)^{k-1}}k\]
converges by the Alternating series test. However, it is possible to rearrange the terms around to make this diverge to $+\infty$ (or anything in $\RR\cup\{\pm\infty\}$ for that matter). For example, write positive terms $\frac1{2k+1}$ until we are above $1,$ then write down $-1/2.$ Then continue writing down positive terms until we are above $2,$ and write down $-1/3.$ Then continue in this matter.

The problem here is that the alternating harmonic series is conditionally convergent.
\begin{definition}[Conditionally convergent]
	Fix $\{a_k\}_{k\in\NN}$ a sequence of real numbers. If $\sum a_n$ converges but $\sum|a_n|$ does not converge, then $\sum a_n$ is called \textit{conditionally convergent}.
\end{definition}
It is a fact that rearranging the terms in a conditionally convergent series can be rearranged to make the sum whatever we want. In contrast, absolutely convergent series do not have this problem: any rearrangement is safe.

Anyways, let's do an exercise.
\begin{exercise}
	The series
	\[\sum_{k=2}^\infty\frac1{k(\ln k)^p}\]
	will converge if and only if $p>1.$
\end{exercise}
\begin{proof}
	We use the integral test because it can cover most $p$ at once. Comparison test might seem viable, but because we need to cover the entire real spectrum of $p,$ we have to be careful. Set $f(x)=\frac1{x(\ln x)^p}$ which is decreasing and hence safe.
	
	We can compute, for $p\ne1,$
	\[\int_2^\infty\frac{dx}{x(\ln x)^p}=\int_{\ln2}^\infty\frac{du}{u^p}=\frac{u^{-p+1}}{-p+1}\bigg|_{\ln2}^\infty.\]
	If $p>1,$ then the exponent is negative, so the series converges. If $p<1,$ then the exponent is positive, so the series diverges. And for $p=1,$ we have
	\[\int_{\ln2}^\infty\frac{du}u=\ln u\bigg|_2^\infty=+\infty,\]
	so we diverge again.
\end{proof}
\begin{exercise}
	Suppose $\{a_k\}_{k\in\NN}$ is a decreasing sequence with $\sum a_k$ convergent. Then $\lim na_n=0.$
\end{exercise}
\begin{proof}
	Note that some care is required: doing this by contradiction is difficult because the $\lim na_n\ne0$ is not very helpful.

	We have $a_k\ge0$ for each $k$ because otherwise $\lim a_k$ will be less than $0$ or nonexistent, breaking the divergence test. Now, by the Cauchy criterion, for each $\varepsilon>0,$ we can find $N$ such that $n>m>N$ implies
	\[\sum_{k=m+1}^na_k<\varepsilon.\]
	The point here is that all of these terms are at least $a_n$ because the $a_\bullet$ decrease, so $(n-m)a_n<\varepsilon.$ For example, we may take $n=2m$ so that $ma_{2m}<\varepsilon,$ which means $(2m)a_{2m}<\varepsilon.$ So this shows that $(2m)a_{2m}\to0,$ which is good enough because the $a_\bullet$ are decreasing.
\end{proof}

\subsection{Talking Reals}
Let's talk about \S16; it's nice but fairly irrelevant. Namely, it is perhaps not worth studying closely for the purposes of the class.

We are building the real numbers. Let's recall some ways to define the rational numbers.
\begin{definition}
	A rational number is the ratio of two integers.
\end{definition}
\begin{definition}
	A rational number is an eventually repeating decimal expansion.
\end{definition}
Then we can define our real numbers as decimal expansions, using the idea of extending the second definition. However, this is somewhat subtle: how do we show that this expansion is unique? Well, the answer is that this is false:
\[0.0\overline9=0.10.\]
To rigorize this, we note that
\[0.d_1d_2\ldots=\sum_{k=1}^\infty\frac{d_k}{10^k}.\]
Then if two decimal expansions $0.d_1\ldots$ and $0.e_1\ldots$ give the same real number, then we have
\[\sum_{k=1}^\infty\frac{d_k-e_k}{10^k}=0.\]
If these aren't identically equal, then say that the differ first at $N.$ Then
\[\frac{e_N-d_N}{10^N}=\sum_{k>N}\frac{d_k-e_k}{10^k},\]
but this is very restrictive: the sum on the right is at most $\sum_{k>N}\frac9{10^k}=\frac1{10^N}$ to begin with, so the only way for this to occur is if $e_N=d_N+1$ (where $e_N>d_N$) without loss of generality, and $d_k-e_k=0$ for $k>N.$

We can also show that rational numbers have eventually repeating decimal expansions. For example, suppose we are looking at
\[\alpha=0.d_1\ldots d_{k-1}\overline{d_k\ldots d_n}.\]
Then
\[\beta:=10^{k-1}\alpha=0.\overline{d_k\ldots d_n}.\]
However, $10^{n-k}\beta-\beta$ is equal to $d_k\ldots d_n$ as an integer, so $\beta$ is a rational, so $\alpha$ is a rational.

In the other direction, suppose we have a ratio of integers $a/b,$ and we take $\gcd(b,10)=1,$ for otherwise we can shift over the decimal expansion by multiplying the fraction by a sufficiently large power of $10.$ Now study the sequence of remainders
\[10^k\pmod b\]
for $k\in\ZZ.$ This must repeat eventually because there are infinitely many integers and finitely many residues, so find some $k>\ell$ for which $10^k\equiv10^\ell\pmod b$ so that $n:=k-\ell$ has $10^n\equiv1\pmod b.$ But then
\[\frac ab=\frac{a\cdot\frac{10^n-1}b}{10^n-}\]
is a repeating decimal.

And here is an exercise to round our our discussion.
\begin{exercise}[Ross 16.13]
	Suppose that $\sum a_k$ and $\sum b_k$ both converge with $a_k\le b_k$ for each $k$ and $a_k<b_k$ for at least one $k.$ Then
	\[\sum_{k=1}^\infty a_k<\sum_{k=1}^\infty b_k.\]
\end{exercise}
\begin{proof}
	We show a kind of contrapositive. Suppose that $a_k\le b_k$ and
	\[L:=\sum_{k=1}^\infty a_k=\sum_{k=1}^\infty b_k.\]
	Combining the convergences, we see that, for any $\varepsilon>0,$ there exists some $N$ such that $n>N$ implies
	\[\left|L-\sum_{k=1}^na_k\right|,\left|L-\sum_{k=1}^nb_k\right|<\varepsilon,\]
	which gives
	\[\left|\sum_{k=1}^n(b_k-a_k)\right|<2\varepsilon\]
	by the triangle inequality. But $b_k\ge a_k$ for each $k,$ so we see
	\[\sum_{k=1}^n(b_k-a_k)<2\varepsilon.\]
	Now, for any particular $m\in\NN,$ we note that, for any $\varepsilon>0,$ we have some $N$ such that $n>\max\{m,N\}$ implies
	\[0\le(b_m-a_m)\le\sum_{k=1}^n(b_k-a_k)<2\varepsilon.\]
	So we must have $b_m=a_m$ because their difference is smaller than any positive real number $2\varepsilon.$
\end{proof}

\end{document}