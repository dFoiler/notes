% !TEX root = ../notes.tex

Here we go.
\begin{convention}
	For today's lecture, an $S$-algebra $R$ should be thought of as providing an embedding $R\subseteq S$ (though we will not actually assume that this ring map is injective).
\end{convention}

\subsection{A Better Integrality}
Last time we introduced the following proposition.
\integralitydef*
\noindent This gave rise to the following definition.
\integraldefi*
\noindent Being integral is intended to be a generalization of having a finite extension of fields. Along these lines, we get the following definition.
\finitedefi*
\noindent As with fields, we know that any finite field extension must be algebraic, so we might hope that an integral extension is also finite.
\begin{lemma} \label{lem:finiteimpliesintegral}
	Every finite $R$-algebra $S$ is integral.
\end{lemma}
\begin{proof}
	We use the Cayley--Hamilton theorem. Fix any element $s\in S$ so that we want to show $s$ is integral over $R$. The key point is that $\mu_s:x\mapsto sx$ is an endomorphism $\mu_s:S\to S$ of $S$ as an $R$-module. Namely, for $s_1,s_2\in S$ and $r_1,r_2\in R$, we merely have to check that
	\[\mu_s(r_1s_1+r_2s_2)=sr_1s_1+sr_2s_2=r_1ss_1+s_2ss_2=r_1\mu_s(s_1)+r_2\mu_s(s_2).\]
	Thus, \autoref{thm:betterch} promises a monic polynomial
	\[p_{\mu_s}(x)=x^n+\sum_{k=0}^{n-1}r_kx^k\]
	such that $p_{\mu_s}(\mu_s)=0$. In particular, we see that
	\[0=0\cdot1=p_{\mu_s}(\mu_s)(1)=\left(\mu_s^n+\sum_{k=0}^{n-1}r_k\mu_s^k\right)(1)=\mu_s^n(1)+\sum_{k=0}^{n-1}r_k\mu_s^k(1)=s^n+\sum_{k=0}^{n-1}r_ks^k,\]
	which verifies that $s$ is the root of a monic polynomial $p_{\mu_s}(x)\in R[x]$.
\end{proof}
In fact, we can provide a converse.
\begin{lemma} \label{lem:betterfinite}
	Fix $S$ an $R$-algebra. Then $S$ is finite if and only if it is finitely generated as an $R$-algebra with integral generators.
\end{lemma}
\begin{proof}
	We show the two directions independently. The key to the backwards direction will be the following lemma.
	\begin{lemma} \label{lem:finiteoverfinite}
		If $A\subseteq B\subseteq C$ are rings, then if $C$ is finite over $B$ and $B$ is finite over $A$, then $C$ is finite over $A$.
	\end{lemma}
	\begin{proof}
		Because $C$ is finite over $B$, we get generators $c_1,\ldots,c_n$. Similarly, because $B$ is finite over $A$, we get generators $b_1,\ldots,b_m$. We claim that the $b_ic_j$ generate $C$ as an $A$-module, which will finish the proof.

		Indeed, any $c\in C$ we can write as
		\[c=\sum_{j=1}^nr_jc_j\]
		where $r_j\in B$. Then, expanding the $r_j$ along the generators $b_1,\ldots,b_m$, we get
		\[r_j=\sum_{i=1}^ms_{ij}b_i.\]
		Distributing, we see that
		\[c=\sum_{j=1}^n\sum_{i=1}^ns_{ij}(b_ic_j),\]
		which finishes the proof.
	\end{proof}
	We now attack the proof directly.
	\begin{itemize}
		\item In one direction, suppose that $S=R[s_1,\ldots,s_n]$ where the elements $s_1,\ldots,s_n$ are all integral over $R$. The key is to consider the chain
		\[R\subseteq R[s_1]\subseteq R[s_1,s_2]\subseteq\cdots\subseteq R[s_1,\ldots,s_n].\]
		We show that $R[s_1,\ldots,s_k]$ is finite over $R$ by induction on $k$; when $k=0$, there is nothing to say. For the inductive step, suppose that some $R[s_1,\ldots,s_k]$ is finite over $R$ by the elements $\{x_1,\ldots,x_m\}$, and we show $R[s_1,\ldots,s_{k+1}]$ is finite over $R$.
		
		Well, $s_{k+1}$ is the root of some monic polynomial which we name $p\in R[x]$. But $p\in R[s_0,\ldots,s_k]$ as well, so $s_{k+1}$ is integral over $R[s_0,\ldots,s_k]$. Thus, by\autoref{prop:integralitydef}, $R[s_1,\ldots,s_k][s_{k+1}]$ will still be finitely generated as an $R[s_1,\ldots,s_k]$-module.

		It follows that $R[s_1,\ldots,s_{k+1}]$ is finitely generated as an $R$-module by \autoref{lem:finiteoverfinite}.

		\item In the other direction, take $S$ a finite $R$-algebra. In particular, we see that $S=Rs_0R+\cdots+s_nR$ for some elements $s_0,\ldots,s_n\in S$. But \autoref{lem:finiteimpliesintegral} now forces the elements $s_k$ to all be integral, so we see that $S$ is finite over $R$ with generators which are integral.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}[Nir]
	The real point of the above discussion is to give a better description of integral elements looks like: they generate finite algebras. (Again, note the analogy with fields: algebraic elements generate finite field extensions.) This will be more apparent in \autoref{prop:integralclosure}.
\end{remark}

\subsection{The Integral Closure}
Sometimes an algebra isn't integral, but we can always make an integral extension out of our algebra.
\begin{definition}[Integral closure]
	Fix $S$ an $R$-algebra. Then the \textit{integral closure} $S'$ of $S$ over $R$ is the set of all elements of $S$ which are integral over $R$.
\end{definition}
\begin{remark}
	The integral closure depends on the choice of $S$: making $S$ bigger permits more integral elements. This is analogous to the algebraic closure of a field technically depending on our choice of parent field.
\end{remark}
\begin{proposition} \label{prop:integralclosure}
	Fix $S$ an $R$-algebra. Then the integral closure $S'$ of $S$ is an $R$-subalgebra of $S$. In particular, if $s_1,s_2\in S$ are integral elements, then $s_1+s_2$ and $s_1s_2$ are also both integral elements.
\end{proposition}
\begin{proof}
	The main idea is to use \autoref{lem:betterfinite}, emulating the proof that the set of algebraic elements is a (sub)field. Namely, for any elements $s_1,s_2\in S$ which are integral over $R$, \autoref{lem:betterfinite} tells us that
	\[R[s_1,s_2]\]
	is a finite $R$-algebra, so all of its elements are integral by \autoref{lem:finiteimpliesintegral}. Thus, $s_1s_1$ and $s_1+s_2$ are integral.
	
	The above argument shows that the integral closure $S'$ is closed under addition and multiplication, so $S'$ is a subring of $S$. Lastly, we note that, for any $r\in R$, the polynomial $x-r\in R[x]$ shows that each $r\in $ is integral. So closure of $S'$ under multiplication shows that $S'$ is also closed under $R$-multiplication, which is the $R$-action. Thus, $S'$ is an $R$-subalgebra of $S$.
\end{proof}
\begin{remark}[Nir] \label{rem:gettingintclosure}
	Here is another corollary of \autoref{lem:betterfinite}. Suppose that $s\in S$ is the root of some monic polynomial
	\[s^n+s_{n-1}s^{n-1}+\cdots+s_1s+s_0=0\]
	where $s_{n-1},\ldots,s_1\in S$ are all integral elements. Then we show $s$ is integral. Indeed, $R[s_0,\ldots,s_{n-1}]$ is integral and hence finite over $R$ by \autoref{lem:betterfinite}.
	
	Thus, $s$ is integral over $R[s_0,\ldots,s_{n-1}]$, so it follows $R[s_0,\ldots,s_{n-1},s]$ is integral and hence finite over $R[s_0,\ldots,s_{n-1}]$ by \autoref{lem:betterfinite}. Then $R[s_0,\ldots,s_{n-1},s]$ is finite over $R$ by \autoref{lem:finiteoverfinite}, so $s$ is integral over $R$ by \autoref{lem:finiteimpliesintegral}, finishing.
\end{remark}
\begin{remark}[Nir]
	The real reason we care about \autoref{rem:gettingintclosure} is to show that the integral closure $S'$ of $S$ over $R$ is ``integrally closed'': we show that any element $s\in S$ integral over $S'$ has $s'\in S$. Indeed, $s\in S$ being integral over $S'$ means that we get a monic polynomial
	\[s^n+s_{n-1}s^{n-1}+\cdots+s_1s+s_0=0,\]
	where $s_{n-1},\ldots,s_0\in S'$. But this means the $s_k$ are integral over $R$ by definition of $S'$, so $s$ is integral over $R$ by \autoref{rem:gettingintclosure}, so $s\in S'$. (Note the analogy between this and showing that the algebraic closure is algebraically closed.)
\end{remark}
We close our discussion by quickly discussing localization: localization commutes with the integral closure.
\begin{proposition}
	Fix $S$ an $R$-algebra with integral closure $S'$; further take $U\subseteq R$ a multiplicative subset. Then $S'\left[U^{-1}\right]$ is the integral closure of $R\left[U^{-1}\right]$ in $S\left[U^{-1}\right]$.
\end{proposition}
\begin{proof}
	We will show $\frac su\in S\left[U^{-1}\right]$ is integral over $R\left[U^{-1}\right]$ if and only if $\frac su=\frac{s'}{u'}$ for some $s'\in S$ is integral over $R$. For this, we attack the directions independently.
	\begin{itemize}
		\item Suppose that $s\in S$ is integral over $R$. Further, fixing any $u\in U$, we show that $\frac su$ is integral over $R\left[U^{-1}\right]$. (It will follow that any $\frac tv$ equal to such a $\frac su$ is also integral.) Well, integrality of $s$ promises a monic polynomial
		\[s^n+r_{n-1}s^{n-1}+\cdots+r_1s+r_0=0\]
		with coefficients in $R$. Transporting to $R\left[U^{-1}\right]$, we multiply everything by $\frac1{u^n}$ to find
		\[\left(\frac su\right)^n+\frac{r_{n-1}}u\left(\frac su\right)^{n-1}+\cdots+\frac{r_1}{u^{n-1}}\left(\frac su\right)+\frac{r_0}{u^n}=0.\]
		So $\frac su$ is the root of a monic polynomial over $R\left[U^{-1}\right]$, finishing.
		\item Conversely, suppose that $\frac su$ is integral over $R\left[U^{-1}\right]$. We show that $\frac su=\frac{s'}{u'}$ for some $\frac{s'}{u'}$ such that $s'$ is integral over $R$. This grants us a monic polynomial
		\[\left(\frac su\right)^n+\frac{r_{n-1}}{u_{n-1}}\left(\frac su\right)^{n-1}+\cdots+\frac{r_1}{u_1}\left(\frac su\right)+\frac{r_0}{u_0}=0.\]
		Now, set $v:=uu_0u_1\cdots u_{n-1}$ and multiply through by $v=u^n$ to get
		\[(vs)^n+\left(v\cdot\frac{r_{n-1}}{u_{n-1}}\right)(vs)^{n-1}+\cdots+\left(v^{n-1}\cdot\frac{r_1}{u_1}\right)(sv)+v^n\cdot\frac{r_0}{u_0}=0.\]
		Each of the coefficients can be made equal to $\frac{r'_\bullet}1$ for some $r'_\bullet\in R$ because $v$ contains within it a factor of each $u_\bullet$. In particular, we see that
		\[\frac{(vs)^n+r'_{n-1}(vs)^{n-1}+\cdots+r'_1(vs)+r'_0}1=\frac01,\]
		so there exists some $v'\in U$ such that $v'\cdot\left((vs)^n+r'_{n-1}(vs)^{n-1}+\cdots+r'_1(vs)+r'_0\right)=0$. In particular, we get
		\[(v'vs)^n+v'r'_{n-1}(v'vs)^{n-1}+\cdots+(v')^{n-1}r'_1(v'vs)+(v')^nr'_0=0,\]
		so $v'vs$ is the root of the monic polynomial in $R$ and hence integral over $R$. So we finish by noting $\frac su=\frac{v'vs}{v'vu}$ because $v'v\in U$.
		\qedhere
	\end{itemize}
\end{proof}

\subsection{Normality}
We have the following definitions.
\begin{definition}[Normal]
	Fix $R$ a domain with field of fractions $K(R)$. Then $R$ is \textit{normal} if and only if $R$ is integrally closed in $K(R)$; i.e., the integral closure of $R$ in $K(R)$ is $R$.
\end{definition}
\begin{definition}[Normalization]
	Fix $R$ a domain with field of fractions $K(R)$. We can define the \textit{normalization} of $R$ to be the integral closure of $R$ in $K(R)$.
\end{definition}
Let's see some examples.
\begin{example}
	The ring $\ZZ$ is normal. This will follow from the following proposition.
\end{example}
\begin{proposition}
	Fix $R$ a unique factorization domain. Then $R$ is normal.
\end{proposition}
\begin{proof}
	Fix any integral element $\frac ab\in K(R)$ with $b\ne0$. If $a=0$, then we note that $\frac ab=\frac01$, which is integral, witnessed by the monic polynomial $x\in R[x]$.
	
	Otherwise, we have $a,b\in R\setminus\{0\}$ so that they each have a unique factorization into irreducibles. The outline is to choose $a$ and $b$ minimally and show that $b$ is a unit to get $\frac ab\in R$. We quickly outsource some work to a lemma.
	\begin{lemma}
		Fix $R$ a unique factorization domain and $\frac ab\in K(R)\setminus\{0\}$. Then we can choose $a'$ and $b'$ such that $\frac ab=\frac{a'}{b'}$ such that no irreducible $\pi$ divides both $a'$ and $b'$.
	\end{lemma}
	\begin{proof}
		Let $q=\frac ab$ and consider all pairs $(a,b)\in R^2$ such that $q=\frac ab$. Note that $q\ne0$ implies that $a,b\ne0$ in all cases because $R$ is an integral domain. Now, it is possible that $a$ and $b$ some number of irreducibles in their factorizations, so choose $a$ and $b$ to minimize the number of shared irreducibles.
		
		We claim that $a$ and $b$ have no common irreducibles. Indeed, suppose $\pi\in R$ is irreducible and $\pi\mid a,b$ with $\pi^\alpha$ and $\pi^\beta$ the largest powers of $\pi$ dividing $a$ and $b$ respectively. In particular, writing out the factorizations for $a=\pi^\alpha\cdot a/\pi^\alpha$ and $b=\pi^\beta\cdot b/\pi^\beta$, we see $\alpha$ and $\beta$ are the exponents in the factorizations.
		
		Now, without loss of generality, we take $\beta\ge\alpha$ and note
		\[\frac ab=\frac{a/\pi^\alpha}{b/\pi^\alpha},\]
		witnessed by $\pi^\alpha\in R\setminus\{0\}$. But now we see that $a/\pi^\alpha$ does not feature the irreducible $\pi$ in its factorization, so $a/\pi^\alpha$ and $b/\pi^\alpha$ share one fewer irreducible, which contradicts the minimality of the chosen $a$ and $b$.
	\end{proof}
	So we can choose $a$ and $b$ to share no common factors. We claim that $b$ is a unit. The main trick of the proof, now, is to use the integrality condition on $\frac ab$ to write a monic polynomial
	\[\left(\frac ab\right)^n+r_{n-1}\left(\frac ab\right)^{n-1}+\cdots r_1\left(\frac ab\right)+r_0=\frac01.\]
	Multiplying through by $b^n$, we see
	\[\frac{a^n+r_{n-1}a^{n-1}b+\cdots r_1ab^n+r_0b^n}1=\frac01,\]
	so because $R$ is a domain, we get
	\[a^n+r_{n-1}a^{n-1}b+\cdots r_1ab^n+r_0b^n=0\]
	in $R$. Rearranging, we have
	\[a^n=-b\left(r_{n-1}a^{n-1}+\cdots r_1ab^n+r_0b^{n-1}\right).\]
	In particular, each irreducible $\pi\in R$ dividing $b$ will divide into $a^n$. Because irreducibles are prime in unique factorization domains (\autoref{rem:ufdimpliesirredisprime}), we see $\pi\mid a^n$ forces $\pi\mid a$, so $\pi$ actually divides both $a$ and $b$!

	But no such irreducible $\pi$ may exist, so $b$ is divisible by no irreducible, so $b$ is a unit by unique factorization. Thus,
	\[\frac ab=\frac{ab^{-1}}{bb^{-1}}=\frac{ab^{-1}}1,\]
	so $ab^{-1}/1$ lived in $R$ all along. This finishes.
\end{proof}
\begin{example}
	The ring $\ZZ[i]$ is a unique factorization domain and hence integrally closed in $K(\ZZ[i])=\QQ(i)$.
\end{example}
\begin{nex}
	The ring $\ZZ\left[\sqrt5\right]$ is not normal. Indeed, our the field of fractions is $\QQ(\sqrt5)$, so we may consider $\frac{1+\sqrt5}2\in\QQ(\sqrt5)\setminus\ZZ\left[\sqrt5\right]$, which is the root of the polynomial
	\[x^2-x-1\]
	by the quadratic formula. However, one can check that the integral closure is $\ZZ\left[\frac{1+\sqrt5}2\right]$, so $\frac{1+\sqrt5}2$ is essentially the only exception. We will not prove this claim because it is on the homework.
\end{nex}
\begin{example}
	The integral closure $\overline\ZZ$ of $\ZZ$ in $\CC$ is the ring of all the roots of monic polynomials; these are called the algebraic integers. For example, $\overline\ZZ\subseteq\overline\QQ$ because being the root of a monic polynomial implies being the root of some polynomial.
\end{example}

\subsection{Normality via Geometry}
There is also a context for normality in algebraic geometry.
\begin{exe}
	We compute the integral closure of the ring $R=k[x,y]/\left(y^2-x^3\right)$.
\end{exe}
\begin{proof}
	Here is our image.
	\begin{center}
		\begin{asy}
			unitsize(1cm);
			import graph;
			real y(real t)
			{
				return t;
			}
			real x(real t)
			{
				return cbrt(t*t);
			}
			draw(graph(x, y,-2,2));
		\end{asy}
	\end{center}
	Note that, working in the fraction field, $\left(\frac yx\right)^2=x$ because $y^2=x^3$, so $R$ is not normal because it does not include $\frac yx$.

	To compute our integral closure, we create a map $R\to k[t]$ by $y\mapsto t^3$ and $x\mapsto t^2$ (so that $t=y/x$), and we find $R$ embeds into $k[t]$. But because $k[t]$ is now integrally closed (it's a unique factorization domain), we see that the pull-back $R[y/x]$ will in fact be integrally closed, so this is our integral closure.
\end{proof}
\begin{example}
	Consider the ring $R=k[x,y]/\left(y^2-x^2(x+1)\right)$. Then $\left(\frac yx\right)^2=x+1$, so $R$ is not normal because it does not include $\frac yx$.
\end{example}
More generally, suppose that we have affine algebraic sets $X$ and $Y$ with an embedding $A(X)\to A(Y)$. This corresponds to a map $Y\to X$. Normality then means that the image of $Y$ in $X$ is ``Zariski dense'' so that there is no proper closed subset of $X$ which contains $Y$.

Speaking with more geometry, a map $Y\to X$ of affine varieties is proper (over $\CC$, say) essentially gives us the result that the pre-image of a compact set is compact.
\begin{remark}
	I did not follow the above discussion.
\end{remark}
We have the following proposition.
\begin{proposition}
	Fix $S$ an $R$-algebra with a monic polynomial $f\in R[x]$. If we can factor $f=gh$ for $g,h\in S[x]$. Then the coefficients of $g$ and $h$ are integral over $R$.
\end{proposition}
\begin{proof}
	Imagine adding some root $\alpha_1$ of $g$ to $S$ to get a bigger $R$-algebra named $R[\alpha_1]$. So, writing $g(x)=(x-\alpha_1)g_1(x)$, we see that we can divide out to get
	\[\frac{f(x)}{(x-\alpha_1)}=g_1(x)h(x).\]
	Inductively removing all roots $\alpha_1,\ldots,\alpha_m$ of $g$ and $\beta_1,\ldots,\beta_n$ of $h$, we see that
	\[f(x)=(x-\alpha_1)\cdots(x-\alpha_m)(x-\beta_1)\cdots(x-\beta_n).\]
	Here the leading coefficients match, so we do not inherit a leading term. However, upon expansion, we see that the coefficients of $g$ and $h$ will be elementary symmetric functions of the $\alpha_\bullet$ and $\beta_\bullet$, so in particular they will all be contained in the finite extension $R[\alpha_1,\ldots,\alpha_m,\beta_1,\ldots,\beta_n]$ and hence be integral.
\end{proof}
\begin{corollary}
	Fix $R$ a normal domain and $f(x)\in R[x]$ some monic polynomial. Then, if $f(x)$ is irreducible, then $f(x)$ is prime.
\end{corollary}
\begin{proof}
	Fix $f(x)\in R[x]$. Then $f$ will remain irreducible in $K(R)$, which comes from the above proposition. In particular, we are promised an embedding
	\[\frac{R[x]}{(f(x))}\into\frac{K[x]}{(f(x))},\]
	so $R[x]/(f(x))$ is a subring of a field and hence an integral domain.
\end{proof}
\begin{remark}
	This generalizes the result that, if $R$ is a unique factorization domain, then $R[x]$ is also a unique factorization domain.
\end{remark}

\subsection{Lifting Primes}
Speaking generally for a moment, suppose we have an $S$-algebra $R$. Then, if $\varphi:R\to S$ is our promised map, we note that we have a map $\op{Spec}S\to\op{Spec}R$ by $\varphi^{-1}:\mf q\mapsto\varphi^{-1}(\mf q)$. In particular, thinking of $\varphi$ as providing an ``embedding'' $R\subseteq S$, we get that primes $\mf q$ of $S$ go to
\[\varphi^{-1}(\mf q)=\{r\in R:\varphi(r)\in\mf q\}=:\mf q\cap R,\]
where we are setting this equal to $\mf q\cap R$ by abuse of notation.

When $R\subseteq S$ is an integral extension, we get some control of the map $\varphi^{-1}$.
\begin{proposition}
	Fix $S$ an $R$-algebra by $\varphi:R\to S$. If $R\subseteq S$ is an integral extension of rings, then the map $\varphi^{-1}:\op{Spec}S\to\op{Spec}R$ is surjective. In other words, for any $\mf p\in\op{Spec}R$, there exists $\mf q\in\op{Spec}S$ such that $\mf q\cap R=\mf p$.
\end{proposition}
\begin{proof}
	Set $U:=R\setminus\mf p$, and we will localize at $U$. Because localization is flat and hence preserves embeddings (\autoref{rem:ufdimpliesirredisprime}), we get an embedding $R_\mf p=R\left[U^{-1}\right]\subseteq S\left[U^{-1}\right]$. It will suffice to show the statement for the localization because then we can pre-image back to the original statement.

	Now, by how primes work in localization, we know that
	\[\mf pS\left[U^{-1}\right]\cap R_\mf p=\mf p.\]
	Thus, because $\mf p$ is the unique maximal ideal of $R_\mf p$, it suffices to put $\mf pS\left[U^{-1}\right]$ in any larger ideal and then pull-back, as long as we don't get the full ring $R\left[U^{-1}\right]$.

	Well, any maximal ideal containing $\mf pS\left[U^{-1}\right]$ will do, so we have to show $\mf pS\left[U^{-1}\right]\cap R_\mf p=R_\mf p$. Well, suppose for the sake of contradiction this is true so that
	\[1=p_1s_1+\cdots+p_ns_n\]
	for some $p_1,\ldots,p_n\in\mf p$ and $s_1,\ldots,s_n\in S$. But then $M=R[s_1,\ldots,s_n]$ is a finitely generated $R$-module (by integrality) where $\mf pM=M$ (because of the above equation), which forces $M=0$ by Nakayama's lemma, which is a contradiction.
\end{proof}
In fact, we have the following.
\begin{corollary}
	Fix $R\subseteq S$ an integral extension. Further, if $I\subseteq R$ is an ideal with $SI\subseteq R\subseteq\mf p$ for some $\mf p\in\op{Spec}R$, then we can choose $\mf q$ with $\mf q\cap R=\mf p$ which contains $I$.
\end{corollary}
\begin{proof}
	One can work in the integral extension $R/I\subseteq S/SI$ and then use the previous proposition.
\end{proof}
In the case of domains, we have some communication with the field extensions. 
\begin{lemma}
	Fix $R\subseteq S$ an integral extension of domains. Then $K(S)$ is algebraic over $K(R)$.
\end{lemma}
\begin{proof}
	This follows from simply choosing finitely many integral generators of $S$ over $R$.
\end{proof}
This gives us the following lack of ``avoidance'' in integral domains.
\begin{prop}
	Fix $R\subseteq S$ an integral extension of domains and $I\ne0$ a nonzero ideal of $S$. Then $I\cap R\ne0$.
\end{prop}
\begin{proof}
	Suppose $b\in I$. By writing out the polynomial for $b$ over $K(R)$ and then multiplying out by all the denominators, we get some equation in $R$ of the form
	\[a_nb^n+\cdots+a_0=0.\]
	By forcing $n$ minimal, we get $a_0\ne0$ (here we use that these are domains), but then $a_0\in Sb\subseteq I$ as well as $a_0\in R$. This finishes.
\end{proof}
\begin{proposition}
	Fix $R\subseteq S$ an extension of integral domains. Then, $R$ is a field if and only if $S$ is a field.
\end{proposition}
\begin{proof}
	In one direction, if $R$ is a field, then take any $s\in S$ and write out its equation
	\[s^n+a_1s^{n-1}+\cdots+a_0=0.\]
	Again, we can force $a_0\ne0$, so $a_0\in R$ is a unit. By factoring out $s$ from the first $n$ terms, we get $s(\text{stuff})=-a_0\in R^\times$, so $s$ is a unit.

	In the other direction, suppose for the sake of contradiction that $S$ is a field while $S$ is not. Then $R$ has some nonzero maximal ideal $\mf p$ which lifts to a nonzero maximal ideal $\mf P$ up in $S$. But the only ideals of $S$ are $(0)$ or $S$, neither of which can be the lift of $\mf P$.
\end{proof}