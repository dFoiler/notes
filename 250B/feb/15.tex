% !TEX root = ../notes.tex

Here we go.
\begin{convention}
	For today's lecture, an $R$-algebra $S$ should be thought of as providing an embedding $R\subseteq S$ (though we will not actually assume that this ring map is injective until the end).
\end{convention}

\subsection{A Better Integrality}
Last time we introduced the following proposition.
\integralitydef*
\noindent This gave rise to the following definition.
\integraldefi*
\noindent Being integral is intended to be a generalization of having a finite extension of fields. Along these lines, we get the following definition.
\finitedefi*
\noindent As with fields, we know that any finite field extension must be algebraic, so we might hope that an integral extension is also finite.
\begin{lemma} \label{lem:finiteimpliesintegral}
	Every finite $R$-algebra $S$ is integral.
\end{lemma}
\begin{proof}
	We use the Cayley--Hamilton theorem. Fix any element $s\in S$ so that we want to show $s$ is integral over $R$. The key point is that $\mu_s:x\mapsto sx$ is an endomorphism $\mu_s:S\to S$ of $S$ as an $R$-module. Namely, for $s_1,s_2\in S$ and $r_1,r_2\in R$, we merely have to check that
	\[\mu_s(r_1s_1+r_2s_2)=sr_1s_1+sr_2s_2=r_1ss_1+s_2ss_2=r_1\mu_s(s_1)+r_2\mu_s(s_2).\]
	Thus, \autoref{thm:betterch} promises a monic polynomial
	\[p_{\mu_s}(x)=x^n+\sum_{k=0}^{n-1}r_kx^k\]
	such that $p_{\mu_s}(\mu_s)=0$. In particular, we see that
	\[0=0\cdot1=p_{\mu_s}(\mu_s)(1)=\left(\mu_s^n+\sum_{k=0}^{n-1}r_k\mu_s^k\right)(1)=\mu_s^n(1)+\sum_{k=0}^{n-1}r_k\mu_s^k(1)=s^n+\sum_{k=0}^{n-1}r_ks^k,\]
	which verifies that $s$ is the root of a monic polynomial $p_{\mu_s}(x)\in R[x]$.
\end{proof}
In fact, we can provide a converse.
\begin{lemma} \label{lem:betterfinite}
	Fix $S$ an $R$-algebra. Then $S$ is finite if and only if it is finitely generated as an $R$-algebra with integral generators.
\end{lemma}
\begin{proof}
	We show the two directions independently. The key to the backwards direction will be the following lemma.
	\begin{lemma} \label{lem:finiteoverfinite}
		If $A\subseteq B\subseteq C$ are rings, then if $C$ is finite over $B$ and $B$ is finite over $A$, then $C$ is finite over $A$.
	\end{lemma}
	\begin{proof}
		Because $C$ is finite over $B$, we get generators $c_1,\ldots,c_n$. Similarly, because $B$ is finite over $A$, we get generators $b_1,\ldots,b_m$. We claim that the $b_ic_j$ generate $C$ as an $A$-module, which will finish the proof.

		Indeed, any $c\in C$ we can write as
		\[c=\sum_{j=1}^nr_jc_j\]
		where $r_j\in B$. Then, expanding the $r_j$ along the generators $b_1,\ldots,b_m$, we get
		\[r_j=\sum_{i=1}^ms_{ij}b_i.\]
		Distributing, we see that
		\[c=\sum_{j=1}^n\sum_{i=1}^ns_{ij}(b_ic_j),\]
		which finishes the proof.
	\end{proof}
	We now attack the proof directly.
	\begin{itemize}
		\item In one direction, suppose that $S=R[s_1,\ldots,s_n]$ where the elements $s_1,\ldots,s_n$ are all integral over $R$. The key is to consider the chain
		\[R\subseteq R[s_1]\subseteq R[s_1,s_2]\subseteq\cdots\subseteq R[s_1,\ldots,s_n].\]
		We show that $R[s_1,\ldots,s_k]$ is finite over $R$ by induction on $k$; when $k=0$, there is nothing to say. For the inductive step, suppose that some $R[s_1,\ldots,s_k]$ is finite over $R$ by the elements $\{x_1,\ldots,x_m\}$, and we show $R[s_1,\ldots,s_{k+1}]$ is finite over $R$.
		
		Well, $s_{k+1}$ is the root of some monic polynomial which we name $p\in R[x]$. But $p\in R[s_0,\ldots,s_k]$ as well, so $s_{k+1}$ is integral over $R[s_0,\ldots,s_k]$. Thus, by \autoref{prop:integralitydef}, $R[s_1,\ldots,s_k][s_{k+1}]$ will still be finitely generated as an $R[s_1,\ldots,s_k]$-module.

		It follows that $R[s_1,\ldots,s_{k+1}]$ is finitely generated as an $R$-module by \autoref{lem:finiteoverfinite}.

		\item In the other direction, take $S$ a finite $R$-algebra. In particular, we see that $S=Rs_0+\cdots+s_nR$ for some elements $s_0,\ldots,s_n\in S$. But \autoref{lem:finiteimpliesintegral} now forces the elements $s_k$ to all be integral, so we see that $S$ is finite over $R$ with generators which are integral.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}[Nir]
	The real point of the above discussion is to give a better description of integral elements looks like: they generate finite algebras. (Again, note the analogy with fields: algebraic elements generate finite field extensions.) This will be more apparent in \autoref{prop:integralclosure}.
\end{remark}

\subsection{The Integral Closure}
Sometimes an algebra isn't integral, but we can always make an integral extension.
\begin{definition}[Integral closure]
	Fix $S$ an $R$-algebra. Then the \textit{integral closure} $S'$ of $S$ over $R$ is the set of all elements of $S$ which are integral over $R$.
\end{definition}
\begin{remark}
	The integral closure depends on the choice of $S$: making $S$ bigger permits more integral elements. This is analogous to the algebraic closure of a field technically depending on our choice of parent field.
\end{remark}
\begin{proposition} \label{prop:integralclosure}
	Fix $S$ an $R$-algebra. Then the integral closure $S'$ of $S$ is an $R$-subalgebra of $S$. In particular, if $s_1,s_2\in S$ are integral elements, then $s_1+s_2$ and $s_1s_2$ are also both integral elements.
\end{proposition}
\begin{proof}
	The main idea is to use \autoref{lem:betterfinite}, emulating the proof that the set of algebraic elements is a (sub)field. Namely, for any elements $s_1,s_2\in S$ which are integral over $R$, \autoref{lem:betterfinite} tells us that
	\[R[s_1,s_2]\]
	is a finite $R$-algebra, so all of its elements are integral by \autoref{lem:finiteimpliesintegral}. Thus, $s_1s_1$ and $s_1+s_2$ are integral.
	
	The above argument shows that the integral closure $S'$ is closed under addition and multiplication, so $S'$ is a subring of $S$. Lastly, we note that, for any $r\in R$, the polynomial $x-r\in R[x]$ shows that each $r\in $ is integral. So closure of $S'$ under multiplication shows that $S'$ is also closed under $R$-multiplication, which is the $R$-action. Thus, $S'$ is an $R$-subalgebra of $S$.
\end{proof}
\begin{remark}[Nir] \label{rem:gettingintclosure}
	Here is another corollary of \autoref{lem:betterfinite}. Suppose that $s\in S$ is the root of some monic polynomial
	\[s^n+s_{n-1}s^{n-1}+\cdots+s_1s+s_0=0\]
	where $s_{n-1},\ldots,s_1\in S$ are all integral elements. Then we show $s$ is integral. Indeed, $R[s_0,\ldots,s_{n-1}]$ is integral and hence finite over $R$ by \autoref{lem:betterfinite}.
	
	Thus, $s$ is integral over $R[s_0,\ldots,s_{n-1}]$, so it follows $R[s_0,\ldots,s_{n-1},s]$ is integral and hence finite over $R[s_0,\ldots,s_{n-1}]$ by \autoref{lem:betterfinite}. Then $R[s_0,\ldots,s_{n-1},s]$ is finite over $R$ by \autoref{lem:finiteoverfinite}, so $s$ is integral over $R$ by \autoref{lem:finiteimpliesintegral}, finishing.
\end{remark}
\begin{remark}[Nir]
	The real reason we care about \autoref{rem:gettingintclosure} is to show that the integral closure $S'$ of $S$ over $R$ is ``integrally closed'': we show that any element $s\in S'$ integral over $S'$ has $s\in S'$. Indeed, $s\in S$ being integral over $S'$ means that we get a monic polynomial
	\[s^n+s_{n-1}s^{n-1}+\cdots+s_1s+s_0=0,\]
	where $s_{n-1},\ldots,s_0\in S'$. But this means the $s_k$ are integral over $R$ by definition of $S'$, so $s$ is integral over $R$ by \autoref{rem:gettingintclosure}, so $s\in S'$. (Note the analogy between this and showing that the algebraic closure is algebraically closed.)
\end{remark}
We close our discussion by quickly discussing localization: localization commutes with the integral closure.
\begin{proposition}
	Fix $S$ an $R$-algebra with integral closure $S'$; further take $U\subseteq R$ a multiplicative subset. Then $S'\left[U^{-1}\right]$ is the integral closure of $R\left[U^{-1}\right]$ in $S\left[U^{-1}\right]$.
\end{proposition}
\begin{proof}
	We will show $\frac su\in S\left[U^{-1}\right]$ is integral over $R\left[U^{-1}\right]$ if and only if $\frac su=\frac{s'}{u'}$ for some $s'\in S$ is integral over $R$. For this, we attack the directions independently.
	\begin{itemize}
		\item Suppose that $s\in S$ is integral over $R$. Further, fixing any $u\in U$, we show that $\frac su$ is integral over $R\left[U^{-1}\right]$. (It will follow that any $\frac tv$ equal to such a $\frac su$ is also integral.) Well, integrality of $s$ promises a monic polynomial
		\[s^n+r_{n-1}s^{n-1}+\cdots+r_1s+r_0=0\]
		with coefficients in $R$. Transporting to $R\left[U^{-1}\right]$, we multiply everything by $\frac1{u^n}$ to find
		\[\left(\frac su\right)^n+\frac{r_{n-1}}u\left(\frac su\right)^{n-1}+\cdots+\frac{r_1}{u^{n-1}}\left(\frac su\right)+\frac{r_0}{u^n}=0.\]
		So $\frac su$ is the root of a monic polynomial over $R\left[U^{-1}\right]$, finishing.
		\item Conversely, suppose that $\frac su$ is integral over $R\left[U^{-1}\right]$. We show that $\frac su=\frac{s'}{u'}$ for some $\frac{s'}{u'}$ such that $s'$ is integral over $R$. This grants us a monic polynomial
		\[\left(\frac su\right)^n+\frac{r_{n-1}}{u_{n-1}}\left(\frac su\right)^{n-1}+\cdots+\frac{r_1}{u_1}\left(\frac su\right)+\frac{r_0}{u_0}=0.\]
		Now, set $v:=u_0u_1\cdots u_{n-1}$ and multiply through by $(uv)^n$ to get
		\[(vs)^n+\left((uv)\cdot\frac{r_{n-1}}{u_{n-1}}\right)(vs)^{n-1}+\cdots+\left((uv)^{n-1}\cdot\frac{r_1}{u_1}\right)(vs)+(uv)^n\cdot\frac{r_0}{u_0}=0.\]
		Each of the coefficients can be made equal to $\frac{r'_\bullet}1$ for some $r'_\bullet\in R$ because $v$ contains within it a factor of each $u_\bullet$. In particular, we see that
		\[\frac{(vs)^n+r'_{n-1}(vs)^{n-1}+\cdots+r'_1(vs)+r'_0}1=\frac01,\]
		so there exists some $v'\in U$ such that $v'\cdot\left((vs)^n+r'_{n-1}(vs)^{n-1}+\cdots+r'_1(vs)+r'_0\right)=0$. In particular, multiplying through by an additional $(v')^{n-1}$, we get
		\[(v'vs)^n+v'r'_{n-1}(v'vs)^{n-1}+\cdots+(v')^{n-1}r'_1(v'vs)+(v')^nr'_0=0,\]
		so $v'vs$ is the root of the monic polynomial in $R$ and hence integral over $R$. So we finish by noting $\frac su=\frac{v'vs}{v'vu}$ because $v'v\in U$.
		\qedhere
	\end{itemize}
\end{proof}
\begin{corollary} \label{cor:localizeintegral}
	Fix $S$ an integral $R$-algebra. Then $S\left[U^{-1}\right]$ is an integral $R\left[U^{-1}\right]$-algebra.
\end{corollary}
\begin{proof}
	Because $S$ is integral over $R$, we see that $S$ is the integral closure of $R$ in $S$. Thus, by the proposition, $S\left[U^{-1}\right]$ is the integral closure of $R\left[U^{-1}\right]$ in $S\left[U^{-1}\right]$, so $S\left[U^{-1}\right]$ is an integral $R\left[U^{-1}\right]$-algebra.
\end{proof}

\subsection{Normality}
We have the following definitions.
\begin{definition}[Normal]
	Fix $R$ a domain with field of fractions $K(R)$. Then $R$ is \textit{normal} if and only if $R$ is integrally closed in $K(R)$; i.e., the integral closure of $R$ in $K(R)$ is $R$.
\end{definition}
\begin{definition}[Normalization]
	Fix $R$ a domain with field of fractions $K(R)$. We can define the \textit{normalization} of $R$ to be the integral closure of $R$ in $K(R)$.
\end{definition}
Let's see some examples.
\begin{example}
	The ring $\ZZ$ is normal. This will follow from the following proposition.
\end{example}
\begin{proposition} \label{prop:ufdnormal}
	Fix $R$ a unique factorization domain. Then $R$ is normal.
\end{proposition}
\begin{proof}
	Fix any integral element $\frac ab\in K(R)$ with $b\ne0$. If $a=0$, then we note that $\frac ab=\frac01$, which is integral, witnessed by the monic polynomial $x\in R[x]$.
	
	Otherwise, we have $a,b\in R\setminus\{0\}$ so that they each have a unique factorization into irreducibles. The outline is to choose $a$ and $b$ minimally and show that $b$ is a unit to get $\frac ab\in R$. We quickly outsource some work to a lemma.
	\begin{lemma}
		Fix $R$ a unique factorization domain and $\frac ab\in K(R)\setminus\{0\}$. Then we can choose $a'$ and $b'$ such that $\frac ab=\frac{a'}{b'}$ such that no irreducible $\pi$ divides both $a'$ and $b'$.
	\end{lemma}
	\begin{proof}
		Let $q=\frac ab$ and consider all pairs $(a,b)\in R^2$ such that $q=\frac ab$. Note that $q\ne0$ implies that $a,b\ne0$ in all cases because $R$ is an integral domain. Now, it is possible that $a$ and $b$ some number of irreducibles in their factorizations, so choose $a$ and $b$ to minimize the number of shared irreducibles.
		
		We claim that $a$ and $b$ have no common irreducibles. Indeed, suppose $\pi\in R$ is irreducible and $\pi\mid a,b$ with $\pi^\alpha$ and $\pi^\beta$ the largest powers of $\pi$ dividing $a$ and $b$ respectively. In particular, writing out the factorizations for $a=\pi^\alpha\cdot a/\pi^\alpha$ and $b=\pi^\beta\cdot b/\pi^\beta$, we see $\alpha$ and $\beta$ are the exponents in the factorizations.
		
		Now, without loss of generality, we take $\beta\ge\alpha$ and note
		\[\frac ab=\frac{a/\pi^\alpha}{b/\pi^\alpha},\]
		witnessed by $\pi^\alpha\in R\setminus\{0\}$. But now we see that $a/\pi^\alpha$ does not feature the irreducible $\pi$ in its factorization, so $a/\pi^\alpha$ and $b/\pi^\alpha$ share one fewer irreducible, which contradicts the minimality of the chosen $a$ and $b$.
	\end{proof}
	So we can choose $a$ and $b$ to share no common factors. We claim that $b$ is a unit. The main trick of the proof, now, is to use the integrality condition on $\frac ab$ to write a monic polynomial
	\[\left(\frac ab\right)^n+r_{n-1}\left(\frac ab\right)^{n-1}+\cdots r_1\left(\frac ab\right)+r_0=\frac01.\]
	Multiplying through by $b^n$, we see
	\[\frac{a^n+r_{n-1}a^{n-1}b+\cdots r_1ab^n+r_0b^n}1=\frac01,\]
	so because $R$ is a domain, we get
	\[a^n+r_{n-1}a^{n-1}b+\cdots r_1ab^n+r_0b^n=0\]
	in $R$. Rearranging, we have
	\[a^n=-b\left(r_{n-1}a^{n-1}+\cdots r_1ab^n+r_0b^{n-1}\right).\]
	In particular, each irreducible $\pi\in R$ dividing $b$ will divide into $a^n$. Because irreducibles are prime in unique factorization domains (\autoref{rem:ufdimpliesirredisprime}), we see $\pi\mid a^n$ forces $\pi\mid a$, so $\pi$ actually divides both $a$ and $b$!

	But no such irreducible $\pi$ may exist, so $b$ is divisible by no irreducible, so $b$ is a unit by unique factorization. Thus,
	\[\frac ab=\frac{ab^{-1}}{bb^{-1}}=\frac{ab^{-1}}1,\]
	so $ab^{-1}/1$ lived in $R$ all along. This finishes.
\end{proof}
\begin{example}
	The ring $\ZZ[i]$ is a unique factorization domain and hence integrally closed in $K(\ZZ[i])=\QQ(i)$.
\end{example}
\begin{nex} \label{nex:zrootfivenotnormal}
	The ring $\ZZ\left[\sqrt5\right]$ is not normal. Indeed, our the field of fractions is $\QQ(\sqrt5)$, so we may consider $\frac{1+\sqrt5}2\in\QQ(\sqrt5)\setminus\ZZ\left[\sqrt5\right]$, which is the root of the polynomial
	\[x^2-x-1\]
	by the quadratic formula. However, one can check that the integral closure is $\ZZ\left[\frac{1+\sqrt5}2\right]$, so $\frac{1+\sqrt5}2$ is essentially the only exception. We will not prove this claim because it is on the homework.
\end{nex}
\begin{example}
	The integral closure $\overline\ZZ$ of $\ZZ$ in $\CC$ is the ring of all the roots of monic polynomials; these are called the algebraic integers. For example, $\overline\ZZ\subseteq\overline\QQ$ because being the root of a monic polynomial implies being the root of some polynomial.
\end{example}
\begin{remark}
	Of course, not all normal rings are unique factorization domains. For example, $\ZZ\left[\sqrt{-5}\right]$ is normal but not a unique factorization domain (by \autoref{warn:nonufd}). The fact that $\ZZ[\sqrt{-5}]$ is normal in $\QQ(\sqrt{-5})$ is a problem on the homework.
	% To see that $\ZZ\left[\sqrt{-5}\right]$ is normal, suppose that $a+b\sqrt{-5}\in\QQ(\sqrt{-5})$ is integral over $\ZZ\left[\sqrt{-5}\right]$ and hence integral over $\ZZ$. Then one can show that the Galois conjugate $a-b\sqrt{-5}$ is also integral over $\ZZ$ (it's the root of the same polynomial), so
	% \[2a=\left(a+b\sqrt{-5}\right)+\left(a-b\sqrt{-5}\right)\qquad\text{and}a^2+5b^2=\left(a+b\sqrt{-5}\right)\left(a-b\sqrt{-5}\right)\]
	% are also both integral over $\ZZ$ and hence integers. If $a\in\ZZ$, then $5b^2\in\ZZ$, so $b\in\ZZ$ because $5$ is squarefree. Otherwise, $a\in\frac12+\ZZ$ and then $a^2+5b^2\in\ZZ$ forces $b\in\frac12+\ZZ$. Subtracting, it follows $\frac12+\frac12\sqrt{-5}$ is integral, but this implies $\frac{1+5}2\in\ZZ$ by the above, which is false.
\end{remark}

\subsection{Normality via Geometry}
There is also a context for normality in algebraic geometry; roughly speaking, it is about trying to make the curve smoother.
\begin{exe}
	We compute the integral closure of the ring $R=k[x,y]/\left(y^2-x^3\right)$ as $R[y/x]$.
\end{exe}
\begin{proof}
	Here is our image.
	\begin{center}
		\begin{asy}
			unitsize(1cm);
			import graph;
			real y(real t)
			{
				return t;
			}
			real x(real t)
			{
				return cbrt(t*t);
			}
			draw(graph(x, y,-2,2));
			draw((-1,0) -- (2,0), dotted); label("$x$", (2,0), E);
			draw((0,-2) -- (0,2), dotted); label("$y$", (0,2), N);
		\end{asy}
	\end{center}
	The issue here is the ``cusp'' at $0$. To normalize, we need to make this curve look more like a line and normalize as a line.

	So the main point is that there is a map $\varphi:k[x,y]\to k[t]$ by sending $x\mapsto t^2$ and $y\mapsto t^3$, and it is not too hard to check that the kernel of this map is $y^2-x^3$. Indeed, certainly
	\[\varphi\left(y^2-x^3\right)=t^6-t^6=0,\]
	so $\left(y^2-x^3\right)\subseteq\ker\varphi$. Conversely, if $f(x,y)\in\ker\varphi$, then we can use the fact $y^2\equiv x^3\pmod{y^2-x^3}$ to write
	\[f(x,y):=\sum_{k,\ell\in\NN}a_{k,\ell}x^ky^\ell\equiv\sum_{k=0}^\infty b_kx^k+y\sum_{k=0}^\infty c_kx^k\pmod{y^2-x^3},\]
	where $b_k$ and $c_k$ are some sequences which vanish for all but finitely many values. Then, passing this through $\varphi$, we see that the left- and right-hand polynomials go to the same polynomials, but the right-hand side evaluates as
	\[\sum_{k=0}^\infty b_kt^{2k}+\sum_{k=0}^\infty c_kt^{2k+3},\]
	which must vanish component-wise. So indeed, $f\equiv0\pmod I$, so $f\in I$.

	It follows that we have an embedding $\varphi:R\into k[t]$, and in fact we can track the image as $k\left[t^2,t^3\right]\subseteq k[t]$; in the other direction, note that we can extend this to an isomorphism $K(R)\to k(t)$. So it suffices to compute the integral closure of $k\left[t^2,t^3\right]$ in $k(t)$ and then pull back.

	Well, $t\in k(t)$ is the root of the polynomial $x^2-t^2=0$ in $k\left[t^2,t^3\right][x]$, so the integral closure must contain $k[t]$. However, $k[t]$ itself is integrally closed (by \autoref{prop:ufdnormal}), so it is the integral closure of $k\left[t^2,t^3\right]$; more explicitly, if $f$ is integral over $k\left[t^2,t^3\right]$, then it will be integral over $k[t]$ (because the coefficients of $f$'s polynomial also live in $k[t]$), so it will be in $k[t]$.

	So to finish, we note that $\varphi(y/x)=t$, so because $\varphi:K(R)\to k(t)$ is injective, we see that we can just callously pull $t$ back to $y/x$. In particular, our integral closure is
	\[\varphi^{-1}(k[t])=\boxed{R[y/x]},\]
	which is our answer.
\end{proof}
\begin{exe}
	We compute the integral closure of $R=k[x,y]/\left(y^2-x^2(x+1)\right)$ as $R[y/x]$.
\end{exe}
\begin{proof}
	Here is our image.
	\begin{center}
		\begin{asy}
			unitsize(1cm);
			import graph;
			real x(real t)
			{
				return t*t-1;
			}
			real y(real t)
			{
				return t*(t*t-1);
			}
			draw(graph(x, y,-1.521,1.521));
			draw((-1.5,0) -- (2,0), dotted); label("$x$", (2,0), E);
			draw((0,-2) -- (0,2), dotted); label("$y$", (0,2), N);
		\end{asy}
	\end{center}
	Once more, the issue here is the singularity at $0$. So to normalize, we will make this look more like a line and then normalize as a line.

	The key to effect this plan is to note that we have a map $\varphi:k[x,y]\to k[t]$ by sending
	\[x\mapsto t^2-1\qquad\text{and}\qquad y\mapsto t\left(t^2-1\right).\]
	We can again check that the kernel of this mapping is $y^2-x^2(x+1)$, but we will be a little sketchier. On one hand, we compute
	\[\varphi\left(y^2-x^2(x+1)\right)=t^2\left(t^2-1\right)-\left(t^2-1\right)^2\left(t^2-1+1\right)=0.\]
	On the other hand, if $f(x,y)\in\ker\varphi$, then we can use the fact that $y^2\equiv x^2(x+1)\pmod{y^2-x^2(x+1)}$ to reduce the exponent of $y$ and write
	\[f(x,y)\equiv\sum_{k=0}^\infty b_kx^k+\sum_{k=0}^\infty c_kx^ky\pmod{y^2-x^2(x+1)}.\]
	Because $\left(y^2-x^2(x+1)\right)\subseteq\ker\varphi$, both sides of this equivalence will go to the same place upon being pushed through $\varphi$. However, upon pushing through $\varphi$, we see that the left-hand sum only creates terms of even degree, and the right-hand sum only creates terms of odd degree, so it is not too hard to see that we must have $b_k=c_k=0$ for each $k$. Explicitly, the term of the largest degree in either sum must vanish by looking in $k[t]$.

	The rest of the argument proceeds as before. We note that
	\[\im\varphi=k\left[t^2-1,t\left(t^2-1\right)\right],\]
	and we will compute the integral closure of $\im\varphi$. As before, we note that $t$ is a root of
	\[x^2-\left(t^2-1\right)-1\in(\im\varphi)[x],\]
	so $t$ must live in the integral closure. However, $k[t]\supseteq k\left[t^2-1,t\left(t^2-1\right)\right]$ is integrally closed, so this now must be our integral closure. Pulling back, we note that $\varphi(y/x)=t$, so our integral closure is
	\[\varphi^{-1}(k[t])=\boxed{R[y/x]},\]
	which is what we wanted.
\end{proof}
\begin{remark}[Nir]
	Somewhere around here Professor Serganova gave a more rigorously sound discussion of normality via geometry, but I did not follow it. My notes are included in the comments on this file, but they are pretty incomprehensible.
\end{remark}
% More generally, suppose that we have affine algebraic sets $X$ and $Y$ with an embedding $A(X)\to A(Y)$. This corresponds to a map $Y\to X$. Normality then means that the image of $Y$ in $X$ is ``Zariski dense'' so that there is no proper closed subset of $X$ which contains $Y$.

% Speaking with more geometry, a map $Y\to X$ of affine varieties is proper (over $\CC$, say) essentially gives us the result that the pre-image of a compact set is compact.
% \begin{remark}
% 	I did not follow the above discussion.
% \end{remark}

\subsection{Normality and Factorization}
\autoref{prop:ufdnormal} suggests some connection between normality and unique factorization, but the connection is clearer when working in the polynomial ring. To start, we have the following proposition.
\begin{proposition}
	Fix $S$ an $R$-algebra by an injective map $\varphi:R\into S$. If we can factor a monic polynomial $f\in R[x]$ by $f=gh$ for monic $g,h\in S[x]$, then the coefficients of $g$ and $h$ are integral over $R$.
\end{proposition}
\begin{proof}
	The idea is to force a factorization. To start off, we note that $g$ is monic, we can work in the ring
	\[\frac{S[\alpha_1]}{(g(\alpha_1))},\]
	which is a finite, free $S$-algebra generated by a power basis, from \autoref{prop:integralitydef}. In particular, there is an embedding $S\into S[\alpha_1]/(g(\alpha_1))$. The point of doing this is that $g(\alpha_1)=0$, so doing long division by $(x-\alpha_1)$ by hand gives
	\[g(x)=(x-\alpha_1)g_1(x),\]
	where $g_1(x)$ is again monic by comparing leading coefficients (which makes sense as long as the leading coefficients are not zero-divisors). In particular, if the leading term of $g(x)$ is $x^n$, then the leading term of $g_1(x)$ will have to be $x^{n-1}$ to be able to achieve $x^n$ and no further.
	
	Thus, $g_1(x)$ is monic of strictly smaller degree, so we can inductively continue this process to get
	\[g(x)=(x-\alpha_1)(x-\alpha_2)\cdots(x-\alpha_n)\]
	with coefficients in $S[\alpha_1,\ldots,\alpha_n]$.

	Running the same process for $h$ but now starting with $S[\alpha_1,\ldots,\alpha_n]$, we see that we can factor
	\[h(x)=(x-\beta_1)(x-\beta_2)\cdots(x-\beta_m)\]
	with coefficients in $S[\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_m]$.

	The key trick, now, is to imagine working in $R[\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_m]\subseteq S[\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_m]$. The point is that
	\[f(x)=g(x)h(x)=(x-\alpha_1)(x-\alpha_2)\cdots(x-\alpha_n)\cdot(x-\beta_1)(x-\beta_2)\cdots(x-\beta_m),\]
	so each of the $\alpha_\bullet$s and $\beta_\bullet$s are in fact the roots of the monic polynomial $f(x)\in R[x]$ and therefore integral over $R$. In particular, $R[\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_m]$ is generated by finitely many integral elements, so it is finite and hence integral by \autoref{lem:betterfinite}. So when we expand
	\[g(x)=(x-\alpha_1)(x-\alpha_2)\cdots(x-\alpha_n)\qquad\text{and}\qquad h(x)=(x-\beta_1)(x-\beta_2)\cdots(x-\beta_m),\]
	we see that their coefficients will live in $R[\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_m]$ and so must be integral over $R$.
\end{proof}
And here is our application.
\begin{corollary}
	Fix $R$ a normal domain. Then, if $f(x)\in R[x]$ is a monic irreducible, then $f(x)$ is prime.
\end{corollary}
\begin{proof}
	Fix $f(x)\in R[x]$. Then we claim $f$ will remain irreducible in $K(R)$, which comes from the above proposition: if we factor $f=g_0h_0$ in $K(R)$, comparing leading coefficients of $g_0$ and $h_0$ lets us force $g_0$ and $h_0$ to be monic. Namely, if the leading coefficient of $g_0$ is $ax^d$, and the leading coefficient of $h_0$ is $bx^e$, then the leading coefficient of $f$ is $abx^{d+e}$, which must be $ab=1\in R$. So by replacing
	\[g=bg_0\qquad\text{and}h=ah_0,\]
	we will have $gh=abg_0h_0=f$ while $g$ and $h$ are now monic.

	The point of all this is that $f=gh$ with $g$ and $h$ monic force the coefficients of $g$ and $h$ to be integral by the above proposition. But $R$ is normal (!), so $g,h\in R[x]$, so with $f$ irreducible in $R[x]$, we have one of $g$ or $h$ a unit in $R[x]$ and hence in $K(R)[x]$. So $f$ is irreducible and hence prime in $K(R)[x]$, where we are using \autoref{rem:ufdimpliesirredisprime} on $K(R)[x]$.

	So to finish, we create an embedding
	\[\frac{R[x]}{(f(x))}\into\frac{K[x]}{(f(x))}\]
	by lifting $R\into K(R)$ to $R[x]\to K(R)[x]/(f(x))$ and computing the kernel as $R[x]\cap f(x)K(R)[x]$, which we claim equals $f(x)R[x]$. If the kernel is in fact $f(x)R[x]$, then the above is an embedding, so we see that $R[x]/(f(x))$ embeds into an integral domain and hence is an integral domain.
	
	So we have left to show $R[x]\cap f(x)K(R)[x]=f(x)R[x]$. This will hold for arbitrary domains $R$. Indeed, if we have some $f(x)q_0(x)=g(x)\in R[x]\cap f(x)K(R)[x]$, then clearing deminators of $q(x)$ of lets us assume that
	\[f(x)q(x)=c\cdot g(x)\]
	for some $q\in R[x]$. We claim that $c\mid q(x)$, from which $q_0(x)\in R[x]$ will follow. Indeed, if $c\nmid q(x)$, then we expand
	\[f(x)=\sum_{k=0}^Ka_kx^k\qquad\text{and}\qquad q(x)=\sum_{\ell=0}^Lb_\ell x^\ell\]
	where $a_K=1$ and $b_L\ne0$, and we can write
	\[f(x)q(x)=\sum_{n=0}^\infty\left(\sum_{k+\ell=n}a_kb_\ell\right)x^n\in(c).\]
	We know show that $b_\ell\in(c)$ for each $\ell$ by inducting downwards. For example, the $n=K+L$ term above will have only the nonzero term $a_Kb_L=b_L\in(c)$. More generally speaking, if all terms above $b_\ell$ are in $(c)$, then we can look at the $n=K+\ell$ term
	\[a_Kb_\ell+\sum_{i=0}^{K-1}a_i\underbrace{b_{K+\ell-i}}_{\in(c)}\in(c)\]
	so that we see $b_\ell=a_Kb_\ell\in(c)$. So we are done.
\end{proof}
\begin{remark}
	This generalizes the result that, if $R$ is a unique factorization domain, then $R[x]$ is also a unique factorization domain.
\end{remark}

\subsection{Lifting Primes}
Speaking generally for a moment, suppose we have an $R$-algebra $S$. Then, if $\varphi:R\to S$ is our promised map, we note that we have a map $\op{Spec}S\to\op{Spec}R$ by $\varphi^{-1}:\mf q\mapsto\varphi^{-1}(\mf q)$. In particular, thinking of $\varphi$ as providing an ``embedding'' $R\subseteq S$, we get that primes $\mf q$ of $S$ go to
\[\varphi^{-1}(\mf q)=\{r\in R:\varphi(r)\in\mf q\}=:\mf q\cap R,\]
where we are setting this equal to $\mf q\cap R$ by abuse of notation.

Mostly for psychological reasons, we will make this abuse of notation no longer abuse.
\begin{convention}
	For the rest of this section, we will take our ring extensions to actually be embeddings and will notate this by $R\subseteq S$.
\end{convention}
\begin{remark}[Nir] \label{rem:localizebasering}
	Here is one reason to not be so nervous about this: $R\subseteq S$ being an injection behaves well with the universal property of localization. Explicitly, suppose that we have an injective map $\varphi:R\into S$ of domains where all elements of some multiplicative set $U\subseteq R$ go to units $\varphi(U)\subseteq S^\times$. Then we claim the induced map
	\[\overline\varphi:R\left[U^{-1}\right]\to S\]
	is also injective. Indeed, $\overline\varphi(r/u)=0$ implies $\varphi(r)/\varphi(u)=0$, so $\varphi(r)=0$, so $r=0$, where the last step is because $\varphi$ is injective.
\end{remark}
So we will actually get to write $\mf q\cap R$ without feeling guilty.

Now, when $R\subseteq S$ is an integral extension, we get some control of the map $\varphi^{-1}$.
\begin{definition}[Lying over]
	Fix $R\subseteq S$ an integral extension. Given a prime $\mf p\in\op{Spec}R$, we say that a prime $\mf q\in\op{Spec}S$ \textit{lies over $\mf p$} if and only if $\mf q\cap R=\mf p$.
\end{definition}
\begin{proposition} \label{prop:liftprimes}
	Fix $R\subseteq S$ an integral extension by $\varphi:R\into S$. Then the map $\varphi^{-1}:\op{Spec}S\to\op{Spec}R$ is surjective. In other words, for any $\mf p\in\op{Spec}R$, there exists a prime $\mf q\in\op{Spec}S$ lying over $\mf p$ so that $\mf q\cap R=\mf p$.
\end{proposition}
\begin{proof}
	If $R=0$, then $S=0$ follows (because $0_S=0_R=1_R=1_S$), so $\op{Spec}S=\op{Spec}R=\emp$, so the statement holds vacuously.
	
	Otherwise, we may assume $R\ne0$. Set $U:=R\setminus\mf p$, and we will localize at $U$ to get a local ring and then will use Nakayama's lemma to finish. We take this proof in steps, taking the following reductions.
	% Because localization is flat and hence preserves embeddings (\autoref{cor:localflat}), we get an embedding $R_\mf p=R\left[U^{-1}\right]\subseteq S\left[U^{-1}\right]$.
	\begin{itemize}
		\item We quickly say that it will be enough to find a prime of $S\left[U^{-1}\right]$ over $\mf pR\left[U^{-1}\right]$. Indeed, by \autoref{thm:localizedprimes}, we see that
		\[\op{Spec}S\left[U^{-1}\right]=\left\{\mf qS\left[U^{-1}\right]:\mf q\in\op{Spec}S\text{ and }\mf q\cap U=\emp\right\},\]
		so a prime $\mf qS\left[U^{-1}\right]$ lying over $\mf pR\left[U^{-1}\right]$ will automatically have $\mf q\cap U=\emp$ and therefore $\mf q\cap R\subseteq\mf p$. But in fact, lying over tells us stronger: we know
		\[\mf qS\left[U^{-1}\right]\cap R\left[U^{-1}\right]=\mf pR\left[U^{-1}\right],\]
		and so, for any $x\in\mf p$, we have $\frac x1\in\mf pR\left[U^{-1}\right]$, so $\frac{x}1\in\mf qS\left[U^{-1}\right]$, so we may write
		\[\frac{x}1=\frac y{u}\]
		for some $y\in\mf q$ and $u\in U$. This implies $vy=vxu$ for some $v\in U$, so $vux\in\mf q$, but $vu\notin\mf q$ (because $\mf q\cap U=\emp$), so $x\in\mf q$. So indeed, $\mf p\supseteq\mf q\cap R$ follows, and we get $\mf q\cap R=\mf p$.
	
		\item So we have reduced to the case of finding a prime of $S\left[U^{-1}\right]$ lying over the maximal ideal $\mf pR\left[U^{-1}\right]$ of the local ring $R\left[U^{-1}\right]$.
		
		However, we note that $S\left[U^{-1}\right]$ is still an integral $R\left[U^{-1}\right]$-extension (by \autoref{cor:localizeintegral}), and in fact $R\left[U^{-1}\right]\subseteq S\left[U^{-1}\right]$ still (by \autoref{prop:localexact}), so we might as well rename $S\left[U^{-1}\right]$ to $S$ and $R\left[U^{-1}\right]$ to $R$ and $\mf pR\left[U^{-1}\right]$ to $\mf p$. So now we are showing that $S$ contains a prime lying over the (unique) maximal ideal $\mf p$ of $R$.
		
		Very quickly, we consider the ideal $\mf pS$. Any ideal $\mf q$ containing $\mf pS$ will have pre-image $\mf q\cap R\supseteq\mf p$. In fact, if we force $\mf q$ to be a prime containing $\mf pS$, then we get
		\[\mf q\cap R\supseteq\mf p,\]
		but $\mf q$ is a prime (and hence proper) ideal containing the maximal ideal $\mf p$, so we will get $\mf q\cap R=\mf p$ for free, as needed.
		
		\item So we need a prime of $S$ containing $\mf pS$, for which we could take any maximal ideal containing $\mf pS$ if only we knew that $\mf pS$ is proper. Well, if $1\in\mf pS$, then we can write $1$ as an element of $\mf pS$, which means we can write
		\[1=p_1s_1+\cdots+p_ns_n\]
		for some $p_1,\ldots,p_n\in\mf p$ and $s_1,\ldots,s_n\in S$. Now, each of the elements $s_1,\ldots,s_n$ are integral, so $M=R[s_1,\ldots,s_n]$ is an $R$-subalgebra generated by finitely many integral elements and therefore finitely generated as an $R$-module (by \autoref{lem:betterfinite}).
		
		In fact, we have $\mf pM=M$ (because of the above equation), so we get $M=0$ by \autoref{thm:nakayama}, which forces $R\subseteq M$ to vanish; in particular, $R=0$. But we have already dealt with the case of $R=0$, so we are done.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}[Nir]
	The requirement that $R\into S$ by injective is actually necessary here. For example, $\FF_p$ is a $\ZZ$-algebra by $\pi:\ZZ\onto\FF_p$, but $\pi^{-1}:\op{Spec}\FF_p\to\op{Spec}\ZZ$ is definitely not surjective: the $\op{Spec}\FF_p$ has only one element!
\end{remark}
\begin{remark}[Nir]
	It is somewhat subtle to figure out where we actually used the fact that $R\into S$ is injective. The place we used this is at the end: saying that $R[s_1,\ldots,s_n]=0$ implies that $1_R=0_R$ and so $R=0$ is assuming that the map $R\into R[s_1,\ldots,s_n]$ is nonzero.
	
	It is interesting to track through $R=\ZZ$ and $S=\FF_2$ with $\mf p=(3)$. After localizing, we get $R=\ZZ_{(3)}$ while $S=0$, so indeed $M=R[s_1,\ldots,s_n]$ will still vanish because the $R$-action on $S$ is the zero action, but this no longer implies that $R$ vanishes.
\end{remark}
In fact, we have the following slightly stronger statement.
\begin{corollary}
	Fix $R\subseteq S$ an integral extension. Further, if $I\subseteq R$ is an ideal with $I\subseteq\mf p$ for some $\mf p\in\op{Spec}R$, then we can choose $\mf q\in\op{Spec}S$ with $\mf q\cap R=\mf p$ which contains $IS$.
\end{corollary}
\begin{proof}
	The point here is to mod out by $I$ everywhere. We have the following checks.
	\begin{itemize}
		\item We see $S/IS$ is an integral $R/I$-algebra. (Here, $R/I\into S/IS$ is defined by $[x]_I\mapsto[x]_{IS}$.) Indeed, every element $[s]_{IS}\in S/IS$ will have $s$ the root of some monic polynomial in $R[x]$, which we can then mod out by $I$ to see that $[s]_{IS}$ is the root of a monic polynomial in $(R/I)[x]$.
		\item We see $\mf p+I$ is a prime ideal of $R/I$: if $[a]_I\cdot[b]_I\in\mf p+I$, then $ab=x+i$ where $x\in\mf p$ and $i\in I$. But $I\subseteq\mf p$, so actually $ab\in\mf p$, so $a\in\mf p$ or $b\in\mf p$, so $[a]_I\in\mf p+I$ or $[b]_I\in\mf p+I$.
		\item Thus, the proposition promises some prime ideal $\mf q\in\op{Spec}S/IS$ such that $\mf q\cap(R/I)=\mf p+I$. In particular, we take
		\[\mf q+IS,\]
		which is prime as the pull-back of $\mf q$ under $S\onto S/IS$. Now, $r\in R$ will have $r\in\mf q+IS$ if and only if $[r]_{IS}\in\mf q$ if and only if $[r]_I\in\mf q$ (because of how $R/I\into S/IS$ is defined) if and only if $[r]_I\in\mf p+I$ if and only if $r\in\mf p+I$ if and only if $r\in\mf p$. So $(\mf q+IS)\cap R=\mf p$.
	\end{itemize}
	So we see that $\mf q+IS$ satisfies the needed constraints, so we are done.
\end{proof}

\subsection{Integral Domains}
In the case of domains, we get a little more structure out of our integral extensions by appealing to field extensions. For example, we have the following.
\begin{lemma} \label{lem:intgivesalg}
	Fix $R\subseteq S$ an integral extension of domains. Then $K(S)$ is algebraic over $K(R)$.
\end{lemma}
\begin{proof}
	Even though \autoref{cor:localizeintegral} doesn't technically apply, we may imitate its proof. Fix any element $\frac su\in K(S)$. Because $s\in S$ and $s$ is integral over $R$, we see that $s$ will satisfy some monic polynomial
	\[s^n+a_{n-1}s^{n-1}+a_{n-2}s^{n-2}+\cdots+a_1s+a_0=0\]
	with coefficients in $R$. But, porting this over to $K(S)$ and dividing by $u^n$, we see that
	\[\left(\frac su\right)^n+ua_{n-1}\left(\frac su\right)^{n-1}+a_{n-2}\left(\frac su\right)^{n-2}+\cdots+a_1\left(\frac su\right)+a_0=0,\]
	so indeed, $\frac su$ is algebraic over $K(R)$.
\end{proof}
\begin{remark}[Nir]
	The converse is not true: taking $R=\ZZ$ and $S=K(R)=\QQ$, we note that $S$ is not an integral extension of $R$ (indeed, the integral closure of $R$ in $S$ is $R$ by \autoref{prop:ufdnormal}, but $R\subsetneq S$). But surely the extension $K(S)/K(R)$ is algebraic because $K(S)=K(R)$.
\end{remark}
This gives us the following lack of ``avoidance'' in integral domains.
\begin{prop} \label{prop:nonzerointersect}
	Fix $R\subseteq S$ an integral extension of domains. If $I\ne0$ is a nonzero ideal of $S$, then $I\cap R\ne0$.
\end{prop}
\begin{proof}
	Fix any $b\in I\setminus\{0\}$, and we will focus on this element alone.\footnote{At a high level, we are basically ``replacing'' $I$ with $(b)$, for which the statement must hold anyways.} Using \autoref{lem:intgivesalg}, we may write out the polynomial
	\[\sum_{k=0}^n\frac{a_k}{u_k}\left(\frac b1\right)^k=0,\]
	where $\frac{a_n}{u_n}\ne0$. Note that each denominator $u_\bullet$ is nonzero, so we may safely multiply through by $u_0u_1\cdots u_n$ to get the polynomial
	\[\sum_{k=0}^nr_kb^k=0\]
	for some $r_1,\ldots,r_n\in R$. Technically, removing the deminators tells us that $\frac{\sum_{k=0}^nr_kb^k}1=\frac01$ in $K(S)$, but because $S$ is an integral domain, the above equation follows.

	We will read off the nonzero element of $I\cap R$ from the constant term of this polynomial. We note that, if $r_0=0$, then we would have
	\[b\cdot\sum_{k=0}^{n-1}r_{k+1}b^k=0,\]
	so $b\ne0$ forces $\sum_{k=0}^{n-1}r_{k+1}b^k=0$. Thus, by choosing the degree of our polynomial to be as small as possible, we may assume that $r_0\ne0$, lest we would be able to make the degree smaller. Rearranging, we see that
	\[r_0=-\sum_{k=1}^nr_kb^k=b\left(\sum_{k=1}^nr_kb^{k-1}\right).\]
	In particular, we see that $r_0\in(b)\subseteq I$ while $r_0\ne0$, so $r_0\in I\cap(R\setminus\{0\})$. This is what we wanted.
\end{proof}
\begin{remark}[Nir]
	In fact, the above proof technically only needed the fact that $K(R)/K(S)$ is an algebraic extension, not that $S$ is an integral $R$-algebra.
\end{remark}
To close off, we use our avoidance of ideal structure to create an avoidance of field structure.
\begin{proposition} \label{prop:integralfields}
	Fix $R\subseteq S$ an integral extension of integral domains. Then, $R$ is a field if and only if $S$ is a field.
\end{proposition}
\begin{proof}
	We show the directions independently.
	\begin{itemize}
		\item Suppose that $R$ is a field. Picking up any $x\in S\setminus\{0\}$, we need to show that $x$ is a unit. Well, we note that $(x)\subseteq S$ is a nonzero ideal, so \autoref{prop:nonzerointersect} tells us that $(x)\cap R\ne0$. So find $u\in R$ with $u=sx\ne0$ for some $sx\in(x)$.

		Now, $u\ne0$ implies that $u$ is a unit in the field $R$. So find $v\in R$ with $uv=1$. Thus,
		\[(vs)\cdot x=v\cdot(sx)=vu=1,\]
		so we see that $x$ is indeed a unit.

		\item Suppose that $S$ is a field. To show that $R$ is a field, it suffices to show that $(0)$ is a maximal ideal because then all proper ideals will contain $(0)$ and hence equal $(0)$.
		
		Well, pick up any prime ideal $\mf p$ of $R$. By \autoref{prop:liftprimes}, we are promised some prime ideal $\mf q$ of $S$ such that $\mf q\cap R=\mf p$. However, because $S$ is a field, the only prime ideal of $S$ is $\mf q=(0)$. Thus, all prime ideals $\mf p$ of $R$ are equal to
		\[\mf p=(0)\cap R=(0).\]
		In particular, fixing $\mf m$ as one of $R$'s maximal ideals, we see that $\mf m=(0)$, so $(0)$ is a maximal ideal.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}[Nir] \label{rem:goingdownmax}
	Here is a nice application of \autoref{prop:integralfields}. If $\mf m\subseteq S$ is a maximal ideal, then we claim $\mf m\cap R\subseteq R$ is also a maximal ideal. Indeed, the kernel of $R\into S\onto S/\mf m$ is the ideal $\mf m\cap R$, so we have an embedding
	\[\frac R{\mf m\cap R}\into\frac S{\mf m}.\]
	Note both are integral domains because $S/\mf m$ is a field. In fact, this extension is also integral: any $[s]_\mf m\in S/\mf m$ can use the same monic polynomial as $s\in S$ and then mod out by $\mf m\cap R$. So \autoref{prop:integralfields} kicks in to tell us that $S/\mf m$ is a field requires $R/(\mf m\cap R)$ to be a field, so $\mf m\cap R$ is maximal.
\end{remark}
To close this day's notes, we insert a result from the book that we will need a little later and fits best here in our story.
\begin{lemma}[Incomparability] \label{lem:incomparability}
	Fix $R\subseteq S$ an integral extension of rings. If we have primes $\mf q,\mf q'\in\op{Spec}S$ lying over the prime $\mf p\in\op{Spec}R$, then $\mf q\subseteq\mf q'$ requires $\mf q=\mf q'$. In other words, primes lying over $\mf p\in\op{Spec}R$ are incomparable.
\end{lemma}
\begin{proof}
	The point is to reduce to \autoref{prop:nonzerointersect}. In particular, we let $\varphi:R\into S$ be the promised embedding, and we claim that $\overline\varphi:R/\mf p\to S/\mf q$ is an integral embedding of domains. We have domains because $\mf p$ and $\mf q$ are primes, and $\overline\varphi$ is well-defined and injective because $\varphi^{-1}(\mf p)=\mf q$ by definition of lying over.

	It remains to show that $\overline\varphi$ gives us an integral extension of rings. Well, for any $s+\mf q\in S/\mf q$, we note that $s$ is integral over $R$, so we have a monic polynomial
	\[s^n+\sum_{k=0}^{n-1}\varphi(a_k)s^k=0\]
	such that $a_0,\ldots,a_{n-1}\in R$. Taking$\pmod{\mf q}$, we see
	\[(s+\mf q)^n+\sum_{k=0}^{n-1}\varphi(a_k)(s+\mf q)^k=0,\]
	but $\varphi(a_k)\cdot(x+\mf q)=\overline\varphi(a_k+\mf p)\cdot(x+\mf q)$ by construction of $\overline\varphi$. So indeed, the above equation provides a monic polynomial for $s+\mf q$ over $R/\mf p$.

	Thus, we have an integral extension of rings $R/\mf p\subseteq S/\mf q$. Notably, $\mf q'+\mf q\subseteq S/\mf q$ will have
	\begin{align*}
		\overline\varphi^{-1}(\mf q'+\mf q) &= \{a+\mf p:\overline\varphi(a+\mf p)\in\mf q'+\mf q\} \\
		&= \{a+\mf p:\varphi(a)\in\mf q'+\mf q\} \\
		&= \{a+\mf p:\varphi(a)\in\mf q'\} \\
		&= \left\{a+\mf p:a\in\varphi^{-1}(\mf q')\right\} \\
		&= \left\{a+\mf p:a\in\mf p\right\} \\
		&= \{0+\mf p\}.
	\end{align*}
	Thus, by \autoref{prop:nonzerointersect}, we see that $\mf q'+\mf q\subseteq S/\mf q$ must be the zero ideal, so $\mf q'+\mf q=\mf q$, so $\mf q'\subseteq\mf q$, so $\mf q'=\mf q$. This finishes.
\end{proof}