% !TEX root = ../notes.tex

Here we go.

\subsection{Unique Factorization Domains}

We start with the following result; it is due to Gauss.
\begin{theorem} \label{thm:rxisufd}
	Fix $R$ a unique factorization domain. Then $R[x]$ is a unique factorization domain.
\end{theorem}
\begin{proof}
	The main character in our story is as follows.
	\begin{definition}[Content]
		Fix $R$ a ring and $f(x)=a_0x^0+\cdots+a_nx^n\in R[x]$. Then we define the \textit{content} of $f$ to be the ideal
		\[\op{cont}(f):=(a_0,\ldots,a_n)\subseteq R.\]
	\end{definition}
	\begin{remark}[Nir]
		This definition always looked unnatural until I realized that it does in fact preserve some structure of $R[x]$. For example, for $r\in R$ and $f(x)=a_0x^0+\cdots+a_nx^n\in R[x]$, we see $(rf)(x)=ra_0x^0+\cdots+ra_nx^n$ so that
		\[\op{cont}(rf)=(ra_0,\ldots,ra_n)=r(a_0,\ldots,a_n)=r\op{cont}(f).\]
		Additionally, we can show $\op{cont}(f(x+r))=\op{cont}(f(x))$. By symmetry, it suffices for $\op{cont}(f(x+r))\subseteq\op{cont}(f(x))$, for which we note
		\[f(x+r)=\sum_{k=0}^na_k(x+r)^k=\sum_{k=0}^n\left(\sum_{\ell=0}^ka_k\binom k\ell x^\ell r^k\right)=\sum_{\ell=0}^n\left(\sum_{k=\ell}^n\binom k\ell r^ka_k\right)x^\ell\]
		has all coefficients in $\op{cont}(f)$.
	\end{remark}
	Here is the main claim.
	\begin{lemma}[Gauss] \label{lem:gauss}
		Fix $R$ a ring and $f,g\in R[x]$. Then $\op{cont}(fg)\subseteq\op{cont}(f)\op{cont}(g)\subseteq\rad\op{cont}(fg)$.
	\end{lemma}
	\begin{proof}
		We show the inclusions independently.
		\begin{itemize}
			\item That $\op{cont}(fg)\subseteq\op{cont}(f)\op{cont}(g)$ is easier. We write out
			\[f(x)=\sum_{k=0}^\infty a_kx^k\qquad\text{and}\qquad g(x)=\sum_{\ell=0}^\infty b_\ell x^\ell,\]
			where all but finitely many of the $a_k$ and $b_\ell$ vanish. Then
			\[(fg)(x)=\sum_{n=0}^\infty\left(\sum_{k+\ell=n}a_kb_\ell\right)x^n.\]
			Now, by definition, each $a_k$ and $b_\ell$ will have $a_k\in\op{cont}(f)$ and $b_\ell\in\op{cont}(g)$ so that $a_kb_\ell\in\op{cont}(f)\op{cont}(g)$. In particular all coefficients of $fg$ live in $\op{cont}(f)$ and $\op{cont}(g)$, so $\op{cont}(fg)\subseteq\op{cont}(f)\op{cont}(g)$.

			\item The other inclusion $\op{cont}(f)\op{cont}(g)\subseteq\rad\op{cont}(fg)$ is harder. Note that, by \autoref{prop:radprimes},
			\[\rad\op{cont}(fg)=\bigcap_{\op{cont}(fg)\subseteq\mf p}\mf p.\]
			Thus, to show that $\op{cont}(f)\op{cont}(g)\subseteq\rad\op{cont}(fg)$, we will show that $\op{cont}(f)\op{cont}(g)\subseteq\mf p$ for each prime $\mf p$ containing $\op{cont}(fg)$.

			The key trick is to work in $R/\mf p$. Let $\overline p$ denote the image of some $p\in R[x]$ along $R[x]\to(R/\mf p)[x]$. (Importantly, yhe map $R[x]\to(R/\mf p)[x]$ merely mods coefficients.) Because $\op{cont}(fg)\subseteq\mf p$, all the coefficients of $fg$ live in $\mf p$, so
			\[\overline f\cdot\overline g=\overline{fg}=0\]
			in $(R/\mf p)[x]$. But now we see that $R/\mf p$ is an integral domain, so $(R/\mf p)[x]$ is also an integral domain!
			
			So without loss of generality, we take $\overline f=0$ in $(R/\mf p)[x]$, so all of the coefficients of $f$ live in $\mf p$, so $\op{cont}(f)\subseteq\mf p$, so $\op{cont}(f)\op{cont}(g)\subseteq\mf p$. This finishes.
			\qedhere
		\end{itemize}
	\end{proof}
	\begin{remark}[Nir] \label{rem:optimizegauss}
		The above proof actually gave us something which looks a little stronger: for each $\mf p$ containing $\op{cont}(fg)$, we have $\op{cont}(f)\subseteq\mf p$ or $\op{cont}(g)\subseteq\mf p$.
	\end{remark}
	\begin{remark}[Nir] \label{rem:primeslift}
		Here is one way to view Gauss's lemma: if $\mf p$ is a prime ideal in $R$, then $\mf pR[x]$ remains prime in $R[x]$. Namely, if $fg\in\mf pR[x]$, then $\op{cont}(fg)\subseteq\mf p$, so \autoref{rem:optimizegauss} forces $\op{cont}(f)\subseteq\mf p$ or $\op{cont}(g)\subseteq\mf p$. In other words, $f\in\mf pR[x]$ or $g\in\mf pR[x]$.
	\end{remark}
	\begin{remark}[Nir] \label{rem:unitslift}
		Additionally, when $R$ is an integral domain the units of $R[x]$ are precisely the units in $R[x]$. Certainly any unit in $R$ will remain a unit in $R[x]$ because the inverse lives in $R\subseteq R[x]$. However, if $u\in R[x]$ is a unit with inverse $v$, then the equation $uv=1$ forces $\deg u=\deg v=0$, so $u,v\in R$, so $u\in R^\times$.
	\end{remark}
	Now, the key to getting unique factorization in $R[x]$ is to get it via unique factorization in $K[x]$, where $K:=\op{Frac}(R)$ is the field of fractions. In particular, recall $K[x]$ is a unique factorization domain because it is a Euclidean domain (see \autoref{ex:fieldpolyufd}).

	Our next step is to create a weak classifiaction of irreducibles in $R[x]$.
	\begin{lemma} \label{lem:irredfracfield}
		Fix $R$ a unique factorization domain. Then if $f$ is a nonconstant irreducible in $R[x]$, then $f$ is irreducible in $K[x]$.
	\end{lemma}
	\begin{proof}
		Fix some nonconstant $f(x)$. We proceed by contraposition; taking $f$ not irreducible in $K[x]$ and showing that it is not irreducible in $R[x]$. Well, in this case we can write $f=g_0h_0$ for some $g_0,h_0\in K[x]$, where $0<\deg g_0,\deg h_0<\deg f$. (Note this factorization exists because $f$ remains not a unit in $K[x]$ because the units in $K[x]$ are constants by \autoref{rem:unitslift}.)
		
		Now we move back to $R[x]$. Callously, let $a\in R$ (respectively, $b\in R$) be the product of all the denominators of all the coefficients of $g_0$ (respecitvely, $h_0$) so that $g:=ag_0$ and $h:=bh_0$ live in $R[x]$. Then we set $r:=ab$, which gives
		\[rf=gh\]
		where $g,h\in R[x]$. Now that we're in $R[x]$, we can talk about the content. To set up our discussion, we use the fact that $R$ is a unique factorization domain to write
		\[r=u\prod_{k=1}^n\pi_k\]
		for some $u\in R^\times$ and some (not necessarily distinct) irreducibles $\pi_k$.

		Note that if $n=0$, then $r=u$ is a unit, so we get the factorization $f=(g/u)h$ in $R[x]$, which witnesses $f$ not being irreducible. (In particular, $g/u$ and $h$ are not units by \autoref{rem:unitslift}.) So we claim that we can find some triple $(r,g,h)$ consisting of elements $r\in R$ and $g,h\in R[x]$ where $rf=gh$ and $n=0$. Because we already have such an example, we choose $r$ to have minimal $n$.

		To finish, suppose for the sake of contradiction\footnote{It is possible to remove the contradiction by doing induction on $n$, showing that ``for any $n$, there exists a triple $(r,g,h)$ where $r$ is a unit.''} $n>1$ so that $r$ is divisible by the irreducible element $\pi_n$. Because $R$ is a unique factorization domain, \autoref{rem:ufdimpliesirredisprime} tells us $(\pi_n)$ is prime. So by \autoref{rem:optimizegauss}, we see $gh\in(\pi_1)R[x]$ implies $\op{cont}(gh)\in(\pi_1)$ implies
		\[\op{cont}(g)\subseteq(\pi_n)\qquad\text{or}\qquad\op{cont}(h)\subseteq(\pi_n).\]
		So without loss of generality, we take $\op{cont}(g)\subseteq(\pi_1)$, so all coefficients of $f$ are divisible by $\pi_n$, so $g/\pi_n\in R[x]$, so we can write
		\[(r/\pi_n)f=(g/\pi_n)h,\]
		so we have a triple $(r/\pi_n,g/\pi_n,h)$ where $r/\pi_n$ has strictly fewer irreducibless than $r$. This contradicts the minimality of $r$, finishing.
	\end{proof}
	% To finish checking that $R[x]$ is a unique factorization domain, we recall that it suffices to show all irreducibles are prime.
	We can extend our classification to show that all irreducibles are prime in $R[x]$.
	\begin{remark}[Nir] \label{rem:irredslift}
		Taking $R$ to be an integral domain, we note $\pi\in R$ an irreducible in $R$ will remain irreducible in $R[x]$. Indeed, degrees add in integral domains, so if $f,g\in R[x]$ have $fg=\pi$, then $\deg f=\deg g=0$, so $f,g\in R$. In particular, $\pi=fg$ now forces one of $f$ or $g$ to be a unit in $R$ and hence a unit in $R[x]$.
	\end{remark}
	\begin{lemma} \label{lem:classifyirreds}
		Fix $R$ a unique factorization domain. If $f$ is irreducible in $R[x]$, then either
		\begin{itemize}
			\item $f$ is a constant irreducible in $R$, or
			\item $f$ is a (nonconstant) irreducible in $K[x]$, and $\op{cont}(f)\not\subseteq(\pi)$ for any irreducible $\pi\in R$.
		\end{itemize}
		In either case, $f$ is prime in $R[x]$.
	\end{lemma}
	\begin{proof}
		Observe that, if $f$ is a constant, then $f$ will be irreducible in $R$ automatically: writing $f=ab$ for any $a,b\in R$ forces one of $a$ or $b$ to be a unit in $R[x]$ and hence in $R$ (see \autoref{rem:unitslift}). So because $R$ is a unique factorization domain, \autoref{rem:ufdimpliesirredisprime} gives $(f)$ is prime in $R$, so \autoref{rem:primeslift} gives $(f)$ is prime in $R[x]$ as well.

		Otherwise, $f$ is a nonconstant irreducible in $R[x]$. Thus, it is irreducible and hence prime in $K[x]$ by \autoref{lem:irredfracfield}. In particular, if $f\mid gh$ in $R[x]$ for $g,h\in R[x]$, then $f\mid gh$ in $K[x]$ as well (namely, the quotient lives in $R[x]\subseteq K[x]$), but because $f$ is prime in $K[x]$, we see $f\mid g$ or $f\mid h$ in $K[x]$.

		Without loss of generality take $f\mid g$; setting $fq_0=g$, we can let $r$ be the product of the denominators of $q_0$ (note $r\ne0$) so that $q:=rq_0\in R[x]$ and gives
		\[fq=rg.\]
		We will now argue akin to \autoref{lem:irredfracfield} to show that $f\mid g$. In particular, we have found some pair $(r,q)\in(R\setminus\{0\})\times R[x]$ such that $fq=rg$, we can find an $r$ with minimal number of irreducible factors in its factorization into irreducibles.

		Note that if $r$ is divisible by no irreducibles, then this will imply that $r$'s factorization into irreducibles merely consists of a unit, so $r\in R^\times$. In particular,
		\[f(q/r)=g,\]
		where $q/r\in R[x]$, implying $f\mid g$ in $R[x]$. This finishes the primality check for $f$.

		Otherwise, suppose for the sake of contadiction $\pi\mid r$ for some irreducible in $\pi\in R$. Then $(\pi)$ is a prime ideal by \autoref{rem:ufdimpliesirredisprime}, so \autoref{rem:optimizegauss} tells us that $\op{cont}(fq)\subseteq(\pi)$ implies
		\[\op{cont}(f)\subseteq(\pi)\qquad\text{or}\qquad\op{cont}(q)\subseteq(\pi).\]
		We take the cases separately.
		\begin{itemize}
			\item In the case where $\op{cont}(q)\subseteq(\pi)$, we see $q/\pi\in R[x]$, so we could write
			\[f(q/\pi)=(r/\pi)g\]
			to create an $(r,q)$ pair with strictly fewer irreducibles in $r$, thus violating the minimality of $r$.
			\item In the case where $\op{cont}(f)\subseteq(\pi)$, we see $f/\pi\in R[x]$, so $f=\pi\cdot f/\pi$ provides a factorization of $r$ into non-units: the only units of $R[x]$ are $R^\times$ by \autoref{rem:unitslift}, but $\pi\in R\setminus R^\times$ and $f\notin R$. So this contradicts the irreducibility of $f$.
		\end{itemize}
		We remark that the argument at the end of the second case actually shows that $\op{cont}(f)\not\subseteq(\pi)$ for any irreducible $\pi\in R$.
	\end{proof}
	\begin{remark}[Nir] \label{rem:classifyirreds}
		In fact, \autoref{lem:classifyirreds} is sharp: if $f\in R[x]$ is an irreducible in $R$, then $f$ remains irreducible in $R[x]$ by \autoref{rem:irredslift}.
		
		Otherwise if $f\in R[x]$ is an irreducible in $K[x]$ with $\op{cont}(f)\not\subseteq(\pi)$ for each irreducible $\pi\in R$, then if we factor $f=gh$ where $g,h\in R[x]$, irreducibility in $K[x]$ forces $\deg g=0$ or $\deg h=0$, so without loss of generality $\deg g=0$. But no irreducible $\pi$ may divide $g$ because then it would divide $f$, giving $\op{cont}(f)\subseteq(\pi)$.
	\end{remark}
	The above lemma finishes the proof by \autoref{rem:betterufd}: $R[x]$ is Noetherian by \autoref{thm:hilbasis} and so satisfies the ascending chain condition on principal ideals, and all irreducibles are prime in $R[x]$ by the above lemma.
\end{proof}
\begin{remark}[Nir]
	The above working out was extraordinarily annoying.
\end{remark}
\begin{corollary}
	The ring $k[x_1,\ldots,x_n]$ is a unique factorization domain.
\end{corollary}
\begin{proof}
	We induct on $n$: when $n=0$, we note that fields vacuously have unique factorization. The inductive step is to show that $k[x_1,\ldots,x_{n-1}][x_n]$ has unique factorization from $k[x_1,\ldots,x_{n-1}]$, which is precisely \autoref{thm:rxisufd}.
\end{proof}
\begin{example}
	We show that $\left(y^2-x^3\right)\subseteq k[x,y]$ is prime. Because $k[x,y]$ is a unique factorization domain, it suffices to show that $y^2-x^3$ is irreducible in $k[x,y]=k[x][y]$, for which it suffices to show that $y^2-x^3$ is irreducible in $k(x)[y]$ by \autoref{rem:classifyirreds}.
	
	But $y^2-x^3$ is a quadratic in $k(x)[y]$ and therefore irreducible because it has no roots: there is no $y=f(x)/g(x)$ such that $f(x)^2/g(x)^2=x^3$ because this gives
	\[f(x)^2=x^3g(x)^2,\]
	which fails by degree arguments. Namely, $f,g\ne0$, and $\deg\left(f(x)^2\right)$ is even while $\deg\left(x^3g(x)^2\right)$ is odd.
\end{example}
\begin{example}
	We show that $\left(y^2-x^3\right)\subseteq k[x,y]$ is prime a different way. Indeed, by sending $x\mapsto t^2$ and $y\mapsto t^3$, there is an embedding
	\[\frac{k[x,y]}{\left(y^2-x^3\right)}\into k\left[t^2,t^3\right]\]
	by a homework problem. So the quotient is a domain, so $\left(y^2-x^3\right)$ is prime.
\end{example}

\subsection{The Cayley--Hamilton Theorem}
Here is the main result we are going to prove.
\begin{theorem} \label{thm:ch}
	Fix $R$ a ring and $A\in R^{n\times n}$ a matrix. Further, define $p_A(x):=\det(xI-A)\in R[x]$. Then $p_A(A)=0^{n\times n}\in R^{n\times n}$, where $p_A(A)$ is evaluated by the ring homomorphism $R[x]\to R^{n\times n}$ by $r\mapsto rI$ and $x\mapsto A$.
\end{theorem}
Note in particular that the ring homomorphism $R[x]\to R^{n\times n}$ is legal because it is actually outputting in the $R$-subalgebra of $R^{n\times n}$ generated by $A$, which is a commutative ring because it is essentially a polynomial ring with coefficients $rI$. We will make the statement more precise in the proof.
\begin{remark}
	\autoref{thm:ch} is usually stated in linear algebra for matrices over a field, but it is a purely algebraic result, so there is no reason to believe it shouldn't hold for arbitrary rings.
\end{remark}
\begin{proof}[Proof of \autoref{thm:ch}]
	We need to pick up the following definition for a technical trick at the end.
	\begin{definition}[Cofactor matrix]
		Fix $A\in R^{n\times n}$. Then we define the \textit{cofactor matrix} by
		\[C_{ij}:=(-1)^{i+j}\det A_{i,j},\]
		where $A_{i,j}$ is the matrix $A$ where the $i$th row and $j$th column have been removed.
	\end{definition}
	\begin{example}
		Set
		\[A=\begin{bmatrix}
			a_{11} & a_{12} \\
			a_{21} & a_{22}
		\end{bmatrix}.\]
		Then
		\[C=\begin{bmatrix}
			a_{22} & -a_{21} \\
			-a_{12} & a_{11}
		\end{bmatrix}.\]
		Then we can compute
		\[C^\intercal A=\begin{bmatrix}
			a_{22} & -a_{12} \\
			-a_{21} & a_{11}
		\end{bmatrix}\begin{bmatrix}
			a_{11} & a_{12} \\
			a_{21} & a_{22}
		\end{bmatrix}=\begin{bmatrix}
			a_{11}a_{22}-a_{21}a_{12} & a_{12}a_{22}-a_{22}a_{12} \\
			-a_{11}a_{21}+a_{21}a_{11} & -a_{12}a_{21}+a_{22}a_{11}
		\end{bmatrix}(\det A)I.\]
	\end{example}
	The key fact of the cofactor matrix is that it ``almost inverts'' $A$, as in the above example.
	\begin{lemma} \label{lem:adjugate}
		Fix $A\in R^{n\times n}$ with cofactor matrix $C$. Then $C^\intercal A=(\det A)I$.
	\end{lemma}
	\begin{proof}
		This is essentially Cram\'er's rule. Give $A$ coefficients by $A=(a_{ij})_{i,j=1}^n$, and fix some indices $i$ and $k$. We can compute that
		\[\left(C^\intercal A\right)_{ik}=\sum_{j=1}^n\left(C^\intercal\right)_{ij}a_{jk}=\sum_{j=1}^n(C)_{ji}a_{jk}=\sum_{j=1}^n(-1)^{i+j}a_{jk}\det A_{ji},\]
		which upon expanding $A_{ji}$ looks like
		\[\left(C^\intercal A\right)_{ik}=\sum_{j=1}^n(-1)^{i+j}a_{jk}\det\begin{bmatrix}
			a_{11} & \cdots & a_{1,i-1} & a_{1,i+1} & \cdots & a_{1n} \\
			\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
			a_{j-1,1} & \cdots & a_{j-1,i-1} & a_{j-1,i+1} & \cdots & a_{j-1,n} \\
			a_{j+1,1} & \cdots & a_{j+1,i-1} & a_{j+1,i+1} & \cdots & a_{j+1,n} \\
			\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
			a_{n1} & \cdots & a_{n,i-1} & a_{n,i+1} & \cdots & a_{nn} \\
		\end{bmatrix}\tag{$*$}\label{eq:almostcramer}\]
		To compute this sum, we consider the matrix
		\[A':=\begin{bmatrix}
			a_{11} & \cdots & a_{1,i-1} & \color{red}a_{1k} & a_{1,i+1} & \cdots & a_{1n} \\
			\vdots & \ddots & \vdots & \color{red}\vdots & \vdots & \ddots & \vdots \\
			a_{n1} & \cdots & a_{n,i-1} & \color{red}a_{nk} & a_{n,i+1} & \cdots & a_{nn} \\
		\end{bmatrix}.\]
		In particular, $A'$ is equal to the matrix $A$ where the $i$th column has been replaced with the $k$th column. The point is that applying Cram\'er's rule to compute $\det A_j'$ along the red column gives exactly the right-hand side of \autoref{eq:almostcramer}.

		We now have two cases.
		\begin{itemize}
			\item If $i=k$, then the substitution process used to get $A'$ from $A$ doesn't actually do anything (we replace a row with itself), so $\det A'=\det A$. Thus, $\left(C^\intercal A\right)_{i}=\det A$ for each $i$.
			\item If $i\ne k$, then the substitution process used to get $A'$ will force $A'$ to have two distinct columns equal to the $k$th column, which forces $\det A'=0$. To be explicit, $A'$ looks like
			\[A':=\left[\begin{array}{@{}*{11}{c}@{}}
				a_{11} & \cdots & a_{1,i-1} & \color{red}a_{1k} & a_{1,i+1} & \cdots & a_{1,k-1} & \color{red}a_{1k} & a_{1,k+1} & \cdots & a_{1n} \\
				\vdots & \ddots & \vdots & \color{red}\vdots & \vdots & \ddots & \vdots & \color{red}\vdots & \vdots & \ddots & \vdots \\
				a_{n1} & \cdots & a_{n,i-1} & \color{red}a_{nk} & a_{n,i+1} & \cdots & a_{n,k-1} & \color{red}a_{nk} & a_{n,k+1} & \cdots & a_{1n} \\
			\end{array}\right].\]
			(The above representation technically assumes $i<k$, but there is a similar diagram for $k<i$.) Subtracting the $i$th column from the $k$th column gives
			\[\left[\begin{array}{@{}*{11}{c}@{}}
				a_{11} & \cdots & a_{1,i-1} & a_{1k} & a_{1,i+1} & \cdots & a_{1,k-1} & \color{red}0 & a_{1,k+1} & \cdots & a_{1n} \\
				\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \color{red}\vdots & \vdots & \ddots & \vdots \\
				a_{n1} & \cdots & a_{n,i-1} & a_{nk} & a_{n,i+1} & \cdots & a_{n,k-1} & \color{red}0 & a_{n,k+1} & \cdots & a_{1n} \\
			\end{array}\right].\]
			Doing this column subtraction does not change $\det A'$, but now we can expand along the highlighted column to see that $\det A'=0$.
		\end{itemize}
		Synthesizing the above two cases, we see that $(C^\intercal A)_{ik}=(\det A)1_{i=k}=(\det A)I_{ik}$, so $C^\intercal A=(\det A)I$, which is what we wanted.
	\end{proof}
	We now return to the proof.
	\begin{warn}
		The details in the below proof are somewhat technical because they have to do with matrices. I apologize, but I hope that at least the exposition is clear even if wordy.
	\end{warn}
	The main idea is that we would actually like to substitutte $x=A$ into $p_A(x)=\det(xI-A)$, but this does not currently make sense because $xI$ needs to be a scalar-matrix multiplication.
	
	So the key trick is to consider the elements of $R$ as living in $\op{End}_R\left(R^n\right)$, alongside with $A$. For convenience, given $R^n$ the standard basis $e_1,\ldots,e_n$, and give $A$ coefficients by $A=(a_{ij})_{i,j=1}^n$. We have two steps.
	\begin{itemize}
		\item On one hand, we define $\varphi\in\op{End}_R\left(R^n\right)$ to correspond to $A$ by
		\[\varphi(e_j)=Ae_j=\begin{bmatrix}
			a_{11} & \cdots & a_{1n} \\
			\vdots & \ddots & \vdots \\
			a_{n1} & \cdots & a_{nn}
		\end{bmatrix}e_j=\sum_{i=1}^na_{ij}e_i.\]
		Because $R^n$ is freely generated by the $e_\bullet$, these equations uniquely determine the $R$-module homomorphism $\varphi$.
		\item On the other hand, we note that the action of $r\in R$ on $R^n$ defined by $(x_1,\ldots,x_n)\mapsto(rx_1,\ldots,rx_n)$ defines an $R$-module endomorphism which we name $\mu(r)\in\op{End}_R\left(R^n\right)$. In fact, the function $\mu:R\to\op{End}_R\left(R^n\right)$ is a ring homomorphism: for any $r,s\in R$ and $v\in R^n$, we see
		\[\mu(rs)(v)=(rs)(v)=r(sv)=\mu(r)(\mu(s)v)=(\mu(r)\circ\mu(s))(v),\]
		and
		\[\mu(r+s)(v)=(r+s)v=rv+sv=\mu(r)(v)+\mu(s)(v)=(\mu(r)+\mu(s))(v).\]
	\end{itemize}
	Thus, we see that we can define a ring homomorphism $\mu:R[x]\to\op{End}_R\left(R^n\right)$ by lifting the ring homomorphism $\mu:R\to\op{End}_R\left(R^n\right)$ and sending $x\mapsto\varphi$; let this image be $\mu(R)[\varphi]$. For technicality reasons, we quickly note that $\mu(R)[\varphi]$ is a commutative ring\footnote{We need to say this in order to take determinants in $\mu(R)[\varphi]$ later because determinants only make sense in commutative rings.} because, for any $\mu(f),\mu(g)\in\mu(R)[\varphi]$, we have
	\[\mu(f)\mu(g)=\mu(fg)=\mu(gf)=\mu(g)\mu(f).\]
	So we have indeed pushed both $A$ and elements of $R$ onto equal footing in $\mu(R)[\varphi]$.

	We now attack the proof more directly. We are interested in showing that $p_A(A)=0^{n\times n}\in R^{n\times n}$. To use the machinery we've developed, we should move this statement into $\mu(R)[\varphi]$. After fully expanding out the determinant $p_A(x)=\det(xI-A)\in R[x]$, we can write out the coefficients
	\[p_A(x)=\sum_{k=0}^np_kx^k.\]
	Plugging in $x=A$, we are interested in showing that
	\[\sum_{k=0}^np_kA^k\stackrel?=0^{n\times n}.\]
	This is equivalent to showing that
	\[\sum_{k=0}^np_k\left(A^ke_j\right)=\left(\sum_{k=0}^np_kA^k\right)e_j\stackrel?=0\in R^n\]
	for each basis vector $e_j$. By definition, we see that $\varphi(e_j)=Ae_j$, so inductively, $A^ke_j=\varphi e_j$. With this in mind, we push in $\mu(R)[\varphi]$ by writing
	\[\sum_{k=0}^np_k\left(A^ke_j\right)=\sum_{k=0}^np_k\varphi^k(e_j)=\left(\sum_{k=0}^np_k\varphi^k\right)e_j=\left(\sum_{k=0}^n\mu(p_k)\mu(x)\right)e_j=\mu\left(\sum_{k=0}^np_kx^k\right)e_j=\mu(p_A(x))e_j.\]
	Showing that $\mu(p_A(x))e_j=0\in R^n$ for each $e_j$ is equivalent to showing that $\mu(p_A(x))=0\in\op{End}_R(R^n)$. We note that this is pretty close to literally plugging in $A$ into $p_A$, but instead we have to plug in $\varphi$.

	Indeed, we can undo all of the determinant expansion for $p_A$ to push the $\mu$ inside. Namely, working in $\mu(R)[\varphi]$, the determinant is just a very large polynomial in its coordinates, and polynomials commute with ring homomorphisms, so we can write
	\begin{align*}
		\mu(p_A(x)) &= \mu\left(\det\begin{bmatrix}
			x-a_{11} & -a_{12} & \cdots & -a_{1n} \\
			-a_{21} & x-a_{22} & \cdots & -a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			-a_{n1} & -a_{n2} & \cdots & x-a_{nn}
		\end{bmatrix}\right) \\
		&=\det\begin{bmatrix}
			\mu(x-a_{11}) & \mu(-a_{12}) & \cdots & \mu(-a_{1n}) \\
			\mu(-a_{21}) & \mu(x-a_{22}) & \cdots & \mu(-a_{2n}) \\
			\vdots & \vdots & \ddots & \vdots \\
			\mu(-a_{n1}) & \mu(-a_{n2}) & \cdots & \mu(x-a_{nn})
		\end{bmatrix} \\
		&=\det\underbrace{\begin{bmatrix}
			\varphi-\mu(a_{11}) & -\mu(a_{12}) & \cdots & -\mu(a_{1n}) \\
			-\mu(a_{21}) & \varphi-\mu(a_{22}) & \cdots & -\mu(a_{2n}) \\
			\vdots & \vdots & \ddots & \vdots \\
			-\mu(a_{n1}) & -\mu(a_{n2}) & \cdots & \varphi-\mu(a_{nn})
		\end{bmatrix}}_{\varphi\mu(I)-\mu(A)}.
	\end{align*}
	Here, $\varphi\mu(I)-\mu(A)$ is abuse of notation, but it will do. The point is that, indeed, we have basically just plugged in $x=A$ into the determinant.
	\begin{warn}
		The matrix $\varphi\mu(I)-\mu(A)$ is a matrix whose entires are endomorphisms, not elements of $R$. Explicitly, $\varphi\mu(I)-\mu(A)\in\op{End}_R\left(R^n\right)^{n\times n}$.
	\end{warn}
	To show that $\det(\varphi\mu(I)-\mu(A))=0$, we note that $\varphi\mu(I)-\mu(A)$ will vanish on the vector $(\pi_1,\ldots,\pi_n)\in\op{Mor}_{\mathrm{Set}}(R^n,R^n)^n$, where $\pi_k$ is the function which outputs $e_k$.\footnote{Careful readers may object that our vector $(\pi_1,\ldots,\pi_n)$ does not live in $\mu(R)[\varphi]$, and so some linear algebra might not hold. However, we do not have to fear because are still working in an $R$-module, so we just need to make sure we never do anything that would require anything commuting with the $\pi_\bullet$.} (The $\pi_\bullet$ is how we are bringing the basis vectors $e_\bullet$ into our world of functions.) Indeed, the $j$th component of the expansion of
	\[\begin{bmatrix}
		\varphi-\mu(a_{11}) & -\mu(a_{12}) & \cdots & -\mu(a_{1n}) \\
		-\mu(a_{21}) & \varphi-\mu(a_{22}) & \cdots & -\mu(a_{2n}) \\
		\vdots & \vdots & \ddots & \vdots \\
		-\mu(a_{n1}) & -\mu(a_{n2}) & \cdots & \varphi-\mu(a_{nn})
	\end{bmatrix}^\intercal\begin{bmatrix}
		\pi_1 \\
		\pi_2 \\
		\vdots \\
		\pi_n
	\end{bmatrix}\]
	is (note the tranpose!)
	\[\sum_{i=1}^n(\varphi1_{i=j}-\mu(a_{ij}))\pi_i=\varphi\pi_j-\sum_{i=1}^n\mu(a_{ij})\pi_i.\]
	Evaluating this on any $v\in R^n$, we see $\pi_iv=e_i$ so that
	\[\left(\varphi\pi_j-\sum_{i=1}^n\mu(a_{ij})\pi_i\right)v=\varphi(e_j)-\sum_{i=1}^na_{ij}e_i,\]
	which vanishes by the definition of $\varphi$. (We needed to use the transpose in the above argument in order to make the above equation actually vanish by definition of $\varphi$.)

	Thus,
	\[(\varphi\mu(I)-\mu(A))^\intercal\begin{bmatrix}
		\pi_1 \\
		\vdots \\
		\pi_n
	\end{bmatrix}=\begin{bmatrix}
		0 \\
		\vdots \\
		0
	\end{bmatrix}.\]
	Multiplying on the left by the transpose of the cofactor matrix (!) of $(\varphi\mu(I)-\mu(A))^\intercal$, \autoref{lem:adjugate} gives
	\[\det\left((\varphi\mu(I)-\mu(A))^\intercal\right)\begin{bmatrix}
		\pi_1 \\
		\vdots \\
		\pi_n
	\end{bmatrix}=\begin{bmatrix}
		0 \\
		\vdots \\
		0
	\end{bmatrix}.\]
	Thus, $\det\left((\varphi\mu(I)-\mu(A))^\intercal\right)\pi_j=0$ for each $\pi_j$, so $\det\left((\varphi\mu(I)-\mu(A))^\intercal\right)e_j=0$ for each $e_j$ after pushing $e_j$ through. So $\det(\varphi\mu(I)-\mu(A))=\det\left((\varphi\mu(I)-\mu(A))^\intercal\right)=0$ This finishes the (very long) proof.
\end{proof}

\subsection{Applying the Cayley--Hamilton Theorem}
Our use of \autoref{thm:ch} in commutative algebra will be via the following form.
\begin{theorem} \label{thm:betterch}
	Fix $M$ a finitely generated $R$-module with $n$ generators. Further, fix $\varphi\in\op{End}_R(M)$. Then there exists some monic polynomial
	\[p_\varphi(x)=x^n+p_1x^{n-1}+\cdots+p_n\]
	of degree $n$ such that $p_\varphi(\varphi)$ is zero. In fact, if there is an ideal $I\subseteq R$ such that $IM=M$, then we can choose $p_k\in I^k$.
\end{theorem}
\begin{proof}
	Note that we may assume such an ideal $I$ exists because certainly $I=R$ works. Let $\{m_1,\ldots,m_n\}$ generate $M$ so that we can conjure constants $a_{ij}$ by
	\[\varphi(m_j)=\sum_{i=1}^na_{ij}m_i\tag{$*$}\label{eq:makematrix}\]
	to give a matrix form for $\varphi$, by $A:=(a_{ij})_{i,j=1}^n\in R^{n\times n}$; namely, $\varphi(m_j)=Am_j$ for each $m_j$, by definition of matrix-vector multiplication. This lets us apply \autoref{thm:ch} to get some polynomial $p_A(x):=\det(xI^{n\times n}-A)$ such that $p_A(A)=0^{n\times n}$. To be explicit, let
	\[p_A(x)=\sum_{k=0}^nr_kx^k.\]
	Then, for any $m_j$, we see that $\varphi m_j=Am_j$
	\[p_A(\varphi)(m_j)=\sum_{k=0}^nr_k\varphi^km_j=\sum_{k=0}^nr_kA^km_j=p_A(A)m_j=0,\]
	so $p_A(\varphi)$ vanishes on each of the $m_j$ and therefore is the zero morphism.

	We now stare harder at the coefficients of $p_A(x)$ to get the second statement; suppose $I\subseteq R$ with $IM=M$ (certainly some $I$ exists because $I=R$ suffices). As some technical set-up, each $m_k$ generating $M$ has some $x_k\in I$ and $m_k'\in M_k$ such that $m_k=x_km_k'$. So because the $m_\bullet$ generate $M$ over $R$, we see
	\[M=Rm_1+\cdots+Rm_n=Rx_1m_1'+\cdots+Rx_2m_2'\subseteq Im_1'+\cdots+Im_n',\]
	so all elements in $M$ can be written as an $I$-linear combination of the $\{m_1',\ldots,m_n'\}$. In particular, if we run the above argument again, the matrix representation from \eqref{eq:makematrix} can have all elements in $I$, so $A\in I^{n\times n}$. Then the polynomial $p_A$ we generate will be
	\[p_A(x)=\det\begin{bmatrix}
		x-a_{11} & -a_{12} & \cdots & -a_{1n} \\
		-a_{21} & x-a_{22} & \cdots & -a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		-a_{n1} & -a_{n2} & \cdots & x-a_{nn}
	\end{bmatrix}=\sum_{\sigma\in S_n}\left(\prod_{k=1}^n(1_{k=\sigma k}x-a_{k,\sigma k})\right),\]
	where we have expanded out the determinant in the last step by hand. After a full expansion, we see that the leading term will be $1x^n$, coming from the $\sigma=\id$ term only. As for the other coefficients, the coefficient $p_d$ of $x^{n-d}$ only occurs when we choose $n-d$ terms of $x$ from the product, leaving $d$ terms of $-a_{k,\sigma k}\in I$ left over to multiply, so $p_d\in I^d$. This finishes.
\end{proof}
Let's now see an application.
\begin{proposition} \label{prop:epiisiso}
	Let $M$ be a finitely generated $R$-module and $\psi\in\op{End}_R(M)$. Then if $\psi$ is surjective, then $\psi$ is an isomorphism.
\end{proposition}
\begin{proof}
	The key trick is to give $M$ an $R[t]$-module structure to $M$ by defining $R[t]\to\op{End}_R(M)$ by lifting the given ring map $R\to\op{End}_R(M)$ and sending $t\mapsto\psi$. (Note $M$ is a finitely generated $R$-module and hence a finitely generated $R[t]$-module.) In particular, by \autoref{thm:betterch}, we get some $p_{\id}(x)\in R[t][x]$ such that
	\[p_{\id}({\id})=0.\]
	Further, because $\psi$ is surjective, we see that $(t)\cdot M=M$: every $m\in M$ has some $m'\in M$ such that $tm=\psi(m')=m$, so $m\in tM\subseteq(t)M$. Thus, we can use \autoref{thm:betterch} to write out
	\[p_{\id}(x)=x^n+p_1x^{n-1}+\cdots+p_n\in R[t][x],\]
	where $p_d\in\left(t^d\right)$ for each $d$.
	\begin{remark}[Nir]
		It may look like conjuring $p_{\id}$ shouldn't do anything because the characteristic polynomial for $p_{\id}$ should be $(x-1)^n$, where $n$ is the number of generators for $M$. However, this previous sentence where we invoke \autoref{thm:betterch} to force $p_d\in\left(t^d\right)$ is where we are inputting information about $\psi$.
	\end{remark}
	In particular, plugging in $x=\id$, we see that
	\[0={\id}^n+p_1{\id}^{n-1}+\cdots+p_n={\id}+t\cdot\underbrace{\left(\frac{p_1}t{\id}^{n-1}+\cdots+\frac{p_n}t{\id}^0\right)}_{q(t)}\]
	in $\op{End}_R(M)$. In particular, for some $q\in R[t]$. In particular, $t$ is invertible with inverse $q(t)$. Defining $\varphi:=-q(t)$, we see from the above that $\varphi\psi=\psi\varphi=\id$, so $\varphi$ and $\psi$ are inverses.
\end{proof}
\begin{remark}
	\autoref{prop:epiisiso} need not be true even in vector spaces which are not finitely generated. For example, fixing a field $k$, consider
	\[V:=\bigoplus_{i=1}^\infty kv_i\]
	for some vectors $\{v_i\}_{i=1}^\infty$. Then we have the surjective map defined by $v_1\mapsto0$ and $v_i\mapsto v_{i-1}$ for $i>1$, and this is not an isomorphism because it has kernel.
\end{remark}
\begin{remark}
	The analogous version of \autoref{prop:epiisiso} need not be true for injections giving isomorphisms. For example, $\ZZ\to2\ZZ\into\ZZ$ is injective but not an isomorphism.
\end{remark}
Here is a quick corollary to \autoref{prop:epiisiso}.
\begin{corollary}
	Fix $m$ and $n$ positive integers. If we have an isomorphism of $R$-modules, $R^n\cong R^m$, then $m=n$.
\end{corollary}
\begin{proof}
	Without loss of generality take $n\ge m$. Then we use the canonical projection $R^n\onto R^m$ defined by $(x_1,\ldots,x_n)\mapsto(x_1,\ldots,x_m)$ to construct a surjective map
	\[R^m\cong R^n\onto R^m,\]
	which must be an isomorphism by \autoref{prop:epiisiso}. However, if $n>m$, then the map $R^n\onto R^m$ by projection has nontrivial kernel (e.g., $(1_{k>m})_{k=1}^n\in R^n$), so the composite $R^m\cong R^n\onto R^m$ would have nontrivial kernel (e.g., take the pre-image of $(1_{k>m})_{k=1}^n\in R^n$ under $R^m\cong R^n$), which is a contradiction. So we must have $n=m$.
\end{proof}

\subsection{Nakayama's Lemma}
To ready our discussion of Nakayama's lemma, we recall the following definition.
\begin{definition}[Jacobson radical]
	Fix a ring $R$. Then we define the \textit{Jacobson radical} by
	\[\rad R:=\bigcap_{\mf m}\mf m.\]
\end{definition}
Here is the main fact about $\rad R$ that we will need.
\begin{lemma} \label{lem:altjacobsonrad}
	Fix $R$ a ring. Then $r\in\rad R$ if and only if $1-rs\in R^\times$ for each $s\in R$.
\end{lemma}
\begin{proof}
	We take our implications separately.
	\begin{itemize}
		\item In one direction, suppose that $r\in R$ has $r\in\rad R$, and pick up any $s\in R$. Then, for each maximal ideal $\mf m\subseteq$, we see $r\in\mf m$ and so $rs\in\mf m$ while $1\notin\mf m$, so $1-rs\notin\mf m$. Thus, the ideal $(1-rs)$ is not contained in any maximal ideal, so we must have $(1-rs)=R$, so there exists $u\in R$ such that $(1-rs)u=(1)$, so $1-rs\in R^\times$.
		\item In the other direction, suppose that $r\notin\rad R$ so that there exists a maximal ideal $\mf m$ such that $r\notin\mf m$. It follows that $[r]_\mf m\ne[0]_\mf m\in R/\mf m$, so because $R/\mf m$ is a field, there exists $[s]_\mf m\in R/\mf m$ with
		\[[1-rs]_\mf m=[1]_\mf m-[r]_\mf m\cdot[s]_\mf m=[0]_\mf m.\]
		In particular, $1-rs\in\mf m$, so $(1-rs)\subseteq\mf m$, so $1\notin(1-rs)$, so $1-rs$ is not a unit.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}
	We will mostly use the Jacobson radical in the context where $R$ is a local ring so that $\rad R=\mf m$, where $\mf m$ is the unique maximal ideal of $R$.
\end{remark}

Here is our result.
\begin{theorem}[Nakayama's lemma] \label{thm:nakayama}
	Fix $R$ a ring and an ideal $I\subseteq\op{rad}R$. If $M$ is a finitely generated $R$-module such that $IM=M$, then $M=0$.
\end{theorem}
\begin{proof}
	The main idea is in the following lemma.
	\begin{lemma}
		Fix $R$ a ring and $I\subseteq R$ an ideal. If $M$ is a finitely generated $R$-module such that $IM=M$, then there exists $r\in I$ with $(1-r)M=0$.
	\end{lemma}
	\begin{proof}
		The idea is, as usual, to use \autoref{thm:betterch}. Using $\id\in\op{End}_R(M)$, the fact that $IM=M$ (!) gives us a polynomial
		\[p_{\id}(x):=x^n+p_1x^{n-1}+\cdots+p_n\in R[x],\]
		such that $p_{\id}({\id})=0$ and $p_k\in I^k$ for each $p_k$. But plugging in $x=\id$, we see
		\[0=p_{\id}({\id})={\id}^n+p_1{\id}^{n-1}+\cdots+p_n{\id}^0=\big(1-(-p_1-\cdots-p_n)\big){\id}.\]
		In particular, we set $r:=-p_1-\cdots-p_n\in I$ so that $(1-r)m=(1-r)\id m=0$ for each $m\in M$. This finishes.
	\end{proof}
	From this lemma the result directly follows because the promised $1-r$ is a unit. Indeed, $IM=M$ promises $r\in I$ with $(1-r)M=0$, and $r\in I\subseteq\rad R$ implies $1-r\in R^\times$ by \autoref{lem:altjacobsonrad}. So finding $u\in R$ with $u(1-r)=1$, we see that each $m\in M$ has
	\[m=1m=u(1-r)m=u\cdot 0=0,\]
	so $M=0$ is forced.
\end{proof}
\begin{remark}[Nir] \label{rem:fingennakayama}
	The condition that $M$ is finitely generated is necessary: in $k[x]_{(x)}$-modules, we see $\rad k[x]_{(x)}=(x)$ because $k[x]_{(x)}$ is local, but $(x)\cdot k(x)=k(x)$ while $k(x)$ is a nonzero $k[x]_{(x)}$-module. The analogous arithmetic example is with $\ZZ_2$-modules, where $(2)\cdot\QQ_2=\QQ_2$ while $\QQ_2\ne0$.
\end{remark}
Let's see a quick application.
\begin{corollary}
	Fix a ring $R$ and an ideal $I\subseteq\op{rad}R$. Further, suppose that $M$ is a finitely generated $R$-module, and we have elements $m_1,\ldots,m_n\in M$. Then if the images $\overline{m_1},\ldots,\overline{m_n}$ generate $M/IM$, then the original elements generate $M$.
\end{corollary}
\begin{proof}
	Consider the $R$-submodule
	\[M':=Rm_1+\cdots+Rm_n\subseteq M.\]
	We will show that $M=M'$ by showing $M/M'=0$, for which we will use \autoref{thm:nakayama}. Well, it suffices to show that $M/M'=I(M/M')$. To see this, fix any $m\in M$, and we want to find $x\in I$ and $m_0\in M$ such that $[m]_{M'}=x\cdot[m_0]_{M'}$. Well, $\{[m_1]_I,\ldots,[m_n]_I\}$ generates $M/IM$, so there exists $r_1,\ldots,r_n$ such that
	\[[m]_I=r_1[m_1]_{I}+\cdots+r_n[m_n]_I.\]
	In particular, there is $x\in I$ and $m_0\in M$ such that
	\[m-(r_1m_1+\cdots+r_nm_n)=xm_0\in IM,\]
	so $[m]_{M'}=[xm_0]_{M'}=x\cdot[m_0]_{M'}\in I(M/M')$, which is what we wanted.
\end{proof}
\begin{remark}[Nir]
	Again, the initial hypothesis that $M$ is finitely generated is necessary. The same examples as in \autoref{rem:fingennakayama} will work because $IM=M$ means that the empty set will generate $M/IM$ but will not generte $M$ with $M\ne0$.
\end{remark}

\subsection{Support of Tensor Products}
Let's see another application of \autoref{thm:nakayama}: we can finally close the books on \autoref{rem:supptensor}.
\begin{proposition} \label{prop:localtensorintdomain}
	Fix $R$ a local ring with $M$ and $N$ finitely generated $R$-modules. Then $M\otimes_RN=0$ if and only if $M=0$ or $N=0$.
\end{proposition}
\begin{proof}
	In one direction, if $M=0$ or $N=0$, then of course $M\otimes_RN=0$. For example, if $M=0$, then any generator $m\otimes n$ is just $0\otimes n=(0\cdot0)\otimes n=0(0\times n)=0\otimes0$.
	
	The other direction is harder. Suppose that $M\ne0$ with $M\otimes_RN=0$, and we show that $N=0$. Further, because $R$ is local, we are promised a single maximal ideal $\mf m\subseteq R$, and because this is the only maximal ideal, $\rad R=\mf m$. The point is to use the right-exactness of $-\otimes_RN$ (and a healthy amount of Nakayama's lemma), so we begin by encoding $M\ne0$ into a right-exact sequence.
	
	In particular, $M\ne0$ (and $M$ is finitely generated!) requires that $M/\mf mM\ne0$ by \autoref{thm:nakayama}, but $M/\mf mM$ is now an $R/\mf m$-vector space, with action by
	\[[r]_\mf m\cdot[m]_{\mf mM}=[rm]_{\mf mM}.\]
	Namely, this is well-defined because $[r]_\mf m=[s]_\mf m$ implies that $r-s\in\mf m$, so $([r]_\mf m-[s]_\mf m)\cdot[m]_{\mf mM}=[(r-s)m]_{\mf mM}=[0]_{\mf mM}$.

	But because $M/\mf mM$ is a nonzero $R/\mf m$-vector space, linear algebra lets us give $M/\mf mM$ a basis over $R/\mf m$, and so we have a projection map
	\[M/\mf mM\onto R/\mf m\]
	by choosing any of the coordinates. Composing this projection with $M\onto M/\mf mM$, we have a right-exact sequence
	\[\ker\pi\to M\stackrel{\pi}\to R/\mf m\to0.\]
	To finish, we note that tensoring is right-exact and therefore preserves surjections. Thus, there is an induced surjection
	\[N\otimes_R\ker\pi\to N\otimes_RM\stackrel\pi\to N\otimes_R(R/\mf m)\to0.\]
	Now we can unravel. We know that $M\otimes_RN=0$ by hypothesis, and $N\otimes_R(R/\mf m)\cong N/\mf mN$ by \autoref{prop:tensorquotient}. So the end of our exact sequence is
	\[0\to N/\mf mN\to 0,\]
	so $N/\mf mN=0$. Thus, using that $N$ is finitely generated, \autoref{thm:nakayama} tells us that $N=0$.
\end{proof}
\begin{remark}[Nir]
	The local condition is necessary: using \autoref{exe:zmodtensors}, we see $\ZZ/3\ZZ\otimes_\ZZ\ZZ/4\ZZ\cong\ZZ/\gcd(3,4)\ZZ=0$, but neither $\ZZ/3\ZZ$ nor $\ZZ/4\ZZ$ are zero.
\end{remark}
\begin{remark}[Nir] \label{rem:fingentensordomain}
	The finitely generated condition is necessary: working with $\ZZ_{(2)}$-modules, we note that $\ZZ_{(2)}/2\ZZ_{(2)}=\ZZ/2\ZZ$ (by considering the kernel of $\ZZ\into\ZZ_{(2)}\onto\ZZ_{(2)}/2\ZZ_{(2)}$), so
	\[\QQ\otimes_{\ZZ_{(2)}}\ZZ_{(2)}/2\ZZ_{(2)}\cong\QQ\otimes_{\ZZ_{(2)}}\ZZ/2\ZZ\cong0,\]
	where the last congruence is because $\QQ$ is divisible while $\ZZ/2\ZZ$ is torsion. Explicitly, any generator $q\otimes b\in\QQ\otimes_{\ZZ_{(2)}}\ZZ/2\ZZ$ has $q\otimes b=(q/2)\otimes2b=(q/2)\otimes0=0$. But of course, $\QQ\ne0$ and $\ZZ_{(2)}/2\ZZ_{(2)}\cong\ZZ/2\ZZ\ne0$.
\end{remark}
We can even push beyond the local case by localizing.
\begin{corollary}
	Fix $R$ a ring with $M$ and $N$ finitely generated $R$-modules. Then $M\otimes_RN=0$ if and only if $\op{Ann}M+\op{Ann}N=R$.
\end{corollary}
\begin{proof}
	We take the directions independently.
	\begin{itemize}
		\item In one direction, suppose $\op{Ann}M+\op{Ann}N=R$, and we show $M\otimes_RN=0$. then we are promised some $a\in\op{Ann}M$ and $b\in\op{Ann}N$ such that $a+b=1$. Then, for any generator $m\otimes n\in M\otimes_RN$, we see
		\[m\otimes n=1(m\otimes n)=(a+b)(m\otimes n)=(am)\otimes n+m\otimes(bn)=0\otimes n+m\otimes0.\]
		But $0\otimes n=(0\cdot0)\otimes n=0\otimes(0n)=0\otimes0$, and similarly, $m\otimes0=m\otimes(0\cdot0)=(m\cdot0)\otimes0=0\otimes0$. Thus, $m\otimes n=0\otimes0$, so $M\otimes_RN=0$.
		
		\item For the other direction, we localize. Suppose that $I:=\op{Ann}M+\op{Ann}N\subsetneq R$, and we show that $M\otimes_RN\ne0$. Putting $I$ in some maximal ideal $\mf m$, we note that we can localize at $\mf m$, where we see \autoref{cor:localizetensor} gives
		\[(M\otimes_RN)_\mf m\cong M_\mf m\otimes_{R_\mf m}N_\mf m.\]
		Now, by \autoref{prop:fingensupport}, we see that $\mf m\supseteq I\supseteq\op{Ann}M$ (repsectively, $\mf m\supseteq\op{Ann}N$), so $\mf m\in\op{Supp}M$ (respectively, $\mf m\in\op{Ann}N$), so $M_\mf m\ne0$ (respectively, $N_\mf m\ne0$).

		So now that we are in the local case (note $R_\mf m$ is local by \autoref{prop:localizetolocal}), we see \autoref{prop:localtensorintdomain} tells us $M_\mf m\otimes_{R_\mf m}N_\mf m\ne0$. But surely $0_\mf m\cong 0$, so we must have $M\otimes_RN\ne0$ to localize to a nonzero module. This finishes.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark}[Nir]
	The backward direction does not need $M$ and $N$ to be finitely generated. The forward direction does: note
	\[\QQ\otimes_\ZZ\ZZ/2\ZZ=0\]
	because $\QQ$ is divisible while $\ZZ/2\ZZ$ is torsion. (Namely, any generator $q\otimes b=(q/2)\otimes2b=(q/2)\otimes0=0$ vanishes.) However, $\op{Ann}\QQ=\{0\}$ (because $\QQ$ is a domain, $aq=0$ implies $a=0$ or $q=0$), and $\op{Ann}\ZZ/2\ZZ=2\ZZ$ (by \autoref{ex:annrmodi}), so
	\[\op{Ann}\QQ+\op{Ann}\ZZ/2\ZZ=0+2\ZZ=2\ZZ\subsetneq\ZZ.\]
\end{remark}
% As usual, we note that the condition that $M$ and $N$ are finitely generated is crucial.
% \begin{nex}
% 	We note that $\QQ\otimes_\ZZ\ZZ/n\ZZ=0$ because $\QQ$ is divisible and $\ZZ/n\ZZ$ is torsion. But $\QQ\ne0$ and $\ZZ/n\ZZ\ne0$.
% \end{nex}
Anyways, let's finish off \autoref{rem:supptensor}.
\begin{corollary} \label{cor:supptensor}
	Fix $M$ and $N$ finitely generated $R$-modules. Then $\op{Supp}(M\otimes_RN)=\op{Supp}M\cap\op{Supp}N$.
\end{corollary}
\begin{proof}
	Fix a prime $\mf p$. Then we see from \autoref{cor:localizetensor} that
	\[(M\otimes_RN)_\mf p=M_\mf p\otimes_{R_\mf p}N_\mf p.\]
	Now, because $R_\mf p$ is a local ring (by \autoref{prop:localizetolocal}), we see $M_\mf p\otimes_{R_\mf p}N_\mf p$ will vanish if and only if $M_\mf p=0$ or $N_\mf p=0$ by \autoref{prop:localtensorintdomain}. In other words, $\mf p\notin\op{Supp}(M\otimes_RN)$ if and only if $\mf p\notin\op{Supp}M$ or $\mf p\notin\op{Supp}N$, which is the result.
\end{proof}

\subsection{Integrality Preview}
We will spend the rest of class on the following result.
\begin{prop}
	Fix $R$ a ring and an $R$-algebra generated by one element $s\in S$. Letting $I$ be the kernel of $R[x]\to S$ by $x\mapsto s$ so that $S\cong R[x]/I$, we have the following equivalences.
	\begin{listalph}
		\item $S$ is finitely generated as an $R$-module if and only if $I$ contains a monic polynomial (i.e., there is some monic $p(x)\in R[x]$ such that $p(s)=0$).
		\item $S$ is a free, finitely generated $R$-module if and only if $I=(p)$ for some monic polynomial $p$.
	\end{listalph}
\end{prop}
\begin{proof}
	Here is the proof of (a).
	\begin{itemize}
		\item If $S$ is finitely generated as an $R$-module with $n$ generators, we apply \autoref{thm:betterch} with $\mu_s\in\op{End}_R(S)$, where $\mu_s(m):=sm$. (This is an endomorphism because $s(r_1m_1+r_2m_2)=r_1s(m_1)+r_2s(m_2)$.) In particular, \autoref{thm:betterch} gives us some monic polynomial
		\[p(x)=x^n+p_1x^{n-1}+\cdots+p_n\]
		such that $p(\mu_s)=0$. In particular, plugging in $1$ into $p(\mu_s)$, we see that
		\[0=0\cdot1=\left(\mu_s^n+p_1\mu_s^{n-1}+\cdots+p_n\mu_s^0\right)(1)=s^n+p_1s^{n-1}+\cdots+p_n=p(s).\]
		Under the isomorphism $R[x]/I\cong S$ by $x\mapsto s$, we thus note that $p(s)=0$ implies that $[p]_I=[0]_I$ and $p\in I$ is forced. So $I$ does contain a monic polynomial.

		\item In the other direction, suppose
		\[p(x):=x^n+p_1x^{n-1}+\cdots+p_n\in R[x]\]
		is a monic polynomial in $I$. Then we claim $\{1,s,s^2,\ldots,s^{n-1}\}$ will generate $S$ as an $R$-module. To start, we notice that $S=R[s]\cong R[x]/I$ means that any element $m\in S$ can be written as
		\[m=\sum_{k=0}^\infty a_ks^k\]
		for some coefficients $a_\bullet\in R$, where all but finitely many vanish. Thus, to show that $m\in\sum_{i=0}^{n-1}Rs^i$, it suffices to show that $s^k\in\sum_{i=0}^{n-1}Rs^i$ for each $s^k$.

		For this, we induct. If $k<n$, then $s^k\in\left\{1,s,s^2,\ldots,s^{n-1}\right\}$, so $s^k$ provides its own $R$-linear combination to fit in $\sum_{i=0}^{n-1}Rs^i$. Otherwise, take $k\ge n$, and suppose $Rs^\ell\in \sum_{i=0}^{n-1}Rs^i$ for each $\ell<k$. By hypothesis, we see that
		\[0=s^n+p_1s^{n-1}+\cdots+p_n,\]
		so upon multiplying by $s^k$ and rearranging, we find
		\[s^{k}=-p_1s^{k-1}-p_2s^{k-2}-\cdots-p_ns^{k-n}\in\sum_{k=1}^{n-1}Rs^k.\]
		But by the inductive hypothesis, $Rs^\ell\in \sum_{i=0}^{n-1}Rs^i$ for each $\ell<n$, so $s^k\in \sum_{i=0}^{n-1}Rs^i$. This finishes.
	\end{itemize}
	And here is the proof of (b).
	\begin{itemize}
		\item In one direction, take $S$ to be a free, finitely generated $R$-module by $n$ generators. Our work in (the first direction of) (a) provides us some monic polynomial $p$ in $I$ of degree $n$. Further, the work in the second direction in (a) shows that
		\[\left\{1,s,s^2,\ldots,s^{n-1}\right\}\]
		generates $S$ as an $R$-module, but $S$ is freely generated by $n$ elements, so $S$ must be freely generated by the above $n$ elements.\footnote{Formally, this set of $n$ elements provides us a surjection $R^n\onto S$ by $(r_0,\ldots,r_{n-1})\mapsto r_0s^0+\cdots+r_{n-1}s^{n-1}$. But $S\cong R^n$, so we have a composite surjection $R^n\onto S\cong R^n$, which must be an isomorphism by \autoref{prop:epiisiso}. In particular, $R^n\onto S$ is injective, finishing.} 
		
		We claim that $I=(p)$. Certainly $(p)\subseteq I$, so we have left to show $I\subseteq(p)$. Well, suppose that $f\in I$. Because $p$ is monic, we may do Euclidean division with it (!), so we write
		\[f=pq+r,\]
		where we can expand
		\[r(x)=\sum_{k=0}^dr_kx^k\in R[x],\]
		where $d<n$. We claim that $r_\bullet=0$ for each $r_\bullet$, which will finish because it will imply $f=pq$, so $f\in(p)$.

		So we note that $r=f-pq\in I$, so applying $R[x]\to S$ by $x\mapsto s$, we see that
		\[\sum_{k=0}^dr_ks^k=0.\]
		But because $d<n$, the set $\left\{s^0,\ldots,s^d\right\}$ is $R$-linearly independent (formally, add in the terms $0s^k$ for $d\le k<n$ to reduce to the set $\left\{s^0,\ldots,s^n\right\}$, which freely generates). So it does follow that $r_\bullet=0$ for each $r_\bullet$.

		\item The second direction is similar to the second direction in (a). To be explicit, suppose that $I=(p)$ for $p\in R[x]$ where $n=\deg p$. In (a) above, we showed that $\left\{1,s,\ldots,s^{n-1}\right\}$ will generate $S$ as an $R$-module. We claim that these generators are in fact free: suppose that we have some linear relation
		\[\sum_{k=0}^{n-1}c_ks^k=0\]
		for coefficients $c_\bullet\in R$.
		
		Now, let $f(x):=\sum_{k=0}^{n-1}c_kx^k$ so that we are given $f(s)=0$. In particular, $f$ lives in the kernel $R[x]\to S$ by $x\mapsto s$, so $f\in I$. But then $f=pq$ for some $q\in R[x]$. We claim $q=0$, which will follow from a degree-counting argument. Indeed, if $q\ne0$, then we can expand $p(x)=\sum_{k=0}^na_kx^k$ and $q=\sum_{\ell=0}^mb_\ell x^\ell$ so that
		\[(pq)(x)=\sum_{d=0}^{n+m}\left(\sum_{k+\ell=d}a_kb_\ell\right)x^d,\]
		where we have extended the $a_\bullet$ and $b_\bullet$ to be zero where previously undefiend.
		
		In particular, the largest $a_k$ with $a_k\ne0$ is $k=n$, and the largest $b_\ell$ with $b_\ell\ne0$ is $\ell=m$, so the largest we can achieve is $k+\ell\le n+m$, with equality on $k=n$ and $b_\ell=m$. Thus, our leading term would be $a_nb_mx^{m+n}$, which is nonzero because $a_n=1$ (!) and $b_m\ne0$, but this term is zero in $f$, which is our contradiction.

		So instead, we have $q=0$, so $f=pq=0$, so $c_\bullet=0$ for each $c_\bullet$.
		\qedhere
	\end{itemize}
\end{proof}
We close with some definitions.
\begin{definition}[Finite]
	Fix $S$ an $R$-algebra. Then $S$ is \textit{finite} over $R$ if and only if $S$ is finitely generated over $R$ as an $R$-algebra.
\end{definition}
\begin{definition}[Integral]
	Fix $S$ an $R$-algebra. Then an element $s\in S$ is \textit{integral over $R$} if and only if $s$ is a root of some monic polynomial in $R[x]$. If all elements $s\in S$ are integral over $R$, then we say $S$ is \textit{integral over $R$}.
\end{definition}