% !TEX root = ../notes.tex

We localize more.

\subsection{Flat Modules}
Last time we left off with the right-exactness of the tensor product: a right-exact sequence of $R$-modules
\[A\to B\to C\to 0\]
becomes a right-exact sequence
\[M\otimes_RA\to M\otimes_RB\to M\otimes_RC\to 0\]
for any other $R$-module $M$. More formally, we have the following statement.
\begin{proposition}
	Fix $R$ a ring and $M$ an $R$-module. Then the functor $M\otimes_R-:\mathrm{Mod}_R\to\mathrm{Mod}_R$ is right-exact.
\end{proposition}
\begin{proof}
	This is a restatement of the discussion above.
\end{proof}
However, it is not true that a short exact sequence
\[0\to A\to B\to C\to 0\]
will always become a short exact sequence
\[0\to M\otimes_RA\to M\otimes_RB\to M\otimes_RC\to 0.\]
In fact, this is rather rare! Explicitly, the problem is that $M\otimes_RA\to M\otimes_RB$ might not be injectivem ruining exactness at the front, and this is the only obstruction by right-exactness.
\begin{example}
	We work in $\mathrm{Mod}_\ZZ$, and let $n$ be a positive integer. Then tensoring the short exact sequence
	\[0\to\ZZ\stackrel{\times n}\to\ZZ\to\ZZ/n\ZZ\to0\]
	with $\ZZ/n\ZZ$ will give the commutative diagram
	% https://q.uiver.app/?q=WzAsMTAsWzAsMCwiMCJdLFsxLDAsIlxcWlpcXG90aW1lc19cXFpaXFxaWi9uXFxaWiJdLFsyLDAsIlxcWlpcXG90aW1lc19cXFpaXFxaWi9uXFxaWiJdLFszLDAsIlxcWlovblxcWlpcXG90aW1lc19cXFpaXFxaWi9uXFxaWiJdLFs0LDAsIjAiXSxbMSwxLCJcXFpaL25cXFpaIl0sWzIsMSwiXFxaWi9uXFxaWiJdLFswLDEsIjAiXSxbMywxLCJcXFpaL25cXFpaXFxvdGltZXNfXFxaWlxcWlovblxcWloiXSxbNCwxLCIwIl0sWzAsMV0sWzEsMiwiXFx0aW1lcyBuIl0sWzIsM10sWzMsNF0sWzUsNiwiZiJdLFsxLDVdLFs3LDVdLFsyLDZdLFs2LDhdLFszLDhdLFs4LDldXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		 & {\ZZ\otimes_\ZZ\ZZ/n\ZZ} & {\ZZ\otimes_\ZZ\ZZ/n\ZZ} & {\ZZ/n\ZZ\otimes_\ZZ\ZZ/n\ZZ} & 0 \\
		 & {\ZZ/n\ZZ} & {\ZZ/n\ZZ} & {\ZZ/n\ZZ\otimes_\ZZ\ZZ/n\ZZ} & 0
		\arrow["{\times n}", from=1-2, to=1-3]
		\arrow[from=1-3, to=1-4]
		\arrow[from=1-4, to=1-5]
		\arrow["f", from=2-2, to=2-3]
		\arrow[from=1-2, to=2-2]
		\arrow[from=1-3, to=2-3]
		\arrow[from=2-3, to=2-4]
		\arrow[from=1-4, to=2-4]
		\arrow[from=2-4, to=2-5]
	\end{tikzcd}\]
	after tracking through the canonical isomorphisms $\ZZ\otimes_\ZZ M\cong M$. But we can see that $f$ here sends $[k]_n$ lifts to $1\otimes[k]_n$, which goes to $n\otimes[k]_n=1\otimes[0]_n$ and therefore is $[0]_n$ downstairs. So $f$ is the zero map and not injective for any $n>1$.
\end{example}
But sometimes left-exactness will be preserved, and this is a property worthy of a name.
\begin{definition}[Flat]
	Fix $R$ a ring. Then an $R$-module $M$ is \textit{flat} if and only if the functor $M\otimes_R-$ is exact.
\end{definition}
\begin{remark}
	As above, we note that $M\otimes_R-$ will always be right-exact, so $M$ will be flat if and only if it preserves the injectivity at the end of a short exact sequence. In other words, $A\into B$ induces $M\otimes_RA\into M\otimes_RB$.
\end{remark}
\begin{example}
	The ring $R$ is a flat module because $R\otimes_RM\cong M$ (canonically). Explicitly, the following diagram commutes because the map $M\cong R\otimes_RM$ is $m\mapsto1\otimes m$.
	% https://q.uiver.app/?q=WzAsNCxbMCwwLCJBIl0sWzEsMCwiQiJdLFswLDEsIlJcXG90aW1lc19SQSJdLFsxLDEsIlJcXG90aW1lc19SQiJdLFswLDEsIiIsMCx7InN0eWxlIjp7InRhaWwiOnsibmFtZSI6Imhvb2siLCJzaWRlIjoidG9wIn19fV0sWzIsMywiIiwwLHsic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoiaG9vayIsInNpZGUiOiJ0b3AifX19XSxbMCwyXSxbMSwzXV0=&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		A & B \\
		{R\otimes_RA} & {R\otimes_RB}
		\arrow[from=1-1, to=1-2]
		\arrow[from=2-1, to=2-2]
		\arrow[from=1-1, to=2-1]
		\arrow[from=1-2, to=2-2]
	\end{tikzcd}\]
	It follows $R\otimes_RA\to R\otimes_RB$ is injective when $A\into B$ is injective because this map is the composite $R\otimes_RA\cong A\into B\cong R\otimes_RB$, which is injective as the composite of injective maps.
\end{example}
\begin{example}
	Any free $R$-module $R^n$ is also flat by using direct sums. In particular, if we have $A\to B$, then the following diagram commutes.
	% https://q.uiver.app/?q=WzAsNCxbMCwwLCJSXm5cXG90aW1lc19SQSJdLFsxLDAsIlJeblxcb3RpbWVzX1JCIl0sWzAsMSwiKFJcXG90aW1lcyBBKV5uIl0sWzEsMSwiKFJcXG90aW1lcyBCKV5uIl0sWzAsMV0sWzIsM10sWzAsMl0sWzEsM11d&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		{R^n\otimes_RA} & {R^n\otimes_RB} \\
		{(R\otimes_R A)^n} & {(R\otimes_R B)^n}
		\arrow[from=1-1, to=1-2]
		\arrow[from=2-1, to=2-2]
		\arrow[from=1-1, to=2-1]
		\arrow[from=1-2, to=2-2]
	\end{tikzcd}\]
	Indeed, the map $R^n\otimes_R A\to(R\otimes_R A)^n$ is by $(r_k)_{k=1}^n\otimes a\mapsto(r_k\otimes a)_{k=1}^n$, so the commutativity follows. But we see that $A\into B$ means the individual maps $R\otimes_RA\to R\otimes_RB$ are injective, so the bottom row is injective. Tracking the isomorphisms through, we see the top row is also forced to be injective.
\end{example}

\subsection{Localization via Tensoring}
Now let's return to discussing localization, which plays nicely with the tensor product and flatness.
\begin{proposition} \label{prop:localizeistensor}
	Fix $R$ a ring and $U\subseteq R$ a multiplicatively closed subset. Then, for any $R$-module $M$, we have a canonical $R\left[U^{-1}\right]$-module isomorphism
	\[R\left[U^{-1}\right]\otimes_RM\cong M\left[U^{-1}\right]\]
	by $r/u\otimes m\mapsto r/u\cdot m$. (Here, $R\left[U^{-1}\right]\otimes_RM$ is given $R\left[U^{-1}\right]$ by multiplication on the left coordinate.)
\end{proposition}
\begin{proof}
	We define our maps in both directions explcitly. To go $\varphi:M\left[U^{-1}\right]\to R\left[U^{-1}\right]\otimes_RM$, we define
	\[\boxed{\varphi:m/u\mapsto1/u\otimes m}.\]
	For now, we have to check that this is well-defined and an $R\left[U^{-1}\right]$-module homomorphism.
	\begin{itemize}
		\item Well-defined: suppose that $\frac{m_1}{u_1}=\frac{m_2}{u_2}$. Then there is $u\in U$ so that $uu_2m_1=uu_1m_2$. It follows that
		\[\frac1{u_1}\otimes m_1=\left(\frac1{uu_1u_2}\cdot uu_2\right)\otimes m_1=\frac1{uu_1u_2}\otimes uu_2m_1=\frac1{uu_1u_2}\otimes uu_1m_2,\]
		and now running this in reverse shows $\frac1{u_1}\otimes m_1=\frac1{u_2}\otimes m_2$.
		\item Homomorphic: fix $\frac{m_1}{u_1},\frac{m_2}{u_2}\in M\left[U^{-1}\right]\otimes_RM$ and $\frac{s_1}{v_1},\frac{s_2}{v_2}\in R\left[U^{-1}\right]$. Then we compute
		\begin{align*}
			\varphi\left(\frac{s_1}{v_1}\cdot\frac{m_1}{u_1}+\frac{s_2}{v_2}\cdot\frac{m_2}{u_2}\right) &= \varphi\left(\frac{s_1m_1}{v_1u_1}+\frac{s_2m_2}{v_2u_2}\right) \\
			&= \varphi\left(\frac{v_2u_2s_1m_1+v_1u_1s_2m_2}{v_1u_1v_2u_2}\right) \\
			&= \frac1{v_1u_1v_2u_2}\otimes(v_2u_2s_1m_1+v_1u_1s_2m_2) \\
			&= \frac1{v_1u_1v_2u_2}\otimes v_2u_2s_1m_1+\frac1{v_1u_1v_2u_2}\otimes v_1u_1s_2m_2 \\
			&= \frac{s_1}{v_1u_1}\otimes m_1+\frac{s_2}{v_2u_2}\otimes m_2 \\
			&= \frac{s_1}{v_1}\left(\frac1{u_1}\otimes m_1\right)+\frac{s_2}{v_2}\left(\frac1{u_2}\otimes m_2\right) \\
			&= \frac{s_1}{v_1}\varphi\left(\frac{m_1}{u_1}\right)+\frac{s_2}{v_2}\varphi\left(\frac{m_2}{u_2}\right),
		\end{align*}
		which is what we wanted.
	\end{itemize}
	In the other direction, we note that we have a $R$-bilinear map $\psi:R\left[U^{-1}\right]\times M\to M\left[U^{-1}\right]$ by
	\[(r/u,m)\mapsto rm/u.\]
	Quickly, this is well-defined because $\frac{r_1}{u_1}=\frac{r_2}{u_2}$ promises $u$ such that $uu_2r_1=uu_1r_2$, so $uu_2r_1m=uu_1r_2m$, so $\frac{r_1m}{u_1}=\frac{r_2m}{u_2}$. Now, to check $R$-bilinaerity, it suffices to check that
	\[\psi(r/u,r_1m_1+r_2m_2)=\frac{r(r_1m_1+r_2m_2)}u=r_1\cdot\frac{rm_1}u+r_2\cdot\frac{rm_2}{u_2}=r_1\psi(r/u,m_1)+\psi(r/u,m_2),\]
	and
	\[\psi\left(s_1\cdot\frac{r_1}{u_1}+s_2\cdot\frac{r_2}{u_2},m\right)=\psi\left(\frac{u_2s_1r_1+u_1s_2r_2}{u_1u_2},m\right)=s_1\cdot\frac{r_1}{u_1}\cdot m+\frac{r_2}{u_2}\cdot m\]
	after some moving around, which is what we needed.

	The point is that we are promised an $R$-module homomorphism $\psi:R\left[U^{-1}\right]\otimes_RM\to M\left[U^{-1}\right]$ by
	\[\boxed{\psi:r/u\otimes m\mapsto rm/u}\]
	and extending linearly to the full tensor product. It suffices to show $\psi$ is inverse to to $\varphi$, which will show $\varphi$ is an $R\left[U^{-1}\right]$-module isomorphism, and the same will hold for $\psi$, finishing
	\begin{itemize}
		\item Given $m/u\in M\left[U^{-1}\right]$, we note that $(\psi\circ\varphi)(m/u)=\psi(1/u\otimes m)=1m/u=m/u$, so $\psi\circ\varphi=\id_{M\left[U^{-1}\right]}$.
		\item Given $\sum_{k=1}^n(r_k/u_k\otimes m_k)\in R\left[U^{-1}\right]\otimes_RM$, we see that
		\[(\varphi\circ\psi)\left(\sum_{k=1}^n\frac{r_k}{u_k}\otimes m_k\right)=\varphi\left(\sum_{k=1}^n\frac{r_km_k}{u_k}\right)=\sum_{k=1}^n\frac1{u_k}\otimes r_km_k=\sum_{k=1}^n\frac{r_k}{u_k}\otimes m_k,\]
		so $\varphi\circ\psi=\id_{R\left[U^{-1}\right]\otimes_RM}$.
		\qedhere
	\end{itemize}
\end{proof}
\begin{remark} \label{lem:functorialiso}
	The above canonical isomorphism is functorial in the following sense. If we have a map $\varphi:A\to B$, then the following diagram commutes, where all arrows are the induced maps.
	% https://q.uiver.app/?q=WzAsNCxbMCwwLCJSXFxsZWZ0W1Veey0xfVxccmlnaHRdXFxvdGltZXNfUkEiXSxbMSwwLCJSXFxsZWZ0W1Veey0xfVxccmlnaHRdXFxvdGltZXNfUkIiXSxbMCwxLCJBXFxsZWZ0W1Veey0xfVxccmlnaHRdIl0sWzEsMSwiQlxcbGVmdFtVXnstMX1cXHJpZ2h0XSJdLFswLDFdLFsyLDNdLFswLDJdLFsxLDNdXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		{R\left[U^{-1}\right]\otimes_RA} & {R\left[U^{-1}\right]\otimes_RB} \\
		{A\left[U^{-1}\right]} & {B\left[U^{-1}\right]}
		\arrow[from=1-1, to=1-2]
		\arrow[from=2-1, to=2-2]
		\arrow[from=1-1, to=2-1]
		\arrow[from=1-2, to=2-2]
	\end{tikzcd}\]
	Indeed, we take $\frac ru\otimes a\mapsto\frac ru\otimes\varphi(a)\mapsto\frac{r\varphi(a)}u$ along the top, and we take $\frac ru\otimes a\mapsto\frac{ra}u\mapsto\frac{\varphi(ra)}u=\frac{r\varphi(a)}u$ along the bottom.
\end{remark}
The above is nice because it means we technically would only need to check that $R\left[U^{-1}\right]$ exists in order to define localization of general modules. In other words, we have a somewhat unified paradigm to think about localization by merely focusing on tensor products.

As a quick example, we can see that localization commutes with direct sums.
\begin{proposition} \label{prop:localdirectsums}
	Fix $R$ a ring and $U\subseteq R$ a multiplicatively closed subset with $\mathcal M$ a collection of $R$-modules. Then
	\[\left(\bigoplus_{M\in\mathcal M}M\right)\left[U^{-1}\right]\cong\bigoplus_{M\in\mathcal M}M\left[U^{-1}\right]\]
	by sending $\frac1u(m_M)_{M\in\mathcal M}\mapsto\left(\frac{m_M}u\right)_{M\in\mathcal M}$.
\end{proposition}
\begin{proof}
	The main point is that tensor products commute with direct sums. Indeed, we have the canonical isomorphisms
	\[\left(\bigoplus_{M\in\mathcal M}M\right)[U^{-1}]\cong\left(\bigoplus_{M\in\mathcal M}M\right)\otimes_RR[U^{-1}]\stackrel*\cong\bigoplus_{M\in\mathcal M}M\otimes_RR[U^{-1}]\cong\prod_{i=1}^nM\left[U^{-1}\right],\]
	where in $\stackrel*\cong$ we used the fact that tensor products commute with arbitrary direct sums. Actually tracking these isomorphisms through, we see that $\frac1u(m_M)_{M\in\mathcal M}$ goes to $(m_M)_{M\in\mathcal M}\otimes1/u$ goes to $(m_M\otimes1/u)_{M\in\mathcal M}$ goes to $(m_M/u)_{M\in\mathcal M}$, which is what we wanted.
\end{proof}

\subsection{Localization via Flatness}
The following result looks like it's about localization but is actually about flatness.
\begin{proposition} \label{prop:localexact}
	Fix $R$ a ring and $U\subseteq R$ a multiplicatively closed subset. Then localization is an exact functor: given a short exact sequence of $R$-modules
	\[0\to A\to B\to C\to 0,\]
	then we have a short exact sequence of $R\left[U^{-1}\right]$-modules
	\[0\to A\left[U^{-1}\right]\to B\left[U^{-1}\right]\to C\left[U^{-1}\right]\to 0.\]
\end{proposition}
\begin{proof}
	For visual reasons, note that we have the following commutative diagram where the vertical arrows are $R\left[U^{-1}\right]$-module isomorphisms. (The diagram commutes by \autoref{lem:functorialiso}.)
	% https://q.uiver.app/?q=WzAsMTAsWzEsMSwiUlxcbGVmdFtVXnstMX1cXHJpZ2h0XVxcb3RpbWVzX1JBIl0sWzIsMSwiUlxcbGVmdFtVXnstMX1cXHJpZ2h0XVxcb3RpbWVzX1JCIl0sWzMsMSwiUlxcbGVmdFtVXnstMX1cXHJpZ2h0XVxcb3RpbWVzX1JDIl0sWzEsMCwiQVxcbGVmdFtVXnstMX1cXHJpZ2h0XSJdLFswLDAsIjAiXSxbMCwxLCIwIl0sWzIsMCwiQlxcbGVmdFtVXnstMX1cXHJpZ2h0XSJdLFszLDAsIkNcXGxlZnRbVV57LTF9XFxyaWdodF0iXSxbNCwxLCIwIl0sWzQsMCwiMCJdLFswLDFdLFs0LDNdLFs1LDBdLFszLDBdLFszLDZdLFs2LDFdLFs2LDddLFs3LDJdLFs3LDldLFsxLDJdLFsyLDhdXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		0 & {A\left[U^{-1}\right]} & {B\left[U^{-1}\right]} & {C\left[U^{-1}\right]} & 0 \\
		0 & {R\left[U^{-1}\right]\otimes_RA} & {R\left[U^{-1}\right]\otimes_RB} & {R\left[U^{-1}\right]\otimes_RC} & 0
		\arrow[from=2-2, to=2-3]
		\arrow[from=1-1, to=1-2]
		\arrow[from=2-1, to=2-2]
		\arrow[from=1-2, to=2-2]
		\arrow[from=1-2, to=1-3]
		\arrow[from=1-3, to=2-3]
		\arrow[from=1-3, to=1-4]
		\arrow[from=1-4, to=2-4]
		\arrow[from=1-4, to=1-5]
		\arrow[from=2-3, to=2-4]
		\arrow[from=2-4, to=2-5]
	\end{tikzcd}\]
	This is to say that it suffices to show that the bottom row is exact. The right-exactness of the bottom row follows from the fact that it is induced by the tensoring functor $R\left[U^{-1}\right]\otimes_R-$.

	Thus, we only need to show that localization preserves embeddings. Letting $\varphi:A\into B$ be the original map and $\overline\varphi:A\left[U^{-1}\right]\to B\left[U^{-1}\right]$ be the induced map, then we need to check that $\ker\overline\varphi$ is trivial. Well, if $\overline\varphi\left(\frac au\right)=0$ for some $\frac au\in A\left[U^{-1}\right]$, then we note
	\[\frac01=\overline\varphi\left(\frac au\right)=\frac{\varphi(a)}u,\]
	so there exists $v\in U$ such that $\varphi(va)=v\varphi(a)=0$. Because $\ker\varphi$ is trivial, we are forced to have $va=0$, so $\frac au=0$. Thus, $\ker\overline\varphi$ is indeed trivial.
\end{proof}
\begin{corollary} \label{cor:localflat}
	Fix $R$ a ring and $U\subseteq R$ a multiplicatively subset. Then $R\left[U^{-1}\right]$ is flat as an $R$-module.
\end{corollary}
\begin{proof}
	The commutative diagram in the proof of \autoref{prop:localexact} has been shown to have exact rows (over the course of the entire proof). The exactness of the bottom row shows $R\left[U^{-1}\right]$ is flat.
\end{proof}
\begin{corollary} \label{cor:localizekercoker}
	Fix $R$ a ring and $U\subseteq R$ a multiplicative subset. Then let $\varphi:A\to B$ be an $R$-module homomorphism and $\overline\varphi:A\left[U^{-1}\right]\to B\left[U^{-1}\right]$ be the localized morphism. Then
	\[(\ker\varphi)\left[U^{-1}\right]\cong\ker\overline\varphi\quad\text{and}\quad(\coker\varphi)\left[U^{-1}\right]\cong\coker\overline\varphi.\]
	In particular, if $\varphi$ is injective/surjective/isomorphic, then $\overline\varphi$ is injective/surjective/isomorphc.
\end{corollary}
\begin{proof}
	We deal with the kernel and the cokernel separately.
	\begin{itemize}
		\item The main point is that we have the short exact sequence
		\[0\to\ker\varphi\to A\stackrel\varphi\to\im\varphi\to0.\]
		Localizing, we get the short exact sequence
		\[0\to(\ker\varphi)\left[U^{-1}\right]\to A\left[U^{-1}\right]\stackrel{\overline\varphi}\to(\im\varphi)\left[U^{-1}\right]\to0.\]
		By exactness, we see that $(\ker\varphi)\left[U^{-1}\right]\cong\ker\overline\varphi$.
		
		Thus, $\varphi$ being injective implies $\ker\varphi=0$ implies $\ker\overline\varphi=0$ implies $\overline\varphi$ is injective.

		\item The main point is that we have the short exact sequence
		\[0\to A/\ker\varphi\stackrel\varphi\to B\to\coker\varphi\to0,\]
		where $\stackrel\varphi\to$ is actually the induced map. Localizing, we get the short exact sequence
		\[0\to(A/\ker\varphi)\left[U^{-1}\right]\stackrel{\overline\varphi}\to B\left[U^{-1}\right]\to(\coker\varphi)\left[U^{-1}\right]\to0.\]
		By exactness again, we see that $(\coker\varphi)\left[U^{-1}\right]\cong\coker\varphi$.
		
		Thus, $\varphi$ being surjective implies $\coker\varphi=0$ implies $\coker\overline\varphi=0$ implies $\overline\varphi$ is surjective.
	\end{itemize}
	Combining the two points implies that, if $\varphi$ is isomorphic (namely, bijective), then $\overline\varphi$ will be as well.
\end{proof}
Flatness also gives us the following result, which again looks like it's about localization but is really about flatness.
\begin{corollary} \label{lem:localizeintersection}
	Fix $R$ a ring and $U\subseteq R$ a multiplicatively closed subset. Then, taking $M_1,\ldots,M_n\subseteq M$ finitely many $R$-modules of some $R$-module $M$, we get
	\[\bigcap_{i=1}^nM_i\left[U^{-1}\right]=\left(\bigcap_{i=1}^nM_i\right)\left[U^{-1}\right].\]
	Note these intersections make sense because the $M_i$ all live inside of $M$.
\end{corollary}
\begin{proof}
	The main point is that intersecitons can be realized as a kernel. Namely, consider the left-exact sequence
	\[0\to\bigcap_{i=1}^nM_i\to M\to\prod_{i=1}^nM/M_i.\tag{$*$}\label{eq:lesintersect}\]
	It is not too hard to check manually that this sequence is in fact left-exact: the map $\bigcap M_i\to M$ is an embedding and hence injective, and $x\in\ker\left(M\to\prod M/M_i\right)$ if and only if $x\in M_i$ for each $M_i$ if and only if $x\in\bigcap M_i$.
	
	Now, we would like to localize \autoref{eq:lesintersect}. Before doing so, we note that \autoref{prop:localdirectsums} gives us the canonical isomorphism
	\[\left(\prod_{i=1}^nM/M_i\right)[U^{-1}]\cong\prod_{i=1}^n(M/M_i)\left[U^{-1}\right],\]
	which is legal because finite products are in fact coproducts. (Here is where we use the finiteness condition!) As in \autoref{prop:localdirectsums}, we can actually track through these isomorphisms as sending $\frac1u([x_k]_{M_i})_{i=1}^n$ to $\left(\frac1u[x_k]_{M_i}\right)_{i=1}^n$.
	
	Continuing, we note that we can compute $(M/M_i)\left[U^{-1}\right]$ by localizing the short exact sequence
	\[0\to M_i\to M\to M/M_i\to 0,\]
	which will tell us that $\frac M{M_i}\left[U^{-1}\right]\cong\frac{M\left[U^{-1}\right]}{M_i\left[U^{-1}\right]}$ by $\frac1u[x]_{M_i}\mapsto\left[\frac xu\right]_{M_i\left[U^{-1}\right]}$. Stitching these isomoprhisms together gives us an isomorphism
	\[\left(\prod_{i=1}^nM/M_i\right)[U^{-1}]\cong\prod_{i=1}^n\frac{M\left[U^{-1}\right]}{M_i\left[U^{-1}\right]}\]
	by taking $\frac1u\left([x_k]_{M_i}\right)_{i=1}^n$ to $\left([\frac{x_k}u]_{M_i\left[U^{-1}\right]}\right)_{i=1}^n$.

	Only now we do localize \autoref{eq:lesintersect}. Upon localization, we get the left-exact sequence\footnote{Being exact implies being left-exact. If this causes discomfort, replace the left-exact sequence $0\to A\to B\to C$ with the short exact sequence $0\to A\to B\to\im(B\to C)\to0$.}
	\[0\to\left(\bigcap_{i=1}^nM_i\right)\left[U^{-1}\right]\to M\left[U^{-1}\right]\to\left(\prod_{i=1}^nM/M_i\right)\left[U^{-1}\right]\cong\prod_{i=1}^n\frac{M\left[U^{-1}\right]}{M_i\left[U^{-1}\right]},\]
	By exactness, we see that to prove the result it remains to compute the kernel of the composite
	\[M\left[U^{-1}\right]\to\left(\prod_{i=1}^nM/M_i\right)\left[U^{-1}\right]\cong\prod_{i=1}^n\frac{M\left[U^{-1}\right]}{M_i\left[U^{-1}\right]}.\]
	Well, this map sends $\frac xu$ to $\frac1u([x]_{M_i})_{i=1}^n$ to $\left([\frac xu]\right)_{i=1}^n$, so the only way for to be in the kernel is for $\frac xu\in M_i\left[U^{-1}\right]$ for each $M_i$. It follows that the kernel is
	\[\bigcap_{i=1}^nM_i\left[U^{-1}\right],\]
	which is what we wanted.
\end{proof}
We need to be careful because localization need not commute with infinite interseciotns.
\begin{example}
	Set $R:=k[x]$ and $U=R\setminus\{0\}$. The main issue is that
	\[\bigcap_{a\in k}(x-a)=(0).\]
	Now, on one hand, $(x-a)\left[U^{-1}\right]=k(x)$ because $U$ is allowed to divide by $(x-a)$. On the other hand, $(0)\left[U^{-1}\right]=(0)$ because no amount of division can make $0$ nonzero. Thus,
	\[\left(\bigcap_{a\in k}(x-a)\right)\left[U^{-1}\right]=(0)\left[U^{-1}\right]=(0)\ne k(x)=\bigcap_{a\in k}(x-a)\left[U^{-1}\right].\]
\end{example}

\subsection{Tensor--Restriction Adjunction}
We start by discussing a particular adjuction. We have the following definition.
\begin{definition}[Restriction]
	Fix $S$ an $R$-algebra, which means we are promised a ring homomorphism $\psi:R\to S$. Given an $S$-module $N$, we can give $N$ an $R$-action by
	\[r\cdot x:=\psi(r)x.\]
	The abelian group $N$ with this $R$-action is the \textit{restriction} $\op{Res}_R^SN$.
\end{definition}
In other words, the $S$-action on $N$ is equivalent to a ring map $S\to\op{End}N$, so we get an $R$-action by precomposition: $R\stackrel\psi\to S\to\op{End}N$.
\begin{lemma}
	Fix $S$ an $R$-algebra. Then the map $\op{Res}^S_R:\textrm{Mod}_S\to\textrm{Mod}_R$ is a functor.
\end{lemma}
\begin{proof}
	For concreteness, fix our map $\psi:R\to S$. We start by discussing how to restrict morphisms. Given an $S$-module morphism $f:M\to N$, we claim that the ``function data'' of $\varphi$ in fact makes an $R$-module morphism $\op{Res}_R^S(f):\op{Res}_S^RM\to\op{Res}_S^RN$. In other words, we define
	\[\op{Res}_R^S(f)(m):=f(m).\]
	Note this makes $\op{Res}_R^S(f)$ at least a morphism of abelian groups, so in particular it is additive. So to check that $\op{Res}_R^S(f)$ is an $R$-module morphism, we merely pick up $r\in R$ and $m\in M$ and note
	\[\op{Res}_R^S(f)(rm)f(\psi(r)m)=\psi(r)f(m)=r\cdot\op{Res}_R^S(f)(m).\]
	Now, to show functoriality, we note that $\op{Res}_R^S(\id_M)(m)=m$ for any $S$-module $M$ and $m\in M$. And for $f:A\to B$ and $g:B\to C$ morphisms of $S$-modules, we have $\op{Res}_R^S(g\circ f)(a)=(g\circ f)(a)=\left(\op{Res}_R^S(g)\circ\op{Res}_R^S(g)\right)(a)$.
\end{proof}

In the other direction, if $M$ is an $R$-module, we can create an $S$-module the ``induced'' module $\op{Ind}_R^SM:=S\otimes_RM$, where we get an $S$-action by multiplying on the left coordinate.

Because tensoring is functorial, we get that $S\otimes_R-$ is automatically a functor $\textrm{Mod}_R\to\textrm{Mod}_R$. So to check that $S\otimes_R-$ is a functor $\textrm{Mod}_R\to\textrm{Mod}_S$, it suffices to show $f:A\to B$ in $\textrm{Mod}_R$ can actually be a lifted to an $S$-module morphism $S\otimes_RA\to S\otimes_RB$. Well, $f$ is already additive, so we merely check
\[f(s(x\otimes a))=f(sx\otimes a)=sx\otimes f(a)=s(x\otimes f(a))=s\cdot f(x\otimes a).\]
Thus, we do indeed have a functor $\textrm{Mod}_R\to\textrm{Mod}_S$.

With functors going in both directions introduced like this, they had better form an adjoint pair.
\begin{proposition} \label{prop:tenresadjoint}
	Let $S$ be an $R$-algebra. Then, given an $R$-module $M$ and an $S$-module $N$, we have a canonical isomorphism (of abelian groups)
	\[\op{Hom}_R(M,\op{Res}_R^SN)\cong\op{Hom}_S(S\otimes_RM,N).\]
\end{proposition}
\begin{proof}
	We construt forwards and backwards maps manually.
	\begin{itemize}
		\item Fix $f\in\op{Hom}_R(M,\op{Res}_R^SN)$. Then we define $\widetilde f\in\op{Hom}_S(S\otimes_RM,N)$ by defining
		\[\widetilde f(s\otimes m)=sf(m).\]
		Note the computation $sf(m)$ in the above is viewing $f(m)\in N$ as an $S$-module. We have the following checks on $f\mapsto\overline f$.
		\begin{itemize}
			\item Well-defined: to show there is a map $\widetilde f:S\otimes_RM\to N$ as described, we need to show that $\widetilde f:S\times R\to N$ defined by
			\[\widetilde f(s,m):=sf(m)\]
			is $R$-bilinear. Given $r_1,r_2\in R$ and $s_1,s_2\in S$ and $m\in M$,
			\[\widetilde f(r_1s_1+r_2s_2,m)=(r_1s_1+r_2s_2)f(m)=r_1\widetilde f(s_1,m)+r_2\widetilde f(s_2,m).\]
			Given $s\in S$ and $r_1,r_2\in R$ and $m\in M$,
			\[\widetilde f(s,r_1m_1+r_2m_2)=sf(r_1m_1+r_2m_2)=r_1\widetilde f(s,m_1)+r_2\widetilde f(s,m_2).\]
			Thus, we have an $R$-module map $\widetilde f:S\otimes_RM\to N$. To check $\widetilde f$ is an $S$-module map, we note that $\widetilde f$ is already additive, so it suffices to pick up $s\in S$ and $x\otimes m\in S\otimes_RM$ and note
			\[\widetilde f(s(x\otimes m))=\widetilde f((sx)\otimes m)=(sx)f(m)=s\widetilde f(x\otimes m).\]
			\item Homomorphic: we show that $f\mapsto\widetilde f$ is a homomorphism of (abelian) groups. Indeed, fix $f,g\in\op{Hom}_R(M,\op{Res}_R^SN)$ and $s\otimes m\in S\otimes_RM$ so that
			\[\widetilde{f+g}(s\otimes m)=s(f+g)(m)=sf(m)+sg(m)=(\widetilde f+\widetilde g)(s\otimes m).\]
			\item Injective: we show $f\mapsto\widetilde f$ has trivial kernel. Indeed, suppose $f\in\op{Hom}_R(M,\op{Res}_R^SN)$ has $\widetilde f=0$. Then, for any $m\in M$, we see
			\[f(m)=1_Sf(m)=\widetilde f(1_S\otimes m)=0.\]
		\end{itemize}

		\item In the other direction, motivated by the above injectivity check, we notice that we have an $R$-module map $\iota:M\to S\otimes_RM$ by $m\mapsto 1_S\otimes m$. Indeed, for $r_1,r_2\in R$ and $m_1,m_2\in M$, we see
		\[\iota(r_1m_1+r_2m_2)=1_S\otimes(r_1m_1+r_2m_2)=r_1\iota(m_1)+r_2\iota(m_2).\]
		Now, suppose that we have some $g\in\op{Hom}_S(S\otimes_RM,N)$. Note that the same underlying function $g$ is an $R$-module map as well: $g$ is already additive, so we need to check that $r\in R$ and $s\otimes m\in S\otimes_RM$ has
		\[r\cdot g(s\otimes m)=r1_S\cdot g(s\otimes m)=g(r1_S\cdot s\otimes m)=g(r(s\otimes m)).\]
		Thus, we are greanted the map $g\mapsto g\circ\iota$ from $\op{Hom}_S(S\otimes_RM,N)$ to $\op{Hom}_R(M,\op{Res}_R^SN)$.
	\end{itemize}
	Note that it merely remains to check the surjectivity of $f\mapsto\widetilde f$, so it suffices to show that, for any $g\in\op{Hom}_S(S\otimes_RM,N)$, we have
	\[\widetilde{g\circ\iota}=g.\]
	Indeed, given $m\in M$,
	\[\widetilde{g\circ\iota}(s\otimes m)=s(g\circ\iota)(m)=sg(1\otimes m)=g(s\otimes m),\]
	where in the last step we are viewing $g$ as an $S$-module map. This finishes.
\end{proof}
\begin{remark}
	One can in fact show that the exhibited isomorphism makes tensoring left-adjoint to restriction. We will not run the checks to form an adjoint pair.
\end{remark}

\subsection{Base Change}
Next let's discuss base change. Again, fix $S$ an $R$-algebra. Given two $R$-modules named $M$ and $N$, we can form $S$-modules
\[S\otimes_R\op{Hom}_R(M,N)\qquad\text{and}\qquad\op{Hom}_S(S\otimes_RM,S\otimes_RN),\]
where the functor $S\otimes_R-:\textrm{Mod}_R\to\textrm{Mod}_S$ was described in the previous subsection. In general, there need not be an isomorphism between these $S$-modules, but there is a canonical map from the left to the right.
\begin{lemma} \label{lem:basechange}
	Fix $S$ an $R$-algebra with $R$-modules $M$ and $N$. Then there is a canonical $S$-module map
	\[\alpha:S\otimes_R\op{Hom}_R(M,N)\to\op{Hom}_S(S\otimes_RM,S\otimes_RN).\]
\end{lemma}
\begin{proof}
	The main idea is to use \autoref{prop:tenresadjoint}. To begin with, note that there is a function
	\[\gamma:\op{Hom}_R(M,N)\to\op{Hom}_S(S\otimes_RM,S\otimes_RN)\]
	by using the fact $S\otimes_R-$ is a functor. In particular, $f:M\to N$ has $\gamma(f)(s\otimes m)=s\otimes f(m)$. Observe that $\gamma$ in fact induces a function
	\[\gamma:\op{Hom}_R(M,N)\to\op{Res}^S_R\op{Hom}_S(S\otimes_RM,S\otimes_RN)\]
	because the underlying sets involved have not changed. We claim that $\gamma$ is in fact an $R$-module morphism. Well, fix $r_1,r_2\in R$ and $f_1,f_2\in\op{Hom}_R(M,N)$ with $s\otimes m\in S\otimes_RM$, and we see
	\[\gamma(r_1f_1+r_2f_2)(s\otimes m)=s\otimes(r_1f_1+r_2f_2)(m)=r_1(s\otimes f_1(m))+r_2(s\otimes f_2(m))=\big(r_1\gamma(f_1)+r_2\gamma(f_2)\big)(s\otimes m).\]
	Now, because $\gamma$ is an $R$-module map, \autoref{prop:tenresadjoint} promises a canonical map
	\[\widetilde\gamma:S\otimes_R\op{Hom}_R(M,N)\to\op{Hom}(S\otimes_RM,S\otimes_RN).\]
	In fact, we can compute $\widetilde\gamma$ by tracking \autoref{prop:tenresadjoint} and $\gamma$ through. Given $s\otimes f\in S\otimes_R\op{Hom}_R(M,N)$ and $s_0\otimes m_0\in S\otimes_RM$, we have
	\[\widetilde\gamma(s\otimes f)(s_0\otimes m_0)=s\cdot\gamma(f)(s_0\otimes m_0)=s\cdot(s_0\otimes f(m_0))=(ss_0)\otimes f(m_0).\]
	This finishes.
\end{proof}
\begin{remark}[Nir] \label{rem:functorialbasechange}
	We briefly remark that $\alpha$ is functorial in $M$: if we have a map $\varphi:M\to M'$, then the following diagram commutes, where the vertical maps are induced.
	% https://q.uiver.app/?q=WzAsNCxbMCwwLCJTXFxvdGltZXNfUlxcb3B7SG9tfV9SKE0nLE4pIl0sWzEsMCwiXFxvcHtIb219X1MoU1xcb3RpbWVzX1JNJyxTXFxvdGltZXNfUk4pIl0sWzAsMSwiU1xcb3RpbWVzX1JcXG9we0hvbX1fUihNLE4pIl0sWzEsMSwiXFxvcHtIb219X1MoU1xcb3RpbWVzX1JNLFNcXG90aW1lc19STikiXSxbMCwxLCJcXGFscGhhIl0sWzAsMiwiXFx2YXJwaGkiLDJdLFsyLDMsIlxcYWxwaGEiLDJdLFsxLDMsIlxcdmFycGhpIl1d&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		{S\otimes_R\op{Hom}_R(M',N)} & {\op{Hom}_S(S\otimes_RM',S\otimes_RN)} \\
		{S\otimes_R\op{Hom}_R(M,N)} & {\op{Hom}_S(S\otimes_RM,S\otimes_RN)}
		\arrow["\alpha", from=1-1, to=1-2]
		\arrow["\varphi"', from=1-1, to=2-1]
		\arrow["\alpha"', from=2-1, to=2-2]
		\arrow["\varphi", from=1-2, to=2-2]
	\end{tikzcd}\]
	To see this, track some $s\otimes f$ from the top-left.
	\begin{itemize}
		\item Moving along the top, $s\otimes f$ goes to $(s_0\otimes m_0')\mapsto(ss_0\otimes f(m_0'))$ goes to $(s_0\otimes m_0)\mapsto(ss_0\otimes f(\varphi m_0))$.
		\item Moving along the bottom, $s\otimes f$ goes to $s_0\otimes f\varphi$ goes to $(s_0\otimes m_0)\mapsto(ss_0\otimes f(\varphi m_0))$.
	\end{itemize}
\end{remark}
We would like the above $\alpha$ to be an isomorphism, but this requires some hypotheses. To start, here is a special case, which we will generalize shortly.
\begin{lemma} \label{lem:basechangefree}
	Work in the set-up of \autoref{lem:basechange}. If $M=R^n$ for some positive integer $n$, then $\alpha$ is an isomorphism.
\end{lemma}
\begin{proof}
	We proceed by brute force. We will just show directly that $S\otimes_R\op{Hom}_R(R^n,N)\cong\op{Hom}_S(S\otimes_RR^n,S\otimes_RN)$, and tracking the isomorphism through will reveal that it is $\alpha$. Pick up some $s\otimes f\in S\otimes_R\op{Hom}_R(R^n,N)$ which we will track through.
	\begin{itemize}
		\item Note that $S\otimes_R\op{Hom}_R(R^n,N)\cong S\otimes_R\op{Hom}_R(R,N)^n\cong S\otimes_RN^n$ by sending $s\otimes f$ to $s\otimes(1_R\mapsto f(e_k))_{k=1}^n$ to $s\otimes(f(e_k))_{k=1}^n$.
		
		(Here, the $e_\bullet$ are the basis for $R^n$.)
		\item Note that $S\otimes_RN^n\cong(S\otimes N)^n$ by sending $s\otimes(f(e_k))_{k=1}^n$ to $(s\otimes f(e_k))_{k=1}^n$.
		\item Note that $(S\otimes_RN)^n\cong\op{Hom}_S(S^n,S\otimes_RN)$ by sending $(s\otimes f(e_k))_{k=1}^n$ to the morphism
		\[(s_k)_{k=1}^n\mapsto\sum_{k=1}^nss_k\otimes f(e_k).\]
		\item Note that $\op{Hom}_S(S^n,S\otimes_RN)\cong\op{Hom}_S((S\otimes_RR)^n,S\otimes_RN)$ by sending the morphism $(s_k)_{k=1}^n\mapsto\sum_kss_k\otimes f(e_k)$ to the morphism defined by $(s_k\otimes1)_{k=1}^n\mapsto\sum_kss_k\otimes f(e_k)$.
		
		In particular, the morphism in the codomain is
		\[(s_k\otimes r_k)_{k=1}^n\mapsto\sum_{k=1}^nss_k\otimes f(r_ke_k).\]
		\item Note that $\op{Hom}_S((S\otimes_RR)^n,S\otimes_RN)\cong\op{Hom}_S(S\otimes R^n,S\otimes_RN)$ by sending $(s_k\otimes r_k)_{k=1}^n\mapsto\sum_k ss_k\otimes r_kf(e_k)$ to
		\[s_0\otimes(r_k)_{k=1}^n\mapsto\sum_{k=1}^nss_0\otimes f(r_ke_k)=ss_0\otimes f\left((r_k)_{k=1}^n\right).\]
	\end{itemize}
	So indeed, we have tracked our isomorphism, and we can see from the last point that $s\otimes f$ has gone to $s_0\otimes m\mapsto ss_0\otimes f(m)$, as needed by $\alpha$.
\end{proof}
We would like to extend the above argument to work more generally, but this will require some hypotheses. One condition will be that $S$ is flat over $R$; for the other condition we have the following definition.
\begin{definition}[Finitely presented]
	An $R$-module $M$ is \textit{finitely presented} if and only if there are $M$ is finitely generated, and we can find $R^m$ an $R^n$ making the following right-exact sequence
	\[R^m\to R^n\to M\to 0.\]
\end{definition}
In other words, we need to be able to find some $R^m$ which can surject onto the kernel of $R^n\onto M$; i.e., the kernel of our map $R^n\onto M$ is finitely generated.
\begin{example}
	The free $R$-module $R^n$ is finitely presented due to the sequence $0\to R^n\to R^n\to 0$.
\end{example}
\begin{example} \label{ex:fingennoetherian}
	Fix $R$ a Noetherian ring and $M$ a finitely generated module. Then there is $R^n$ with a map $\varphi:R^n\to M$. Now, because $R$ is Noetherian, $R^n$ will be a Noetherian module (see \autoref{prop:fingennoetherian}), so the $R$-submodule $\ker\varphi\subseteq R^n$ will be finitely generated over $R$. Thus, $M$ is finitely presented.
\end{example}
\begin{nex}
	Let $R=k[x_1,x_2,\ldots]$ and $I=(x_1,x_2,\ldots)$. Then we claim $R/I$ is finitely generated (because $R\onto R/I$) but not finitely presented. Indeed, if $R/I$ were finitely presented, then there would be a sequence $0\to K\to R^m\to R/I\to 0$ where $K$ is finitely generated; comparing this with $0\to I\to R\to R/I\to0$ will force $I$ to be finitely genreated, which is false.
	
	Indeed, we have the following commutative diagram.
	% https://q.uiver.app/?q=WzAsMTAsWzAsMSwiMCJdLFsxLDEsIkkiXSxbMiwxLCJSIl0sWzMsMSwiUi9JIl0sWzQsMSwiMCJdLFsyLDAsIlJebSJdLFszLDAsIlIvSSJdLFsxLDAsIksiXSxbNCwwLCIwIl0sWzAsMCwiMCJdLFs2LDMsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzUsNl0sWzAsMV0sWzEsMl0sWzIsM10sWzMsNF0sWzYsOF0sWzcsNV0sWzUsMiwiIiwxLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzcsMSwiIiwxLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzksN11d
	\[\begin{tikzcd}
		0 & K & {R^m} & {R/I} & 0 \\
		0 & I & R & {R/I} & 0
		\arrow[Rightarrow, no head, from=1-4, to=2-4]
		\arrow[from=1-3, to=1-4]
		\arrow[from=2-1, to=2-2]
		\arrow[from=2-2, to=2-3]
		\arrow[from=2-3, to=2-4]
		\arrow[from=2-4, to=2-5]
		\arrow[from=1-4, to=1-5]
		\arrow[from=1-2, to=1-3]
		\arrow[dashed, from=1-3, to=2-3]
		\arrow[dashed, from=1-2, to=2-2]
		\arrow[from=1-1, to=1-2]
	\end{tikzcd}\]
	The middle arrow is induced by $R^m$ being projective: we take the images of the basis vectors $R^m\to R/I$ and then pull them back in whatever way we want to $R$, defining a map $R^m\to R$. The diagram induces the map $K\to I$.

	The snake lemma now tells us that $\coker(K\to I)\cong\coker\left(R^m\to R\right)$, which is finitely generated because $R$ will surject onto it. But then the short exact sequence
	\[0\to\im(K\to I)\to I\to\coker(K\to I)\to0\]
	forces $I$ to be finitely generated.
\end{nex}
Now, here is the culmination of base change.
\begin{proposition} \label{prop:basechange}
	Work in the set-up of \autoref{lem:basechange}. Then if $S$ is flat and $M$ is finitely presented, then the $\alpha$ from \autoref{lem:basechange} is an isomorphism.
\end{proposition}
\begin{proof}
	We begin by writing down the finite presentation
	\[R^m\to R^n\to M\to 0\]
	of $M$. The idea is to $M$ is ``close enough'' to being $R^n$, allowing us to reduce to \autoref{lem:basechangefree}.
	
	We now create two left-exact sequences.
	\begin{itemize}
		\item Taking $\op{Hom}_R(-,N)$ gives us a left-exact sequence
		\[0\to\op{Hom}_R(M,N)\to\op{Hom}_R(R^n,N)\to\op{Hom}_R(R^m,N),\]
		and by flatness of $S$, we get another left-exact sequence
		\[0\to S\otimes_R\op{Hom}_R(M,N)\to S\otimes_R\op{Hom}_R(R^n,N)\to S\otimes_R\op{Hom}_R(R^m,N).\tag{1}\label{eq:basechangeles1}\]
		\item Alternatively, note that we directly have a right-exact sequence
		\[S\otimes_RR^m\to S\otimes_RR^n\to S\otimes_RM\to0,\]
		upon which $\op{Hom}_S(-,S\otimes_RN)$ gives the right-exact sequence
		\[0\to\op{Hom}_S(S\otimes_RM,S\otimes_RN)\to\op{Hom}_S(S\otimes_RR^n,S\otimes_RN)\to\op{Hom}_S(S\otimes_RR^m,S\otimes_RN).\tag{2}\label{eq:basechangeles2}\]
	\end{itemize}
	Now we can relate \hyperref[eq:basechangeles1]{(1)} and \hyperref[eq:basechangeles2]{(2)} by $\alpha$: functoriality of $\alpha$ (see \autoref{rem:functorialbasechange}) gives the following commutative diagram with exact rows.
	% https://q.uiver.app/?q=WzAsOCxbMCwwLCIwIl0sWzEsMCwiU1xcb3RpbWVzX1JcXG9we0hvbX1fUihNLE4pIl0sWzIsMCwiU1xcb3RpbWVzX1JcXG9we0hvbX1fUihSXm4sTikiXSxbMywwLCJTXFxvdGltZXNfUlxcb3B7SG9tfV9SKFJebSxOKSJdLFswLDEsIjAiXSxbMSwxLCJcXG9we0hvbX1fUyhTXFxvdGltZXNfUk0sU1xcb3RpbWVzX1JOKSJdLFsyLDEsIlxcb3B7SG9tfV9TKFNcXG90aW1lc19SUl5uLFNcXG90aW1lc19STikiXSxbMywxLCJcXG9we0hvbX1fUyhTXFxvdGltZXNfUlJebSxTXFxvdGltZXNfUk4pIl0sWzAsMV0sWzEsMl0sWzIsM10sWzYsN10sWzUsNl0sWzQsNV0sWzEsNSwiXFxhbHBoYSJdLFsyLDYsIlxcYWxwaGEiXSxbMyw3LCJcXGFscGhhIl1d&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
	\[\begin{tikzcd}
		0 & {S\otimes_R\op{Hom}_R(M,N)} & {S\otimes_R\op{Hom}_R(R^n,N)} & {S\otimes_R\op{Hom}_R(R^m,N)} \\
		0 & {\op{Hom}_S(S\otimes_RM,S\otimes_RN)} & {\op{Hom}_S(S\otimes_RR^n,S\otimes_RN)} & {\op{Hom}_S(S\otimes_RR^m,S\otimes_RN)}
		\arrow[from=1-1, to=1-2]
		\arrow[from=1-2, to=1-3]
		\arrow[from=1-3, to=1-4]
		\arrow[from=2-3, to=2-4]
		\arrow[from=2-2, to=2-3]
		\arrow[from=2-1, to=2-2]
		\arrow["\alpha", from=1-2, to=2-2]
		\arrow["\alpha", from=1-3, to=2-3]
		\arrow["\alpha", from=1-4, to=2-4]
	\end{tikzcd}\]
	But now the rightmost two vertical $\alpha$s are isomorphisms by \autoref{lem:basechangefree}, so the leftmost $\alpha$ is also an isomorphism. This finishes.
\end{proof}
\begin{remark}[Nir] \label{rem:applybasechange}
	When $R$ is Noetherian and $M$ is finitely generated (alternatively, only $M$ is finitely presented), we note that $M$ is finitely presented by \autoref{ex:fingennoetherian}. Further, for multiplicative $U$, we see $R\left[U^{-1}\right]$ is flat by \autoref{cor:localflat}. So \autoref{prop:localizeistensor} in addition to the above tells us
	\[\op{Hom}_R(M,N)\left[U^{-1}\right]\cong\op{Hom}_{R\left[U^{-1}\right]}\left(M\left[U^{-1}\right],N\left[U^{-1}\right]\right).\]
	This will be our chief application of \autoref{prop:basechange}.
\end{remark}
\begin{remark}[Nir]
	I am really proud of the working out of the discussion in this subsection. There are a lot of moving parts.
\end{remark}

\subsection{Support of a Module}
We have the following definition.
\begin{definition}[Support]
	Fix $R$ a ring and $M$ an $R$-module. Then we define the \textit{support} of $M$ to be
	\[\op{Supp}M:=\{\mf p\in\op{Spec}R:M_\mf p\ne0\}.\]
\end{definition}
There is an analogous notion of maximal support using maximal ideals instead of prime ideals.

We can provide a more concrete condition for $M_\mf p=0$. For this, we have the following definition.
\begin{definition}[Annihilator]
	Fix $R$ a ring and $M$ an $R$-module. Then, given an element $m\in M$, we define the \textit{annihilator} of $R$ to be
	\[\op{Ann}m:=\{r\in R:rm=0\}.\]
	Analogously, we define $\op{Ann}M:=\{r\in R:rm=0\text{ for all }m\in M\}=\bigcap_{m\in M}\op{Ann}m$.
\end{definition}
\begin{remark}
	It is not hard to check that these are ideals. If $r_1,r_2\in R$ and $x_1,x_2\in\op{Ann}m$, then
	\[(r_1x_1+r_2x_2)m=r_1(x_1m)+r_2(x_2m)=0\]
	verifies that $r_1x_1+r_2x_2\in\op{Ann}m$, so $\op{Ann}m$ is closed under $R$-linear combination. So $\op{Ann}m$ is an ideal, and the fact $\op{Ann}M$ is an ideal follows by taking the (arbitrary) intersection.
\end{remark}
So here is a characerization of $\op{Supp}M$.
\begin{proposition} \label{prop:generalsupport}
	Fix $R$ a ring and $M$ an $R$-module. Then, given $\mf p\in\op{Spec}R$, we have $M_\mf p\ne0$ if and only if $\op{Ann}m\subseteq\mf p$ for some $m\in M$. In other words,
	\[\op{Supp}M=\bigcup_{m\in M}\{\mf p\in\op{Spec}R:\op{Ann}m\subseteq\mf p\}.\]
\end{proposition}
\begin{proof}
	We proceed by contraposition, showing that $M_\mf p=0$ if and only if $\op{Ann}m\not\subseteq\mf p$ for each $m\in M$.
	
	Note that $M_\mf p=0$ if and only if $\frac mu=0$ for each $m\in M$ and $u\in U$. But note that if $\frac m1=0$ for each $m\in M$, then it follows
	\[\frac mu=\frac1u\cdot\frac1m=0\]
	for any $u\in U$. Thus, it suffices to check that $\frac m1=0$ for each $m\in M$.

	Well, fixing any $m\in M$, we see that $\frac m1=\frac01$ if and only if there is some $u\notin\mf p$ such that $um=0$. In other words, $\frac m1=\frac01$ is equivalent to
	\[(R\setminus\mf p)\cap\op{Ann}m\ne\emp,\]
	which is equivalent to $\op{Ann}m\not\subseteq\mf p$.
\end{proof}
The above characterization of the support is a bit annoying, geometrically speaking, because we are taking an arbitrary union of Zariski-closed sets $\{\mf p\in\op{Spec}R:\op{Ann}m\subseteq\mf p\}$. In the case where $M$ is finitely generated (which is essentially a size constraint on $M$), we can make this arbitrary union into a finite one.
\begin{proposition} \label{prop:fingensupport}
	Fix $R$ a ring and $M$ a finitely generated $R$-module. Then
	\[\op{Supp}M=\{\mf p\in\op{Spec}R:\op{Ann}M\subseteq\mf p\}.\]
\end{proposition}
\begin{proof}
	Of course, taking any $m\in M$, if $\op{Ann}m\subseteq\mf p$ for some $m\in M$, then $\op{Ann}M\subseteq\op{Ann}m\subseteq\mf p$. So \autoref{prop:generalsupport} tells us that
	\[\op{Supp}M=\bigcup_{m\in M}\{\mf p\in\op{Spec}R:\op{Ann}m\subseteq\mf p\}\subseteq\{\mf p\in\op{Spec}R:\op{Ann}M\subseteq\mf p\}.\]
	The other direction requires using that $M$ is finitely generated.

	Well, let $\mf p\notin\op{Supp}M$, and we show that $\op{Ann}M\not\subseteq\mf p$. The fact that $\mf p\notin\op{Supp}M$ implies that $\op{Ann}m\not\subseteq\mf p$ for each $m\in M$; in particular, letting $M$ be generated by $x_1,\ldots,x_n$, we see that each $x_k\in M$ promises $u_k$ such that
	\[u_k\in\op{Ann}x_k\setminus\mf p.\]
	In other words, $u_k\notin\mf p$ and $u_kx_k=0$. But now (by finiteness!) we can set
	\[u:=\prod_{k=1}^nu_k.\]
	Because each of the factors is not in $\mf p$, we conclude $u\notin\mf p$. However, $ux_k=0$ for each of the generators $x_k$, so for any $m=\sum a_kx_k\in M$, we see
	\[um=\sum_{k=1}^nua_kx_k=\sum_{k=1}^na_k\cdot0=0.\]
	It follows that $u\in\op{Ann}M\setminus\mf p$, so $\op{Ann}M\not\subseteq\mf p$.
\end{proof}
In particular, this is a Zariski-closed subset of $\op{Spec}R$!

We close this subsection with some examples.
\begin{example}
	Consider the ring $M:=R$ as an $R$-module. Certainly $0\in\op{Ann}R$, but for $r\in R$ to kill $1$, we need $r=0$, so actually $\op{Ann}R=(0)$. But $(0)$ is contained in every prime ideal of $R$, so $\op{Supp}R=\op{Spec}R$. (Yes, $M=R$ is finitely generated over $R$.)
\end{example}
\begin{ex} \label{ex:emptysupport}
	Fix $R$ a ring and $M=(0)$ the zero module. Then everyone in $R$ will kill $0$, so $\op{Ann}0=R$. It follows from \autoref{prop:generalsupport} that $\op{Supp}(0)=\emp$ because no prime contains $R$.
\end{ex}
\begin{example} \label{ex:annrmodi}
	More generally, fix $I\subseteq R$ an ideal. Then we claim $\op{Ann}R/I=I$. Indeed, if $x\in I$, then $x\cdot[r]_I=[rx]_I=[0]_I$, so $x\in\op{Ann}R/I$. Conversely, if $x\in\op{Ann}R/I$, then $x\cdot[1]_I=[x]_I$ must vanish, so $x\in I$.
\end{example}
To set up our last example, we have the following definition and then statement.
\begin{definition}[Simple]
	Fix $R$ a ring. Then an $R$-module $M$ is said to be \textit{simple} if and only if all $R$-submodules of $M$ are either $(0)$ or $M$.
\end{definition}
\begin{exe} \label{exe:classifysimple}
	Fix $R$ a ring and $M$ a simple nonzero $R$-module. Then the following are true.
	\begin{listalph}
		\item We have that $M\cong R/\op{Ann}M$.
		\item We have that $\op{Ann}M$ is a maximal ideal.
		\item We have that $\op{Supp}M=\{\op{Ann}M\}$.
	\end{listalph}
\end{exe}
\begin{proof}
	We take the claims more or less one at a time.
	\begin{listalph}
		\item Because $M$ is nonzero, we may find $x\in M\setminus\{0\}$. Now, $x$ induces an $R$-module homomorphism map $R\to M$ by $r\mapsto rx$ (indeed, $rs\mapsto rsx$ and $r_1+r_2\mapsto r_1x+r_2x$), and the kernel of this map is $\{r\in R:rx=0\}=\op{Ann}x$. Thus, we have the left-exact sequence of $R$-modules
		\[0\to\op{Ann}x\to R\to M.\]
		However, $M$ is simple! Thus, because the image of $R\to M$ will end up being an $R$-submodule of $M$---and nonzero because it contains $1x=x\ne0$---we see that the image of $R\to M$ must be all of $M$. So in fact we have the short exact sequence
		\[0\to\op{Ann}x\to R\to M\to 0.\]
		In particular, we just showed that $M=\{rx:r\in R\}=Rx$. Of course, $\op{Ann}M\subseteq\op{Ann}x$, but in fact equality holds: each $a\in\op{Ann}x$ will have $a(rx)=r(ax)=0$ for each $rx\in Rx=m$.
	
		Anyways, the point is that $R/\op{Ann}M\cong M$ (non-canonically) by $r\mapsto rx$.

		\item We show that $I:=\op{Ann}M$ is a maximal ideal. Certainly $I\ne R$ because then $M\cong R/R=(0)$ would be zero. Thus, $I$ is proper, so we can find a maximal ideal $\mf m$ such that $I\subseteq\mf m$. But then we consider the composite map $\varphi:M\to R/\mf m$ by
		\[M\cong R/I\to R/\mf m.\]
		Consider $\ker\varphi$. On one hand, note that $\ker\varphi\ne M$ because $\varphi$ is the composite of surjective maps and therefore surjective, and $R/\mf m$ is nonzero ($M$ being nonzero forces $R$ nonzero), so $\varphi$ cannot send all of $M$ to $0$.

		But $\ker\varphi$ is an $R$-submodule of $M$, so instead we must have $\ker\varphi=(0)$. So the composite $\varphi$ is injective, so the map $R/I\to R/\mf m$ is injective. But then $x\in\mf m$ implies $[x]_I\mapsto[x]_\mf m=[0]_\mf m$, so $x\in I$ by injectivity. Thus, $\mf m=I$, and so $I$ is in fact maximal.

		\item Because $R\onto R/\op{Ann}M\cong M$, we see that $M$ is finitely generated, so \autoref{prop:fingensupport} tells us that
		\[\op{Supp}M=\{\mf p\in\op{Spec}R:\op{Ann}M\subseteq\mf p\}.\]
		Now, $\op{Ann}M$ is maximal, so $\op{Ann}M\in\op{Supp}M$, but any prime ideal containing $\op{Ann}M$ must equal $\op{Ann}M$ by maximality. So $\op{Supp}M=\{\op{Ann}M\}$.
		\qedhere
	\end{listalph}
\end{proof}
\begin{remark} \label{rem:classifysimple}
	We can complete our classification of simple $R$-modules: for each maximal ideal $\mf m\subseteq R$, we can see $R/\mf m$ is a simple $R$-module. Indeed, any $R$-submodule $M\subseteq R/\mf m$ is in fact an $R/\mf m$-module, for each $x\in\mf m$ and $m\in M$ has $xm=[0]_\mf m=0$. Thus, $M$ is an $(R/\mf m)$-subspace of $R/\mf m$, so for dimension reasons, $M=(0)$ or $M=R/\mf m$.
\end{remark}

\subsection{New Supports from Old}
Let's see how the support behaves with some of our module constructions. For example, the support behaves well in short exact sequences.
\begin{proposition} \label{prop:suppses}
	Fix $R$ a ring. Suppose we have a short exact sequence
	\[0\to A\to B\to C\to 0\]
	of $R$-modules. Then $\op{Supp}B=\op{Supp}A\cup\op{Supp}C$.
\end{proposition}
\begin{proof}
	The main point is that localization is an exact functor. Namely, if $\mf p$ is any prime of $R$, then we get a short exact sequence
	\[0\to A_\mf p\to B_\mf p\to C_\mf p\to0.\]
	In particular, $A_\mf p=C_\mf p=0$ implies $B_\mf p=0$; and conversely, $B_\mf p=0$ implies $A_\mf p=C_\mf p=0$. Thus, $B_\mf p\ne0$ if and only if $A_\mf p\ne0$ or $C_\mf p\ne0$, which is exactly the claim that $\op{Supp}B=\op{Supp}A\cup\op{Supp}C$.
\end{proof}
And here we can see that supports behave with (arbitrary!) direct sums.
\begin{proposition}
	Fix $R$ a ring and $\mathcal M$ a collection of $R$-modules. Then
	\[\op{Supp}\bigoplus_{M\in\mathcal M}M=\bigcup_{M\in\mathcal M}\op{Supp}M.\]
\end{proposition}
\begin{proof}
	Fix a prime $\mf p$. By \autoref{prop:localdirectsums}, we see that
	\[\left(\bigoplus_{M\in\mathcal M}M\right)_\mf p\cong\bigoplus_{M\in\mathcal M}M_\mf p.\]
	In particular, $\left(\bigoplus_{M\in\mathcal M}M\right)_\mf p$ will be nonzero if and only if at least one of the individual $M_\mf p$ are nonzero. This is exactly the claim.
\end{proof}
Additinally, we can learn something from the module itself by studying the support.
\begin{proposition}
	Fix an $R$-module $M$. Then $M=0$ if and only if $M_\mf m=0$ for all maximal ideals $\mf m\subseteq R$.
\end{proposition}
\begin{proof}
	We have already discussed the forwards direction in \autoref{ex:emptysupport}. In the other direction, supose that the $R$-module $M$ has $M_\mf m=0$ for every maximal ideal $\mf m\subseteq R$.

	Well, pick up any $m\in M$. Then $\op{Ann}m$ is an $R$-ideal. Using the proof of \autoref{prop:generalsupport}, we see that each maximal ideal $\mf m$ has $\op{Ann}m\not\subseteq\mf m$, so $\op{Ann}m$ is not contained in any maximal ideal! Thus, we must have
	\[\op{Ann}m=R,\]
	so $1\in\op{Ann}m$, so $m=1m=0$. So all elements of $M$ are zero, so $M=0$.
\end{proof}
\begin{remark}
	In fact, the above implies $M=0$ if and only if $\op{Supp}M=\emp$. Indeed, we note that $\op{Supp}M=\emp$ will directly imply that $M_\mf m=0$ for each maximal ideal $\mf m$, from which $M=0$ follows by the above argument.
	
	In the other direction, if $\op{Supp}M\ne\emp$, then there is a prime $\mf p\in\op{Supp}M$. Thus, by \autoref{prop:generalsupport}, there is some $m$ so that
	\[\op{Ann}m\subseteq\mf p.\]
	Placing $\mf p$ inside of a maximal ideal $\mf m$, we see $\op{Ann}m\subseteq\mf m$, so $M_\mf m\ne0$ as well. So indeed, $M\ne0$.
\end{remark}
\begin{corollary} \label{cor:liftinglocal}
	Fix $\varphi:M\to N$ an $R$-module homomorphism and $\mf m\subseteq R$ a maximal ideal. Then we are promised a localized map $\varphi_\mf m:M_\mf m\to N_\mf m$. Then $\varphi_\mf m$ is injective/surjective/isomorphic for all maximal ideals $\mf m$ if and only if $\varphi$ is as well.
\end{corollary}
\begin{proof}
	The main point is to repeatedly use \autoref{cor:localizekercoker}. Note $\varphi$ is injective if and only if $\ker\varphi=0$ if and only if $\ker\varphi_\mf m=(\ker\varphi)_\mf m=0$ for all maximal ideals $\mf m\subseteq R$ if and only if $\varphi_\mf m$ is injective for all $\mf m$.
	
	Repeating the same argument with $\coker$ gives the analogous result for surjectivity. Combining the results for injectivity and surjectivity gives the result for being an isomorphism. This finishes.
\end{proof}
\begin{remark}[Nir]
	Here is an example application. Fix $C$ finitely presented in a short exact sequence
	\[0\to A\to B\stackrel\pi\to C\to 0.\tag{$*$}\label{eq:someses}\]
	This sequence splits if and only if $\op{Hom}_R(C,B)\stackrel{\pi\circ-}\to\op{Hom}_R(C,C)$ is surjective if and only if, for each maximal $\mf p$, the map $\op{Hom}_R(C,B)_\mf p\stackrel{\pi\circ-}\to\op{Hom}_R(C,C)_\mf p$ is surjecive by \autoref{cor:liftinglocal}. Tracking \autoref{rem:applybasechange} through shows this is equivalent to
	\[\op{Hom}_{R_\mf p}(C_\mf p,B_\mf p)\stackrel{\pi_\mf p\circ-}\to\op{Hom}_{R_\mf p}(C_\mf p,C_\mf p)\]
	being surjective, which is equivalent to \autoref{eq:someses} splitting locally for each maximal $\mf p$.
\end{remark}
We continue our fact-collection.
\begin{proposition}
	Fix $R$ a ring and $R$-modules $M$ and $N$. Then
	\[\op{Supp}(M\otimes_RN)\subseteq\op{Supp}M\cap\op{Supp}N.\]
\end{proposition}
\begin{proof}
	We take $\mf p\notin\op{Supp}M\cup\op{Supp}N$ and show that $\mf p\notin\op{Supp}(M\otimes_RN)$. Without loss of generality, we can actually take $\mf p\notin\op{Supp}M$.

	Well, we are given that $M_\mf p=N_\mf p=0$, so for each $m\in M$, there exists $u\notin\mf p$ such that $um=0$ (using \autoref{prop:generalsupport}). But then each $m\otimes n$ has
	\[u\cdot(m\otimes n)=(um)\otimes n=0,\]
	each $m\otimes n$ has some $u_{m\otimes n}\notin\mf p$ such that $u(m\otimes n)=0$. Extending linearly, any element $\sum_{k=1}^nm_k\otimes n_k$ in $M\otimes_RN$ will have
	\[u:=\prod_{k=1}^nu_{m_k\otimes n_k}\]
	with $u\notin\mf p$ (because $\mf p$ is prime) while
	\[u\cdot\sum_{k=1}^nm_k\otimes n_k=\sum_{k=1}^n(um_k)\otimes n_k=0.\]
	So we have indeed checked by \autoref{prop:generalsupport} that $\mf p\notin\op{Supp}(M\otimes_RN)$.
\end{proof}
\begin{remark} \label{rem:supptensor}
	In fact, in fact, if $M$ and $N$ are finitely generated, then $\op{Supp}(M\otimes_RN)=\op{Supp}M\cap\op{Supp}N$. We do not prove this now because it will require a little more technology; we prove it in \autoref{cor:supptensor}.
\end{remark}
\begin{ex}
	Consider the $\ZZ$-modules $\QQ$ and $\ZZ/2\ZZ$. Note $\QQ\otimes_\ZZ\ZZ/2\ZZ=0$, so
	\[\op{Supp}(\QQ\otimes_\ZZ\ZZ/2\ZZ)=\emp.\]
	However, $\QQ$ is an integral domain, so $\op{Ann}1=(0)$, implying by \autoref{prop:generalsupport} that $\op{Supp}\QQ=\op{Spec}R$. On the other hand, $\op{Ann}\ZZ/2\ZZ=(2)$, so \autoref{prop:fingensupport} gives $\op{Supp}\ZZ/2\ZZ=\{(2)\}$. Thus,
	\[\op{Supp}(\QQ\otimes_\ZZ\ZZ/2\ZZ)=\emp\subsetneq\{(2)\}=\op{Supp}\QQ\cap\op{Supp}\ZZ/2\ZZ.\]
\end{ex}

\subsection{Tensoring Algebras}
For the next construction, we note that if $S$ and $T$ are $R$-algebras, then $S\otimes_RT$ is an $R$-algebra, where our multiplication is defined by
\[(s_1\otimes t_1)(s_2\otimes t_2)=(s_1s_2)\otimes(t_1t_2).\]
One can run through the checks that this will be an $R$-algebra, but because I am actively avoiding proving that anything is a ring, we will not do this here.

We mention this to talk about the tensor product of coordinate rings. Here's a first example.
\begin{exe} \label{exe:basictensorcoordinate}
	If we have two free $k$-algebras $k[x_1,\ldots,x_n]$ and $k[y_1,\ldots,y_m]$, then we claim that
	\[k[x_1,\ldots,x_m]\otimes_kk[y_1,\ldots,y_n]\]
	is freely generated by the elements of the form $x_\bullet\otimes1$ and $1\otimes y_\bullet$; i.e., this is tensor product is a polynomial ring over $k$ with $m+n$ letters.
\end{exe}
\begin{proof}
	Note that any polynomial $f\in k[x_1,\ldots,x_m]$ has a unique representation as
	\[f(x_1,\ldots,x_m)=\sum_{d_1,\ldots,d_m=0}^\infty a_{d_1,\ldots,d_m}\left(x_1^{d_1}\cdots x_m^{d_m}\right),\]
	where all but finitely many of the $a$ coefficients vanish. In other words, this is really saying that the terms $x_1^{d_1}\cdots x_m^{d_m}$ form a $k$-basis of $k[x_1,\ldots,x_m]$; similarly, the terms $y_1^{e_1}\cdots y_n^{e_n}$ form a $k$-basis of $k[y_1,\ldots,y_n]$.

	Now, by \autoref{ex:tensorvecspaces}, it follows that the terms of the form
	\[x_1^{d_1}\cdots x_m^{d_m}\otimes y_1^{e_1}\cdots y_n^{e_n}\]
	will form a $k$-basis of $k[x_1,\ldots,x_m]\otimes_kk[y_1,\ldots,y_n]$.

	We are now ready to attack the statement directly. Indeed, note that the terms of the form $x_\bullet\otimes1$ and $1\otimes y_\bullet$ will indeed generate $k[x_1,\ldots,x_m]\otimes_kk[y_1,\ldots,y_m]$ because we can write
	\[x_1^{d_1}\cdots x_m^{d_m}\otimes y_1^{e_1}\cdots y_n^{e_n}=\left(\prod_{i=1}^m(x_i\otimes1)^{d_i}\right)\left(\prod_{j=1}^n(1\otimes y_j)^{e_j}\right),\]
	meaning that we can generate any basis element and hence any element by linear combination.

	It remains to show that the generation is free. Well, suppose that we can find some algebraic equation
	\[\sum_{\substack{d_1,\ldots,d_m\in\NN\\e_1,\ldots,e_n\in\NN}}a_{d_1,\ldots,d_m,e_1,\ldots,e_n}\left(\prod_{i=1}^m(x_i\otimes1)^{d_i}\right)\left(\prod_{j=1}^n(1\otimes y_j)^{e_j}\right)=0,\]
	where all but finitely many of the $a$ coefficients vanish. We claim that all of the $a$ coefficients must vanish. Indeed, we can expand out the monomials as
	\[\sum_{\substack{d_1,\ldots,d_m\in\NN\\e_1,\ldots,e_n\in\NN}}a_{d_1,\ldots,d_m,e_1,\ldots,e_n}\left(x_1^{d_1}\cdots x_m^{d_m}\otimes y_1^{e_1}\cdots y_n^{e_n}\right)=0.\]
	However, this means that a $k$-linear combination of $x_1^{d_1}\cdots x_m^{d_m}\otimes y_1^{e_1}\cdots y_n^{e_n}$ elements is vanishing, so all coefficients must be $0$ because we already established that these elements form a basis.
\end{proof}
\begin{remark}
	Geometrically, we can write this as $A\left(\AA^n(k)\right)\otimes_kA\left(\AA^m(k)\right)\cong A\left(\AA^n(k)\times\AA^m(k)\right)$, which makes more immediate sense.
\end{remark}
As suggested by the remark, in fact the following more general statement is true.
\begin{proposition}
	Fix affine algebraic sets $X$ and $Y$. Then $A(X\times Y)\cong A(X)\otimes_kA(Y)$ canonically as $k$-algebras.
\end{proposition}
\begin{proof}
	A lot of this problem is finding exactly what statement we want to prove. Let $X=Z(I)$ for an ideal $I\subseteq k[x_1,\ldots,x_m]$ and $Y=Z(J)$ for an ideal $J\subseteq k[y_1,\ldots,y_m]$.

	We now describe $X\times Y$. We see that $(x,y)\in\AA^m(k)\times\AA^n(k)$ if and only if $x\in X$ and $y\in Y$ if and only if $f(x)=0$ for each $f\in I$ and $g(y)=0$ for each $g\in J.$ Embedding the $f$ and $g$ into $k[x_1,\ldots,x_m,y_1,\ldots,y_n]=A\left(\AA^m(k)\times\AA^n(k)\right)$ in the natural way, we see that $f(x)=f(x,y)$ so that $f(x)=0$ is equivalent to $f(x,y)=0$, and $g(x,y)=g(y)$ so that $g(y)=0$ is equivalent to $g(x,y)=0$.

	Thus, $(x,y)\in X\times Y$ if and only if $f(x,y)=g(x,y)=0$ for each $f\in I$ and $g\in J$, implying we see that
	\[X\times Y=Z(I\cup J).\]
	Note that the ideal generated by $I\cup J$ is $(I\cup J)=I+J$. Thus, the claim that $A(X\times Y)\cong A(X)\otimes_kA(Y)$ canonically is the same as saying
	\[\frac{k[x_1,\ldots,x_m,y_1,\ldots,y_n]}{I+J}\cong\frac{k[x_1,\ldots,x_m]}{I}\otimes_k\frac{k[y_1,\ldots,y_n]}{J}.\]
	canonically.
	
	We have now transformed the desired result into an algebra problem. To exhibit the required isomorphism, we provide maps in both directions.
	\begin{itemize}
		\item Note that we can construct a $k$-bilinear map
		\[\psi:\frac{k[x_1,\ldots,x_m]}{I}\times\frac{k[y_1,\ldots,y_n]}{J}\to\frac{k[x_1,\ldots,x_m,y_1,\ldots,y_n]}{I+J}\]
		by $\psi:([f],[g])\mapsto[fg]$. We show that $\psi$ is well-defined and $k$-bilinear separately.
		\begin{itemize}
			\item Well-defined: if $[f]_I=[f']_I$ and $[g]_J=[g']_J$, then $f-f'\in I$ and $g-g'\in J$. Then
			\[(f-f')g,f'(g-g')\in I+J\subseteq k[x_1,\ldots,x_m,y_1,\ldots,y_n],\]
			so $fg-f'g'\in I+J$, so $[fg]=[f'g']$ in $A(X\times Y)$.
			\item Bilinear: given $c,c'\in k$ and $[f],[f']\in A(X)$ and $[g]\in A(Y)$, we find
			\[\psi(c[f]+c'[f'],[g])=\psi([cf+c'f'],[g])=[(cf+c'f')g]=c[fg]+c'[f'g]=c\psi(f,g)+c'\psi(f',g).\]
			Similarly, given $c,c'\in k$ and $[f]\in A(X)$ and $[g],[g']\in A(Y)$, we find
			\[\psi([f],c[g]+c'[g'])=\psi([f],[cg+c'g'])=[f(cg+c'g')]=c[fg]+c'[fg']=c\psi([f],[g])+c'\psi([f],[g']).\]
		\end{itemize}
		So $\psi$ is a $k$-bilinear map and therefore will induce a $k$-module morphism
		\[\overline\psi:\frac{k[x_1,\ldots,x_m]}{I}\otimes_k\frac{k[y_1,\ldots,y_n]}{J}\to\frac{k[x_1,\ldots,x_m,y_1,\ldots,y_n]}{I+J}\]
		by $f\otimes g\mapsto fg$.
		\item We cheat by appealing to \autoref{exe:basictensorcoordinate}, which provides a canonical $k$-algebra isomorphism
		\[k[x_1,\ldots,x_m,y_1,\ldots,y_n]\cong k[x_1,\ldots,x_m]\otimes_kk[y_1,\ldots,y_n]\]
		by $x_\bullet\mapsto x_\bullet\otimes1$ and $y_\bullet\mapsto1\otimes y_\bullet$. Now, modding out by $I$ and $J$ in the left and right coordinates, we get a $k$-algebra morphism
		\[\varphi:k[x_1,\ldots,x_m,y_1,\ldots,y_n]\to\frac{k[x_1,\ldots,x_m]}{I}\otimes_k\frac{k[y_1,\ldots,y_n]}{J}.\]
		Further, note that each $f(x,y)\in I$ will go to $f(x)\otimes1=0\otimes1$ (using the fact that $\varphi$ is a $k$-algebra morphism), so $f\in\ker\varphi$. Similarly, each $g\in J$ has $g\in\ker\varphi$, so $I\cup J\subseteq\ker\varphi$, so $I+J\subseteq\ker\varphi$, so we get an induced $k$-algebra morphism
		\[\overline\varphi:\frac{k[x_1,\ldots,x_m,y_1,\ldots,y_n]}{I+J}\to\frac{k[x_1,\ldots,x_m]}{I}\otimes_k\frac{k[y_1,\ldots,y_n]}{J}.\]
	\end{itemize}
	Now, we claim that $\overline\varphi$ is our desired canonical $k$-algebra isomorphsm. By construction, we know $\overline\varphi$ is a $k$-algebra homomorphism, and because $\overline\varphi$ is induced by the projection of an isomorphism, we know $\overline\varphi$ is surjective.

	Thus, it remains to show that $\overline\varphi$ is injective. It suffices to provide $\overline\varphi$ with a right inverse, which we claim is $\overline\psi$. Namely, we show $\overline\psi\circ\overline\varphi=\id$. Indeed, we see that, for any $x_\bullet$ and $y_\bullet$,
	\[(\overline\psi\circ\overline\varphi)([x_\bullet])=\overline\psi([x_\bullet]\otimes[1])=[x_\bullet]\qquad\text{and}\qquad(\overline\psi\circ\overline\varphi)([y_\bullet])=\overline\psi([1]\otimes[y_\bullet])=[y_\bullet],\]
	so it follows tht $\overline\psi\circ\overline\varphi$ induces the identity on all of $A(X\times Y)$. This finishes.
\end{proof}
\begin{remark}[Nir]
	I am under the impression that some trickery is required to show that (the more natural map) $\overline\psi$ is bijective. At a high level, we can view the above proof as requiring the creation of $\overline\varphi$ to prove the bijectivity and $\overline\psi$, where the ``hard work'' of this proof was in the appeal to \autoref{exe:basictensorcoordinate} to show that $\overline\varphi$ is well-defined.
\end{remark}
\begin{remark}
	One can generalize this construction to fiber products.
\end{remark}
Next class we will finish up localization by discussing modules of finite length.