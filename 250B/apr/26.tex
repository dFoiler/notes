% !TEX root = ../notes.tex

We begin with some announcements.
\begin{itemize}
	\item There will be no class on Thursday.
	\item There will be a review session next Tuesday.
\end{itemize}

\subsection{Projective Varieties}
Today we are speed-running elimination theory. There will be a lot of geometry. At a high level, commutative rings are supposed to be functions over affine varieties. Similarly, graded rings are supposed to be functions over a projective variety.

As in the case of affine rings, points and varieties are cut out by ideals of
\[k[x_0,\ldots,x_n].\]
Explicitly, this time our varieties are cut out by homogeneous polynomials, and our points are in bijection with maximal homogeneous ideals except for the irrelevant ideal $(x_0,\ldots,x_n)$. We will not prove this here, but we will state that these maximal homogeneous ideals take the form
\[\mf m_a=(a_\ell x_k-a_kx_\ell),\]
where $a=(a_0:\cdots:a_n)$ is some point in $\PP^n(k)$; in particular, the thing that we would have to check is that $V(\mf m_a)$ is the line spanned by $(a_0,\ldots,a_n)$ in $\AA^{n+1}(k)$.

We pick up the following facts.
\gradedassociatedprimeprop*
\noindent There is also a notion of Noether normalization for graded rings.
\begin{proposition}
	Fix a graded (affine) ring $R$ of dimension $n$. Then one can choose homogeneous and algebraically independent elements $x_1,\ldots,x_n\in R$ so that $R$ is finite over $k[x_1,\ldots,x_n]$.
\end{proposition}
\begin{proof}
	A similar proof to the Noether normalization theorem (\autoref{thm:noethernormal}) applies here.
\end{proof}
\productofaffinesprop*
\begin{proof}
	This is on the homework, though these notes do include a proof from earlier.
\end{proof}
We would like to do something like \autoref{prop:affineprod} for projective varieties. Namely, \autoref{prop:affineprod} tells us how to turn the product of affine varieties into an affine variety, but this is a little subtler for projective varieties.

Well, let's take two projective spaces $\PP^m(k)$ and $\PP^n(k)$. Then, given two points $(x_0:x_1:\ldots:x_m)$ and $(y_0:y_1:\ldots:y_n)$, we want to glue them together in some meaningful way. In particular, we hope that
\[\PP^m(k)\times\PP^n(k)\]
is a projective variety in some larger space. To begin, we make a matrix
\[M\coloneqq\begin{bmatrix}
	x_0y_0 & \cdots & x_0y_n \\
	\vdots & \ddots & \vdots \\
	x_my_0 & \cdots & x_my_n
\end{bmatrix}.\]
This makes an $(m+1)\times(n+1)$ matrix, defined up to scalar: scaling either $(x_0:\ldots,x_m)$ or $(y_0:\ldots:y_n)$ by some $c\in k^\times$ merely multiplies either all rows or all columns (respectively) by the scalar $c$. Thus, ``linearizing'' $M$, we think about $M$ as living in $\PP^{(m+1)(n+1)-1}$.

This gives us a way to map $\PP^m(k)\times\PP^n(k)$ into a large projective space, but we still need to actually cut out our variety; observe that it is not even completely obvious that we can do this because the map
\[\PP^m(k)\times\PP^n(k)\to\PP^{(m+1)(n+1)-1}(k)\]
is continuous (as an embedding), which need not send Zariski closed sets to Zariski closed sets.
% Regardless, the matrix above is supposed to have rank $1$, so we can just take the rank-$1$ matrices out of this.
Regardless, we define
\[z_{ij}\coloneqq x_iy_j.\]
Now, pick a pair of indices $(i,j)$ and $(i',j')$, and we note that the matrix
\[\begin{bmatrix}
	x_iy_j & x_iy_{j'} \\
	x_{i'}y_j & x_{i'}y_{j'}
\end{bmatrix}\]
is also invariant up to scaling (for the same reason that $M$ is), so in particular its determinant should vanish; as such, we should mod out $\PP^{(m+1)(n+1)-1}(k)$ out by the polynomials
\[z_{i\ell}z_{kj}=z_{ij}z_{k\ell}.\]
We will not prove it here, but these relations are sufficient to cut out $\PP^m(k)\times\PP^n(k)$.
% \todo{Maybe do this check}

\subsection{Elimination Theory Examples}
We are now ready to formulate the main theorem of elimination theory.
\begin{restatable}{theorem}{elimthm} \label{thm:elim}
	Fix an affine variety $X$ over an algebraically closed field $k$ and consider the natural projection map $\pi\colon X\times\PP^n(k)\to X$ for some projective space $\PP^n(k)$. Then, given a closed subset $Z\subseteq X\times\PP^n(k)$, the image $\pi (Z)$ is also closed.
\end{restatable}
Note that there is a reason why we are using projective space for our second coordinate.
\begin{exe} \label{exe:elimnex}
	Fix an algebraically closed field $k$. Let $\pi\colon\AA^1(k)\times\AA^1(k)\to\AA^1(k)$ be the natural projection onto the first coordinate. We exhibit a closed subset $Z\subseteq\AA^1(k)\times\AA^1(k)$ such that $\pi(Z)$ is not closed.
\end{exe}
\begin{proof}
	By \autoref{prop:affineprod}, we might as well think about $\AA^1(k)\times\AA^1(k)$ as $\AA^2(k)$ because they have the same affine coordinate rings. As such, the point is that $\pi$ is merely a continuous map and need not send closed sets to closed sets; to manifest this, we set
	\[Z\coloneqq\left\{(x,y)\in\AA^2(k):xy=1\right\}.\]
	Now, $\pi(Z)=\AA^1(k)\setminus\{0\}$. Here is the image.
	\begin{center}
		\begin{asy}
			unitsize(0.5cm);
			import graph;
			real y(real t)
			{
				return t;
			}
			real x(real t)
			{
				return 1/t;
			}
			draw(graph(x, y,-4,-1/4));
			draw(graph(x, y, 4,1/4));
			draw((-4,0)--(4,0), red); label("$x$", (4,0), E);
			draw((0,-4)--(0,4), dotted); label("$y$", (0,4), N);
			fill(circle((0,0), 0.2), white);
			draw(circle((0,0), 0.2), red);
		\end{asy}
	\end{center}
	It is not terribly difficult to just compute $\pi\left(Z\right)$ directly.
	\begin{itemize}
		\item For $x\ne0$, we see that $(x,1/x)\in Z$, so $x\in\pi(Z)$.
		\item For $x=0$, there is no $y\in k$ such that $xy=1$, so $x\notin\pi(Z)$.
	\end{itemize}
	The above cases do verify that $\pi(Z)=\AA^1(k)\setminus\{0\}$. It remains to see that $\pi(Z)$ is not actually Zariski closed. Well, for any $f\in k[x]$ vanishing on $\pi(Z)$, we see that $f$ must have infinitely many roots from $k\setminus\{0\}$ (because $k$ is algebraically closed), so $f=0$. It follows that
	\[V(I(\pi(Z)))=V((0))=\AA^1(k),\]
	so $\pi(Z)$ is not equal to its own Zariski closure, implying that $\pi(Z)$ is not Zariski closed.
\end{proof}
Let's explain why we are calling our theory ``elimination theory.'' To begin, note that all points take the form
\[\big(\underbrace{(x_1,\ldots,x_m)}_s,(y_0:\ldots:y_n)\big)\in X\times\PP^{n}(k).\]
Now, we can define our Zariski closed subset $Z\subseteq X\times\PP^n(k)$ via some equations
\[\begin{cases}
	f_1(s,y_0,\ldots,y_n)=0, \\
	\quad\quad\vdots \\
	f_r(s,y_0,\ldots,y_n)=0.
\end{cases}\]
Now, $\pi(Z)$ consists of those $s\in X$ such that there exists some point $(y_0:\ldots:y_n)\in\PP^n(k)$ living in $Z$; in other words, we are searching for $s\in X$ such that there is a nonzero solution $(y_0,\ldots,y_n)$ to the above equations $f_1,\ldots,f_s$. So we are in some sense trying to ``eliminate'' these $y_\bullet$ from our parameters.

There was a relatively rich classical field dealing with these types of questions; let's see a few examples; we start with two variables and two linear equations.
\begin{exe}
	Work in the context of \autoref{thm:elim}, and fix $Z\subseteq X\times\PP^1(k)$ given by the equations
	\[\begin{cases}
		f_0(s)y_0+f_1(s)y_1=0, \\
		g_0(s)y_0+g_1(s)y_1=0.
	\end{cases}\tag{1}\label{eq:easyresultant}\]
	We verify that $\pi(Z)\subseteq X$ is Zariski closed.
\end{exe}
\begin{proof}
	We need to find all $s\in X$ such that there is $(y_0:y_1)\in\PP^1(k)$ so that $(s,(y_0:y_1))$ satisfies \autoref{eq:easyresultant}. Of course, the $(y_0:y_1)$ really have no constraints except for being nonzero, so we are really asking for \autoref{eq:easyresultant} to have a solution $(s,y_0,y_1)$ where $(y_0,y_1)\ne(0,0)$.
	
	Imagining that $s$ is fixed for a moment, we see that we are thus really asking for
	\[\begin{bmatrix}
		f_0(s) \\
		g_0(s)
	\end{bmatrix}\quad\text{and}\quad\begin{bmatrix}
		f_1(s) \\
		g_1(s)
	\end{bmatrix}\]
	to have a nontrivial relation (given by $(y_0,y_1)$), which is equivalent to the matrix
	\[M\coloneqq\begin{bmatrix}
		f_0(s) & f_1(s) \\
		g_0(s) & g_1(s)
	\end{bmatrix}\]
	being singular. Well, $M$ is singular if and only if $\det M$ vanishes, so we see we are asking for all $s\in X$ such that
	\[g_1(s)f_0(s)-f_1(s)g_0(s)=0,\]
	This does indeed cut out an algebraic set of $X$, so we are safe.
\end{proof}
Going a little further, we can do arbitrary two variables and two equations in general.
\begin{exe}[Sylvester] \label{exe:elimtwoequations}
	Work in the context of \autoref{thm:elim}, and fix $Z\subseteq X\times\PP^1(k)$ given by the equations
	\[\begin{cases}
		f_0(s)y_0^d+f_1(s)y_0^{d-1}y_1+\cdots+f_{d+1}(s)y_1^d=0, \\
		g_0(s)y_0^e+g_1(s)y_0^{e-1}y_1+\cdots+g_{e+1}(s)y_1^e=0.
	\end{cases}\tag{2}\label{eq:resultant}\]
	We verify that $\pi(Z)$ is Zariski closed.
\end{exe}
\begin{proof}
	The point is to use the resultant. In the following discussion, we will fix some $s\in X$ and write $f_i$ and $g_j$ for $f_i(s)$ and $g_j(s)$, respectively. To construct the resultant, we build the following $e\times(e+d)$ matrix
	\[\begin{bmatrix}
		f_0 & f_1 & \cdots & f_{d+1} & 0 & \cdots & 0 & 0 \\
		0 & f_0 & \cdots & f_d & f_{d+1} & \cdots & 0 & 0\\
		\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & \cdots & 0 & 0 & \cdots & f_{d+1} & 0 \\
		0 & 0 & \cdots & 0 & 0 & \cdots & f_d & f_{d+1} \\
	\end{bmatrix}\]
	and attach the following $d\times(e+d)$ matrix
	\[\begin{bmatrix}
		g_0 & g_1 & \cdots & g_{e+1} & 0 & \cdots & 0 & 0 \\
		0 & g_0 & \cdots & g_e & g_{e+1} & \cdots & 0 & 0\\
		\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & \cdots & 0 & 0 & \cdots & g_{e+1} & 0 \\
		0 & 0 & \cdots & 0 & 0 & \cdots & g_e & g_{e+1} \\
	\end{bmatrix}\]
	below it to make a $(e+d)\times(e+d)$ matrix. For concreteness, our matrices will be written with $d=2$ and $e=3$ (though the argument will work in higher generality), which gives
	\[M\coloneqq\begin{bmatrix}
		f_0 & f_1 & f_2 \\
		    & f_0 & f_1 & f_2 \\
			&     & f_0 & f_1 & f_2 \\
		g_0 & g_1 & g_2 & g_3 \\
		    & g_0 & g_1 & g_2 & g_3
	\end{bmatrix}.\]
	We claim that we have a solution to our equations for a given $s\in X$ if and only if the determinant of $M$ vanishes.

	In one direction, suppose that $s\in\pi(Z)$ so that we have a point $(y_0,y_1)\ne(0,0)$ such that $(s,(y_0,y_1))\in Z$. The main point is to consider the following matrix product
	\[\begin{bmatrix}
		f_0 & f_1 & f_2 \\
		    & f_0 & f_1 & f_2 \\
			&     & f_0 & f_1 & f_2 \\
		g_0 & g_1 & g_2 & g_3 \\
		    & g_0 & g_1 & g_2 & g_3
	\end{bmatrix}\begin{bmatrix}
		y_0^4 \\
		y_0^3y_1 \\
		y_0^2y_1^2 \\
		y_0y_1^3 \\
		y_1^4
	\end{bmatrix}.\]
	Now, by multiplying our equations through, we see that the first $e$ rows are all the first equation of \autoref{eq:resultant} multiplied by some suitable $y_0^ay_1^b$, and the last $d$ rows are the second equation of \autoref{eq:resultant} again multiplied by some suitable $y_0^ay_1^b$. In particular, because our point $(y_0,y_1)$ satisfies \autoref{eq:resultant}, we have found a nontrivial element in $\ker M$ (nontrivial because $y_0\ne0$ or $y_1\ne0$), so $\det M=0$ follows.

	In the reverse direction, suppose $\det M=0$. Set $V_n$ to be the $k$-vector space of homogeneous spaces of degree $n$ in $k[y_0,y_1]$; by the usual monomial-counting arguments, we see $\dim V=n+1$, spanned by the monomials
	\[y_0^n,\,y_0^{n-1}y_1,\,\ldots,\,y_0y_1^{n-1},\,y_1^n.\]
	Note that multiplying by our first equation $f$ gives us a map
	\begin{align*}
		\mu_f\colon V_{e-1} &\to V_{e+d-1} \\
		\alpha &\mapsto \alpha f
	\end{align*}
	because $f$ is homogeneous of degree $d$. Similarly, multiplying by our second equation gives us a map
	\begin{align*}
		\mu_g\colon V_{d-1} &\to V_{e+d-1} \\
		\beta &\mapsto \beta g
	\end{align*}
	because $g$ is homogeneous of degree $e$. In particular, we can glue these into a map $\mu_f\oplus\mu_g\colon V_{e-1}\oplus V_{d-1}\to V_{d+e-1}$, which we can see becomes a $(e+d)\times(e+d)$ matrix under our basis. In particular, we have
	\[f\cdot y_0^ay_1^{e-1-a}=\sum_{k=0}^df_ky_0^{a+d-k}y_1^{(e-1-a)+k}\qquad\text{and}\qquad g\cdot y_0^ay_1^{d-1-a}=\sum_{k=0}^eg_ky_0^{a+e-k}y_1^{(d-1-a)+k},\]
	which means that $\mu_f\oplus\mu_g$ is the matrix (using $(d,e)=(2,e)$)
	\[\begin{bmatrix}
		f_0 &     &     & g_0 &     \\
		f_1 & f_0 &     & g_1 & g_0 \\
		f_2 & f_1 & f_0 & g_2 & g_1 \\
		    & f_2 & f_1 & g_3 & g_2 \\
			&     & f_2 &     & g_3
	\end{bmatrix}=M^\intercal\]
	on the basis $\left\{y_0^{e-1},y_0^{e-2}y_1,\ldots,y_0y_1^{e-2},y_1^{e-1}\right\}\cup\left\{y_0^{d-1},y_0^{d-2}y_1,\ldots,y_0y_1^{d-2},y_1^{d-1}\right\}$ of $V_{e-1}\oplus V_{d-1}$.
	
	So because $\det M=0$, we see that $\det(\mu_f\oplus\mu_g)=0$, so $\mu_f\oplus\mu_g$ is singular as a linear transformation and therefore has nontrivial kernel. Thus, we can find a pair of polynomials $(\alpha,\beta)\in V_{e-1}\oplus V_{d-1}$ with $(\alpha,\beta)\ne(0,0)$ such that
	\[\alpha(y_0,y_1)f(s,y_0,y_1)=\beta(y_0,y_1)g(s,y_0,y_1).\]
	Without loss of generality, we will take $\beta\ne0$ so that $\deg\beta=d-1$. Dividing out by $y_1^{e+d-1}$ makes everything into a polynomial in $y\coloneqq y_0/y_1$ as
	\[\alpha(y,1)f(s,y,1)=\beta(y,1)g(s,y,1)\]
	because all polynomials involved are homogeneous. Now, $f(s,y,1)$ will have $d$ roots counted with multiplicity in $k$ while $\beta(y,1)$ has $d-1$ roots counted with multiplicity in $k$ (recall $\beta\ne0$), so $f(s,y,1)$ is forced to share a root with $g(s,y,1)$. This root is our witness to $(s,(y:1))\in X\times\PP^1(k)$.
\end{proof}
\begin{remark}
	The above construction $\det M$ is called the resultant $\op{res}(f,g)$; its main purpose is that $\op{res}(f,g)$ vanishes if and only if $f$ and $g$ have a common root, which roughly follows from the argument given in the proof. For example, $f$ has a double root if and only if it shares a root with $f'$, which means we want to compute the discriminant $\op{disc}f\coloneqq\op{res}(f,f')$.
\end{remark}

\subsection{Elimination Theory Proofs}
There are two proofs of \autoref{thm:elim} in Eisenbud. The text of the chapter has an advanced proof which achieves more, but we do not have time for it. Instead, we will follow the proof presented in Exercise 14.1. Recall the statement.
\elimthm*
\begin{proof}[Proof of \autoref{thm:elim}]
	We proceed directly. As suggested, we can define our subset $Z\subseteq X\times\PP^n(k)$ as cut out by the equations
	\[\begin{cases}
		f_1(s,y_0,\ldots,y_n)=0, \\
		\quad\quad\vdots \\
		f_r(s,y_0,\ldots,y_n)=0.
	\end{cases}\tag{3}\label{eq:generalelim}\]
	Here each of the equations $f_i$ are homogeneous in the $y_\bullet$s. Now,
	\[\pi(Z)=\left\{s\in X:(s,(y_0:\ldots,y_n))\in X\times\PP^n(k)\right\},\]
	so $\pi(Z)$ consists of the $s\in X$ for which there is a nonzero solution for the $y_\bullet$s in \autoref{eq:generalelim}.
	
	For now, imagine fixing some $s\in S$. If we look for all solutions $(y_0,\ldots,y_n)\in\AA^{n+1}(k)$, we see that the equations in \autoref{eq:generalelim} cut out some ideal
	\[I_s\subseteq k[y_0,\ldots,y_n].\]
	Because $I_s$ is generated by homogeneous polynomials (of positive degree by multiplying an equation $f_i$ by $y_0\cdots y_n$ as necessary), the point $(0,\ldots,0)$ is certainly in $V(I_s)$, but we are hoping that $V(I_s)$ has some point outside $(0,\ldots,0)$. In particular, using the Nullstellensatz, we are hoping for
	\[V(I_s)\ne\{(0,\ldots,0)\}\iff\rad I_s=I(V(I_s))\ne I(V(\{(0,\ldots,0\})))=(y_0,\ldots,y_n)\]
	by \autoref{thm:nullstellensatz}. Well, because we know that $V(I_s)$ at least contains $(0,\ldots,0)$, we get $I\subsetneq k[y_0,\ldots,y_s]$; as such, $\rad I_s=(y_0,\ldots,y_n)$ is equivalent to having $y_k^{e_k}\in I$ for some suitably large $e_k$ for each $e_k$, which is equivalent to
	\[(y_0,\ldots,y_n)^e\subseteq I\]
	for some suitably large $e\in\NN$.

	We now let $s\in X$ vary again. Unraveling the above discussion, we see that
	\[\pi(Z)=\left\{s\in S:I_s\not\supseteq(y_0,\ldots,y_n)^d\text{ for all }d\in\NN\right\}.\]
	For brevity, set $J\coloneqq(y_0,\ldots,y_n)$ (note this is the irrelevant ideal of $k[y_0,\ldots,y_n]$) and define
	\[X_e\coloneqq\left\{s\in X:I_s\not\supseteq J^d\right\}\]
	for $d\in\NN$. It follows that
	\[\pi(Z)=\bigcap_{d\ge0}X_d,\]
	so it now suffices to show that the $X_d$ are Zariski closed (for sufficiently large $d$) and appeal to the topological fact that the intersection of closed sets is closed.

	For concreteness, continue to let $s\in X$ vary somewhat, and let $d_i$ be the degree of the $y_\bullet$ in equation $f_i$, and we will go ahead and suppose that $d>d_i$ for all $i$. To test $I_s\not\supseteq J^d$, we note that $J^d$ is generated by the degree-$d$ monomials in the $y_\bullet$. As such, we let
	\[V_d\subseteq k[y_0,\ldots,y_n]\]
	be the degree-$d$ component as before, and we again recall that $V_d$ has a basis given by the degree-$d$ monomials in the $y_\bullet$, which are exactly the generators of $J^d$. Thus,
	\[J^d\not\subseteq I_s\iff V_d\not\subseteq I_s.\]
	Continuing to take cues from \autoref{exe:elimtwoequations}, we define the map
	\begin{align*}
		\mu_i\colon V_{d-d_i} &\to V_d \\
		\alpha &\mapsto f\alpha
	\end{align*}
	which again is well-defined because $f$ is homogeneous of degree $d_i$. Now, taking the direct sum of all the $\mu_i$ gives us a map
	\begin{align*}
		\mu\colon\bigoplus_{i=1}^rV_{d-d_i} &\to V_d, \\
		(\alpha_i)_{i=1}^r &\mapsto \sum_{i=1}^rf_i\alpha_i.
	\end{align*}
	By the grading, we note that the image of $\mu$ is $I_s\cap J^d=(f_1,\ldots,f_r)\cap J^d$. So to make sure that $I_s$ is avoiding $V_d$, we need to check that $\mu$ is not surjective.
	
	Quickly, observe that, as in the case of \autoref{exe:elimtwoequations}, we can write out $\mu$ as some giant
	\[\dim V_d\times\left(\sum_{i=1}^r\dim V_{d-d_i}\right)\]
	matrix $M$ in terms of the natural bases of $V_{d-d_i}$ and $V_d$, though we will not do this explicitly; the point is that all the entries of this matrix are just the components of $f_i(s)$ and are therefore polynomials in $s$. We now claim that $\mu$ is not surjective if and only if all $(\dim V_d)\times(\dim V_d)$ minors of $M$ vanish. This will finish because each of these minors vanishing cuts out a single polynomial equation in $X$ and will thus show that $X_d$ is an affine set.
	\begin{itemize}
		\item Well, in one direction, if $\mu$ is not surjective, then all its columns need to lie in some subspace of dimension strictly smaller than $\dim V_d$. As such, any $(\dim V_d)\times(\dim V_d)$ minor---which consists entirely of column vectors of $M$---must vanish because the column vectors are forced to have a linear dependence.
		\item Conversely, if $\mu$ is surjective, then its columns contain a spanning set and hence contain a basis for $V_d$, which means any $(\dim V_d)\times(\dim V_d)$ consisting exactly of these $\dim V_d$ column vectors must vanish.
	\end{itemize}
	The above equivalence shows that $X_d$ can be cut out by polynomial equations (namely, the minors of $M$) and therefore is closed. This finishes the proof.
\end{proof}
\begin{remark}
	Annoyingly, this proof is non-constructive (in the equations to cut out the variety $\pi(Z)$) because we took an infinite intersection of these closed sets to show that our set was closed. The exercises we did above were more constructive but not generalizable. Such is life.
\end{remark}

Roughly speaking, we are kind of saying that projective space is compact, in the sense that the image is closed, and compact sets are approximately the only ones which stay compact/closed through a continuous map.\footnote{The Zariski topology is not Hausdorff, but this is fine.} More precisely, 
\begin{corollary}
	The image of a projective variety through a map $\varphi\colon Y\to X$ is closed.
\end{corollary}
\begin{proof}
	We use \autoref{thm:elim}. Place $Y\subseteq\PP^m(k)$ and $X\subseteq\PP^n(k)$. By embedding our spaces in a sufficiently large projective space, we may assume that $m=n$; namely, if $n\le m$, then extend $\varphi$ by the embedding $\PP^n(k)\into\PP^m(k)$ by $(a_0:\ldots:a_n)\mapsto(a_0:\ldots:a_n:1:1:\ldots:a)$. Otherwise, if $m<n$, then we can just extend $\varphi$ to do nothing with the extra coordinates granted by $\PP^n(k)$, and $X$ is still a closed subspace of $\PP^n(k)$ under the embedding $\PP^m(k)\into\PP^n(k)$ while $\im\varphi$ remains unchanged.
	
	Now, the point is to be able to consider the ``graph''
	\[Z\coloneqq\{(x,y)\in X\times\PP^n(k):y\in Y,\,x=\varphi(y)\}.\]
	Note that the direction of the coordinates is reversed so that we can project onto the first coordinate later on to get $\im\varphi$. As such, we proceed in steps.
	\begin{enumerate}
		\item Note that $X\times X\subseteq\PP^n(k)\times\PP^n(k)$ is closed: if $X$ is cut out by some homogeneous polynomials $\{f_1,\ldots,f_r\}$, then $X\times X$ is cut out by the polynomials
		\[\{f_1(x_0,\ldots,x_n),\ldots,f_r(x_0,\ldots,x_n)\}\cup\{f_1(y_0,\ldots,y_n),\ldots,f_r(y_0,\ldots,y_n)\}.\]
		Explicitly, $(a,b)\in X\times X$ if and only if $a\in X$ and $b\in X$ if and only if $a$ satisfies the equations $\{f_1(x_0,\ldots,x_n),\ldots,f_r(x_0,\ldots,x_n)\}$ and $b$ satisfies the equations $\{f_1(y_0,\ldots,y_n),\ldots,f_r(y_0,\ldots,y_n)\}$.
		\item Further, we see that
		\[\Delta_{\PP^n(k)}\coloneqq\left\{(x,x):x\in\PP^n(k)\right\}\]
		is also closed. Explicitly, we can be cut out as a projective variety from $\PP^{2n}(k)$ by the homogeneous equations
		\[x_iy_j-x_jy_i=0.\]
		Namely, certainly any element $(a,b)\in\Delta_{\PP^n(k)}$ does indeed satisfy all the above equations because we can write $a=\lambda b$ for some $\lambda\in k^\times$, which causes all the above equations to vanish as needed.

		Conversely, if a pair of nonzero points $((a_0,\ldots,a_n),(b_0,\ldots,b_n))$ satisfies the given equations, then suppose without loss of generality that $a_0$ and $b_0$ are nonzero. Then the equations
		\[a_ib_0-a_0b_i=0\]
		forces $b_i=(b_0/a_0)b_i$, so $\lambda\coloneqq b_0/a_0\in k^\times$ gives $(b_0,\ldots,b_n)=\lambda(a_0,\ldots,a_n)$, meaning our point does live in $\Delta_{\PP^n(k)}$.
		\item Next, we note that
		\[\Delta_X\coloneqq(X\times X)\cap\Delta_{\PP^n(k)}=\{(a,b):a,b\in X\text{ and }a=b\}=\{(a,a):a\in X\}\]
		is closed as the intersection of closed sets. It follows that $\Delta_X$ is also a closed set in $X\times X$.
		\item Now, $(\id_X\times\varphi)\colon X\times\PP^n(k)\to X\times X$ is a continuous map as the product of continuous maps, so
		\begin{align*}
			(\id_X\times\varphi)^{-1}(\Delta_X) &= \{(a,b)\in X\times\PP^n(k):a=\id_Xx\text{ and }b=\varphi(x)\text{ for some }x\in X\} \\
			&= \{(a,b)\in X\times\PP^n(k):b=\varphi(a)\} \\
			&= Z
		\end{align*}
		\item Now, we may project $Z$ down to the $X$ coordinate to recover
		\[\{x\in X:x=\varphi(y)\text{ for some }y\in Y\}=\im\varphi,\]
		which by \autoref{thm:elim} is also Zariski closed in $X$.
	\end{enumerate}
	The last step finishes the proof.
\end{proof}
Let's see an example of the above.
\begin{ex}
	We embed $\PP^1(k)\to\PP^2(k)$ by
	\[(s:t)\mapsto\left(s^3:s^2t+st^2:t^3\right).\]
	This should have closed image in $\PP^2(k)$, and indeed we can see that the image is cut out by the equation
	\[y^3-x^2z-xz^2-3xyz=0,\]
	which we can see by direct expansion.
\end{ex}
\begin{remark}
	The final exam might ask us to do something like the above computations.
\end{remark}

\subsection{Speed Run}
The following speed-run will not show up on the final, but it is fun. So let's talk about dimension theory.

To continue our geometric story, we as usual fix some morphism of affine varieties $\varphi\colon X\to Y$. One might ask what we can see about the image. The image is allowed to be open (as in \autoref{exe:elimnex}), but we can say more. We have the following definition.
\begin{definition}[Locally closed]
	An affine set $X$ is \textit{locally closed} if and only if it is the intersection a Zariski closed and a Zariski open set.
\end{definition}
\begin{definition}[Constructible]
	An affine set $X$ is \textit{constructible} if and only if it is a finite union of locally closed sets.
\end{definition}
And here is our theorem.
\begin{theorem}[Chevalley]
	Fix a ring homomorphism $\varphi\colon R\to S$ so that $S$ is finite over $R$. Define
	\[X(\varphi)\coloneqq\left\{\varphi^{-1}\mf q:\mf q\in\op{Spec}S\right\}\]
	to be the image of $\varphi$. Then $X(\varphi)$ is constructible.
\end{theorem}
We can even be a little sharper as to how our fibers behave. To review, given a maximal point $\mf p\in\op{Spec}R$, we note that
\[K(R/\mf p)\otimes_RS\]
is the ring of functions on the fiber $\varphi^{-1}\mf p$. One has to be careful that the fiber might not be a variety meaning that $K(R/\mf p)\otimes_RS$ need not be a domain. Regardless, we have the following.
\begin{theorem}
	Fix a ring homomorphism $\varphi\colon R\to S$ so that $S$ is finite over $R$. Then the map
	\[\mf p\in\op{Spec}R\longmapsto\dim K(R/\mf p)\otimes_RS\]
	is semicontinuous. If in fact $S$ is graded as an $R$-algebra by $R=S_0$, then the set of points of $R$ with points of dimension greater than $d$ is closed.
\end{theorem}
The above result can actually recover \autoref{thm:elim}.